{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10587017,"sourceType":"datasetVersion","datasetId":6274971},{"sourceId":10659576,"sourceType":"datasetVersion","datasetId":6601086},{"sourceId":10739670,"sourceType":"datasetVersion","datasetId":6659558},{"sourceId":10759017,"sourceType":"datasetVersion","datasetId":6673684}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Addestramento dei modelli del pool","metadata":{}},{"cell_type":"markdown","source":"#### Configurazioni generali","metadata":{}},{"cell_type":"markdown","source":"Installazione della librerie necessarie.","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets torch peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:44:22.210552Z","iopub.execute_input":"2025-03-23T20:44:22.210902Z","iopub.status.idle":"2025-03-23T20:44:26.556846Z","shell.execute_reply.started":"2025-03-23T20:44:22.210877Z","shell.execute_reply":"2025-03-23T20:44:26.555840Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install tensorflow==2.17.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:44:26.558100Z","iopub.execute_input":"2025-03-23T20:44:26.558435Z","iopub.status.idle":"2025-03-23T20:45:27.721254Z","shell.execute_reply.started":"2025-03-23T20:44:26.558401Z","shell.execute_reply":"2025-03-23T20:45:27.720441Z"}},"outputs":[{"name":"stdout","text":"Collecting tensorflow==2.17.0\n  Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.12.1)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (18.1.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.4.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.17.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.68.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.17.1)\nRequirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.37.1)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.26.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.17.0) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2024.12.14)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (2.18.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.0) (0.1.2)\nDownloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tensorflow\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.17.1\n    Uninstalling tensorflow-2.17.1:\n      Successfully uninstalled tensorflow-2.17.1\nSuccessfully installed tensorflow-2.17.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install codecarbon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:45:27.723078Z","iopub.execute_input":"2025-03-23T20:45:27.723401Z","iopub.status.idle":"2025-03-23T20:45:39.682346Z","shell.execute_reply.started":"2025-03-23T20:45:27.723373Z","shell.execute_reply":"2025-03-23T20:45:39.681362Z"}},"outputs":[{"name":"stdout","text":"Collecting codecarbon\n  Downloading codecarbon-2.8.3-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: arrow in /usr/local/lib/python3.10/dist-packages (from codecarbon) (1.3.0)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from codecarbon) (8.1.7)\nCollecting fief-client[cli] (from codecarbon)\n  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.2.2)\nRequirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.21.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from codecarbon) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from codecarbon) (9.0.0)\nRequirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from codecarbon) (11.4.1)\nCollecting questionary (from codecarbon)\n  Downloading questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\nCollecting rapidfuzz (from codecarbon)\n  Downloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.32.3)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from codecarbon) (13.9.4)\nRequirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.15.1)\nRequirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon) (2.8.2)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon) (2.9.0.20241206)\nCollecting httpx<0.28.0,>=0.21.3 (from fief-client[cli]->codecarbon)\n  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\nCollecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon)\n  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\nCollecting yaspin (from fief-client[cli]->codecarbon)\n  Downloading yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (1.26.4)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\nRequirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from questionary->codecarbon) (3.0.48)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2024.12.14)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (2.18.0)\nRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (4.12.2)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->codecarbon) (1.5.4)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.7)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\nRequirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.10/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->codecarbon) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->codecarbon) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->codecarbon) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->codecarbon) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->codecarbon) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->codecarbon) (2.4.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.13)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.17.0)\nCollecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon)\n  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.2.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas->codecarbon) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas->codecarbon) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas->codecarbon) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas->codecarbon) (2024.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas->codecarbon) (2024.2.0)\nDownloading codecarbon-2.8.3-py3-none-any.whl (516 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.7/516.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading questionary-2.1.0-py3-none-any.whl (36 kB)\nDownloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fief_client-0.20.0-py3-none-any.whl (20 kB)\nDownloading yaspin-3.1.0-py3-none-any.whl (18 kB)\nDownloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\nInstalling collected packages: termcolor, rapidfuzz, yaspin, questionary, httpx, jwcrypto, fief-client, codecarbon\n  Attempting uninstall: termcolor\n    Found existing installation: termcolor 2.5.0\n    Uninstalling termcolor-2.5.0:\n      Successfully uninstalled termcolor-2.5.0\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.28.1\n    Uninstalling httpx-0.28.1:\n      Successfully uninstalled httpx-0.28.1\nSuccessfully installed codecarbon-2.8.3 fief-client-0.20.0 httpx-0.27.2 jwcrypto-1.5.6 questionary-2.1.0 rapidfuzz-3.12.2 termcolor-2.3.0 yaspin-3.1.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Importo i moduli necessari.","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:45:39.683845Z","iopub.execute_input":"2025-03-23T20:45:39.684164Z","iopub.status.idle":"2025-03-23T20:45:43.249125Z","shell.execute_reply.started":"2025-03-23T20:45:39.684131Z","shell.execute_reply":"2025-03-23T20:45:43.248225Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Impostazione del seme casuale per la riproducibilità.","metadata":{}},{"cell_type":"code","source":"seed_value = 42\n\nos.environ['PYTHONHASHSEED'] = str(seed_value)\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\n\n# Imposto il seme casuale anche per i calcoli CUDA\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)  \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:45:43.250012Z","iopub.execute_input":"2025-03-23T20:45:43.250411Z","iopub.status.idle":"2025-03-23T20:45:43.306688Z","shell.execute_reply.started":"2025-03-23T20:45:43.250382Z","shell.execute_reply":"2025-03-23T20:45:43.305984Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## AG News","metadata":{}},{"cell_type":"markdown","source":"### Ottenimento dei dati e preprocessing\n\nCarico il dataset **AG News**, una raccolta di articoli di notizie che devono essere classificate in una delle quattro categorie predefinite: **World**, **Sports**, **Business**, e **Sci/Tech**.","metadata":{}},{"cell_type":"code","source":"# ottenimento del dataset\n\nag_dataset = load_dataset(\"ag_news\")\nprint(ag_dataset)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:45:43.307453Z","iopub.execute_input":"2025-03-23T20:45:43.307683Z","iopub.status.idle":"2025-03-23T20:45:47.326347Z","shell.execute_reply.started":"2025-03-23T20:45:43.307651Z","shell.execute_reply":"2025-03-23T20:45:47.325626Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0baaa38e68448b2a0535f0e7b47eb58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01ad0ae152924218a59e5880c67a9e6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c7896eb9c6e4600a50b12bb14ada87c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0ff8f56d15f44d5955d450004989cfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8facc0a9e8984b39b9b188caf1fd7785"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 120000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 7600\n    })\n})\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Divido i dati di train in training set e validation set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\nag_train_data = ag_dataset[\"train\"]\nag_test_data = ag_dataset[\"test\"]\n\nag_train_sentences, ag_val_sentences, ag_train_labels, ag_val_labels = train_test_split(\n                                                  ag_train_data['text'], \n                                                  ag_train_data['label'],\n                                                  test_size=4000, \n                                                  train_size=20000,\n                                                  random_state=42,\n                                                  shuffle=True,\n                                                  stratify=ag_train_data['label'])\n\nag_test_sentences, ag_test_labels = ag_test_data['text'], ag_test_data['label']\n\nag_train_sentences = ag_train_sentences[:10]\nag_val_sentences = ag_val_sentences[:10]\nag_test_sentences = ag_test_sentences[:10]\nag_train_labels = ag_train_labels[:10]\nag_val_labels  =ag_val_labels[:10]\nag_test_labels=ag_test_labels[:10]\n\nprint(\"Dimensioni dei set:\")\nprint(f\"Train: {len(ag_train_sentences)}\")\nprint(f\"Validation: {len(ag_val_sentences)}\")\nprint(f\"Test: {len(ag_test_sentences)}\")\n\n# Verifica distribuzione delle etichette\nprint(\"\\nDistribuzione delle etichette:\")\nprint(f\"Train: {Counter(ag_train_labels)}\")\nprint(f\"Validation: {Counter(ag_val_labels)}\")\nprint(f\"Test: {Counter(ag_test_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:45:47.327150Z","iopub.execute_input":"2025-03-23T20:45:47.327630Z","iopub.status.idle":"2025-03-23T20:45:47.984504Z","shell.execute_reply.started":"2025-03-23T20:45:47.327606Z","shell.execute_reply":"2025-03-23T20:45:47.983699Z"}},"outputs":[{"name":"stdout","text":"Dimensioni dei set:\nTrain: 10\nValidation: 10\nTest: 10\n\nDistribuzione delle etichette:\nTrain: Counter({1: 3, 2: 3, 0: 2, 3: 2})\nValidation: Counter({2: 3, 1: 3, 0: 3, 3: 1})\nTest: Counter({3: 9, 2: 1})\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass ClassificationDataset(Dataset):\n\n    def __init__(self, sentences, labels, tokenizer, max_len):\n        self.sentences = sentences\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    def __getitem__(self,index):\n        sentence = self.sentences[index]\n        label = self.labels[index]\n        \n        encoding = self.tokenizer.encode_plus(\n            sentence,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors='pt')\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding[\"token_type_ids\"].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:45:48.028746Z","iopub.execute_input":"2025-03-23T20:45:48.029010Z","iopub.status.idle":"2025-03-23T20:45:48.046639Z","shell.execute_reply.started":"2025-03-23T20:45:48.028988Z","shell.execute_reply":"2025-03-23T20:45:48.045769Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Inizializzo il Tokenizer BERT per tokenizzare le frasi e creo i dataset personalizzati.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 128\n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\nag_training_data = ClassificationDataset( sentences = ag_train_sentences,\n                           labels = ag_train_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nag_validation_data = ClassificationDataset( sentences = ag_val_sentences,\n                             labels = ag_val_labels,\n                             tokenizer = tokenizer,\n                             max_len = MAX_SEQ_LEN)\n\nag_test_data = ClassificationDataset( sentences = ag_test_sentences,\n                       labels = ag_test_labels,\n                       tokenizer = tokenizer,\n                       max_len = MAX_SEQ_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:45:48.047352Z","iopub.execute_input":"2025-03-23T20:45:48.047567Z","iopub.status.idle":"2025-03-23T20:45:52.895997Z","shell.execute_reply.started":"2025-03-23T20:45:48.047548Z","shell.execute_reply":"2025-03-23T20:45:52.895320Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5b27660697d4f5588a302cce26f014f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0e907e063ab4ebeb79afe58cb62beeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92cb5e221b64442293df2f9a493a749a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ca42491384345579c491a1ddf9790bb"}},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"### Addestramento del modello","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport time\nimport torch\nfrom sklearn.metrics import accuracy_score, f1_score\nimport torch.nn as nn\nfrom codecarbon import EmissionsTracker\n\n\n# Funzione di training e valutazione\ndef train_and_evaluate_model(model, dataset, train_loader, val_loader, optimizer, scheduler, device, epochs=10, patience=3):\n\n    os.makedirs(\"carbon_emissions\", exist_ok=True)\n    tracker = EmissionsTracker(output_dir=\"carbon_emissions\", output_file=\"emissions.csv\")  \n    tracker.start()  \n\n    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n    best_accuracy = 0\n    best_loss = float('inf')\n    patience_counter = 0  \n\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n\n        # Training\n        train_loss, train_acc = train_model(model, train_loader, optimizer, scheduler, device)\n        \n        # Valutazione\n        val_loss, val_acc, val_f1 = eval_model(model, val_loader, device)\n        \n        # Salvataggio del modello migliore\n        if val_acc > best_accuracy:\n            torch.save(model.state_dict(),  f\"{dataset}_best_model_state.bin\")\n            best_accuracy = val_acc\n\n        # Salvataggio delle metriche\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n\n        # Early stopping\n        if val_loss < best_loss:\n            best_loss = val_loss\n            patience_counter = 0 \n        else:\n            patience_counter += 1\n            print(f\"La loss sul validation set non è migliorata per {patience_counter} epoche.\")\n\n        if patience_counter >= patience:\n            print(f\"Early stopping attivato dopo {patience_counter} epoche senza miglioramenti\")\n            break\n\n    end_time = time.time()\n    total_training_time = end_time - start_time\n\n    emissions = tracker.stop()\n    print(f\"\\nEmissioni CO₂ totali: {emissions:.4f} kg\")  \n\n    return history, total_training_time, emissions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:45:52.896786Z","iopub.execute_input":"2025-03-23T20:45:52.897276Z","iopub.status.idle":"2025-03-23T20:45:53.012756Z","shell.execute_reply.started":"2025-03-23T20:45:52.897242Z","shell.execute_reply":"2025-03-23T20:45:53.012088Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Funzione di training\ndef train_model(model, data_loader, optimizer, scheduler, device):\n\n    model = model.train()\n\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    loop = tqdm(data_loader, desc=f\"Training  \", leave=True)\n\n    for batch in loop:\n\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n\n        # --- Forward pass ---\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            labels=labels \n        )\n\n        loss = outputs.loss  \n        logits = outputs.logits  \n\n        # --- Backward pass ---\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(logits, dim=1)  # Predizioni multiclasse\n\n        all_preds.extend(preds.detach().cpu().numpy())\n        all_labels.extend(labels.detach().cpu().numpy())\n\n        loop.set_postfix(loss=total_loss / (loop.n + 1), accuracy=accuracy_score(all_labels, all_preds))\n\n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    return avg_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:45:53.013495Z","iopub.execute_input":"2025-03-23T20:45:53.013729Z","iopub.status.idle":"2025-03-23T20:45:53.516233Z","shell.execute_reply.started":"2025-03-23T20:45:53.013708Z","shell.execute_reply":"2025-03-23T20:45:53.515335Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Funzione di valutazione\ndef eval_model(model, data_loader, device):\n\n    model = model.eval()\n\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        \n        loop = tqdm(data_loader, desc=f\"Evaluating\", leave=True)\n        for batch in loop:\n            \n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            token_type_ids = batch[\"token_type_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n                labels=labels\n            )\n\n            loss = outputs.loss\n            logits = outputs.logits\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(logits, dim=1)\n\n            all_preds.extend(preds.detach().cpu().numpy())\n            all_labels.extend(labels.detach().cpu().numpy())\n\n            loop.set_postfix(loss=total_loss / (loop.n + 1), accuracy=accuracy_score(all_labels, all_preds))\n\n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n    \n    return avg_loss, accuracy, f1  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:45:53.517051Z","iopub.execute_input":"2025-03-23T20:45:53.517324Z","iopub.status.idle":"2025-03-23T20:45:53.532085Z","shell.execute_reply.started":"2025-03-23T20:45:53.517300Z","shell.execute_reply":"2025-03-23T20:45:53.531474Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"Creo il modello con LoRA.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n\n# Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Pretrained model\nlora_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n\n# LoRA config\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=256,\n    lora_dropout=0.2,\n    target_modules=[\"query\", \"key\", \"value\"],\n    bias=\"none\",\n)\n\nlora_model = get_peft_model(lora_model, lora_config)\nlora_model.print_trainable_parameters()\n\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:45:53.533093Z","iopub.execute_input":"2025-03-23T20:45:53.533405Z","iopub.status.idle":"2025-03-23T20:46:02.602851Z","shell.execute_reply.started":"2025-03-23T20:45:53.533373Z","shell.execute_reply":"2025-03-23T20:46:02.601945Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3af26d8e800b4a6f9408fe578da7f6aa"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 884,736 || all params: 110,370,052 || trainable%: 0.8016\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): BertForSequenceClassification(\n      (bert): BertModel(\n        (embeddings): BertEmbeddings(\n          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (token_type_embeddings): Embedding(2, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): BertEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (activation): Tanh()\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): Linear(in_features=768, out_features=4, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"for name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        param.requires_grad = True\n\nfor name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        print(f\"{name}: requires_grad = {param.requires_grad}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:02.603790Z","iopub.execute_input":"2025-03-23T20:46:02.604512Z","iopub.status.idle":"2025-03-23T20:46:02.611811Z","shell.execute_reply.started":"2025-03-23T20:46:02.604476Z","shell.execute_reply":"2025-03-23T20:46:02.610876Z"}},"outputs":[{"name":"stdout","text":"base_model.model.classifier.weight: requires_grad = True\nbase_model.model.classifier.bias: requires_grad = True\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Imposto i parametri principali ed effettuo l'addestramento.","metadata":{}},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 2e-4\nEPOCHS = 10\nBATCH_SIZE = 32\n\n# Creo i DataLoader\nag_train_loader = DataLoader(ag_training_data, batch_size=BATCH_SIZE, shuffle=True)\nag_val_loader = DataLoader(ag_validation_data, batch_size=BATCH_SIZE, shuffle=False)\nag_test_loader = DataLoader(ag_test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(ag_train_loader) * EPOCHS\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n\n# Scheduler\nscheduler = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:02.613015Z","iopub.execute_input":"2025-03-23T20:46:02.613277Z","iopub.status.idle":"2025-03-23T20:46:02.669760Z","shell.execute_reply.started":"2025-03-23T20:46:02.613255Z","shell.execute_reply":"2025-03-23T20:46:02.668844Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"history, total_time, emissions = train_and_evaluate_model(\n    lora_model,\"ag\", ag_train_loader, ag_val_loader, optimizer, scheduler, device, epochs=10\n) \nprint(f\"\\nBERT with LoRA Training Time: {total_time:.2f} seconds, {total_time/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:02.670712Z","iopub.execute_input":"2025-03-23T20:46:02.670933Z","iopub.status.idle":"2025-03-23T20:46:12.672942Z","shell.execute_reply.started":"2025-03-23T20:46:02.670913Z","shell.execute_reply":"2025-03-23T20:46:12.672059Z"}},"outputs":[{"name":"stderr","text":"[codecarbon INFO @ 20:46:02] [setup] RAM Tracking...\n[codecarbon INFO @ 20:46:02] [setup] CPU Tracking...\n[codecarbon WARNING @ 20:46:02] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon WARNING @ 20:46:03] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon INFO @ 20:46:03] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:46:03] [setup] GPU Tracking...\n[codecarbon INFO @ 20:46:03] Tracking Nvidia GPU via pynvml\n[codecarbon WARNING @ 20:46:03] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:46:03] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:46:03] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:46:03] >>> Tracker's metadata:\n[codecarbon INFO @ 20:46:03]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 20:46:03]   Python version: 3.10.12\n[codecarbon INFO @ 20:46:03]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 20:46:03]   Available RAM : 31.351 GB\n[codecarbon INFO @ 20:46:03]   CPU count: 4\n[codecarbon INFO @ 20:46:03]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:46:03]   GPU count: 1\n[codecarbon INFO @ 20:46:03]   GPU model: 1 x Tesla P100-PCIE-16GB\n[codecarbon INFO @ 20:46:07] Saving emissions data to file /kaggle/working/carbon_emissions/emissions.csv\n[codecarbon WARNING @ 20:46:07] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  1.14it/s, accuracy=0.2, loss=1.46]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 16.90it/s, accuracy=0.2, loss=1.39]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  8.72it/s, accuracy=0.1, loss=1.44]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 17.90it/s, accuracy=0.4, loss=1.34]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.21it/s, accuracy=0.3, loss=1.31]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 18.20it/s, accuracy=0.8, loss=1.3]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.07it/s, accuracy=0.4, loss=1.31]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 16.83it/s, accuracy=0.8, loss=1.29]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  8.63it/s, accuracy=0.6, loss=1.27]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 14.89it/s, accuracy=0.6, loss=1.28]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"\nTraining  : 100%|██████████| 1/1 [00:00<00:00,  8.21it/s, accuracy=0.7, loss=1.22]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 17.89it/s, accuracy=0.6, loss=1.26]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.20it/s, accuracy=0.7, loss=1.17]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 18.00it/s, accuracy=0.6, loss=1.25]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.24it/s, accuracy=0.9, loss=1.09]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 18.31it/s, accuracy=0.6, loss=1.25]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.21it/s, accuracy=0.8, loss=1.13]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 17.41it/s, accuracy=0.6, loss=1.25]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  8.41it/s, accuracy=0.6, loss=1.15]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 14.93it/s, accuracy=0.6, loss=1.25]\n[codecarbon INFO @ 20:46:12] Energy consumed for RAM : 0.000018 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 20:46:12] Energy consumed for all CPUs : 0.000066 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 20:46:12] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:46:12] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 20:46:12] 0.000084 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"\nEmissioni CO₂ totali: 0.0000 kg\n\nBERT with LoRA Training Time: 5.58 seconds, 0.09 minutes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/codecarbon/output_methods/file.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### Valutazione del modello\nValuto il modello calcolando la loss sul test set, l'accuracy e l'F1-score.","metadata":{}},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"ag_best_model_state.bin\"))\n\ntest_loss, test_acc, test_f1 = eval_model(lora_model, ag_test_loader, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:12.674008Z","iopub.execute_input":"2025-03-23T20:46:12.674339Z","iopub.status.idle":"2025-03-23T20:46:13.139269Z","shell.execute_reply.started":"2025-03-23T20:46:12.674305Z","shell.execute_reply":"2025-03-23T20:46:13.138398Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-20-483d14fa905a>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_model.load_state_dict(torch.load(\"ag_best_model_state.bin\"))\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 16.27it/s, accuracy=0.6, loss=1.32]","output_type":"stream"},{"name":"stdout","text":"LoRA Fine-Tuning - Test loss: 1.3153, Accuracy: 0.6000, F1 score: 0.7095\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"model_performance = []\n\n# Funzione per memorizzare le performance sul task appena addestrato\ndef add_task_results(task_name, training_time, emissions, test_loss, test_acc, test_f1):\n    model_performance.append({\n        \"Task\": task_name,\n        \"Training Time\": training_time,\n        \"CO2 Emissions\": emissions,\n        \"Test Loss\": test_loss,\n        \"Accuracy\": test_acc,\n        \"F1 Score\": test_f1,\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:13.140104Z","iopub.execute_input":"2025-03-23T20:46:13.140354Z","iopub.status.idle":"2025-03-23T20:46:13.276314Z","shell.execute_reply.started":"2025-03-23T20:46:13.140320Z","shell.execute_reply":"2025-03-23T20:46:13.275487Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Memorizzazione dei risultati su Sentiment140\nadd_task_results(\n    task_name=\"agnews\", \n    training_time=total_time,\n    emissions=emissions,\n    test_loss=test_loss,\n    test_acc=test_acc,\n    test_f1=test_f1,\n)\n\nperformance = pd.DataFrame(model_performance)\nprint(performance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:13.277361Z","iopub.execute_input":"2025-03-23T20:46:13.277656Z","iopub.status.idle":"2025-03-23T20:46:13.297908Z","shell.execute_reply.started":"2025-03-23T20:46:13.277616Z","shell.execute_reply":"2025-03-23T20:46:13.297192Z"}},"outputs":[{"name":"stdout","text":"     Task  Training Time  CO2 Emissions  Test Loss  Accuracy  F1 Score\n0  agnews       5.584955       0.000012    1.31526       0.6  0.709524\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### Salvataggio dell'adapter LoRA","metadata":{}},{"cell_type":"code","source":"lora_model.save_pretrained(\"ag_lora_adapter\")\n\nclassifier_state_dict = {\n    \"classifier.weight\": lora_model.base_model.model.classifier.weight.cpu(),\n    \"classifier.bias\": lora_model.base_model.model.classifier.bias.cpu()\n}\n\ntorch.save(classifier_state_dict, \"ag_classifier_head.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:13.302568Z","iopub.execute_input":"2025-03-23T20:46:13.302850Z","iopub.status.idle":"2025-03-23T20:46:13.311241Z","shell.execute_reply.started":"2025-03-23T20:46:13.302828Z","shell.execute_reply":"2025-03-23T20:46:13.310471Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# from peft import PeftModel\n\n# base_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n# lora_model = PeftModel.from_pretrained(base_model, \"ag_lora_adapter\")\n\n# classifier_state_dict = torch.load(\"ag_classifier_head.pth\", map_location=device, weights_only=True)\n\n# lora_model.base_model.classifier.weight.data.copy_(classifier_state_dict[\"classifier.weight\"])\n# lora_model.base_model.classifier.bias.data.copy_(classifier_state_dict[\"classifier.bias\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:13.313256Z","iopub.execute_input":"2025-03-23T20:46:13.313483Z","iopub.status.idle":"2025-03-23T20:46:13.326300Z","shell.execute_reply.started":"2025-03-23T20:46:13.313455Z","shell.execute_reply":"2025-03-23T20:46:13.325525Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"## SST-2","metadata":{}},{"cell_type":"markdown","source":"### Ottenimento dei dati e preprocessing\n\nCarico il dataset SST-2, un dataset contenente esempi che consistono in frasi tratte da recensioni di film le cui etichette sono 1 se la recensione positiva, 0 altrimenti.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nsst_dataset = load_dataset('glue','sst2')\nprint(sst_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:13.327023Z","iopub.execute_input":"2025-03-23T20:46:13.327251Z","iopub.status.idle":"2025-03-23T20:46:22.250210Z","shell.execute_reply.started":"2025-03-23T20:46:13.327231Z","shell.execute_reply":"2025-03-23T20:46:22.249376Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78dad59c864c451a9e46e7953fd31206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f29a05bb91f49f4aa0d46f581e1a060"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ac05f326fe4a06889947d3b1ac5f70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae99d077757e484eaa569f41634de82a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6134d262e2bb4ac6bc91d3c5333c2d55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46057b615f25467fb9e0c2a2c3c048c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"638f896ca237425b8dea59b476a18bbe"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 67349\n    })\n    validation: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 872\n    })\n    test: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 1821\n    })\n})\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\nsst_data = sst_dataset['train'].shuffle(seed=42)\n\nsst_temp_sentences, sst_test_sentences, sst_temp_labels, sst_test_labels = train_test_split(\n                                                  sst_data['sentence'], \n                                                  sst_data['label'], \n                                                  test_size=4000, \n                                                  random_state=42,\n                                                  stratify=sst_data['label'])\n\nsst_train_sentences, sst_val_sentences, sst_train_labels, sst_val_labels = train_test_split(\n                                                  sst_temp_sentences, \n                                                  sst_temp_labels,\n                                                  train_size=20000,\n                                                  test_size=4000, \n                                                  random_state=42,\n                                                  stratify=sst_temp_labels)\n\nsst_train_sentences = sst_train_sentences[:10]\nsst_val_sentences = sst_val_sentences[:10]\nsst_test_sentences = sst_test_sentences[:10]\nsst_train_labels = sst_train_labels[:10]\nsst_val_labels  =sst_val_labels[:10]\nsst_test_labels=sst_test_labels[:10]\n\nprint(\"Dimensioni dei set:\")\nprint(f\"Train: {len(sst_train_sentences)}\")\nprint(f\"Validation: {len(sst_val_sentences)}\")\nprint(f\"Test: {len(sst_test_sentences)}\")\n\n# Verifica distribuzione delle etichette\nprint(\"\\nDistribuzione delle etichette:\")\nprint(f\"Train: {Counter(sst_train_labels)}\")\nprint(f\"Validation: {Counter(sst_val_labels)}\")\nprint(f\"Test: {Counter(sst_test_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:22.251348Z","iopub.execute_input":"2025-03-23T20:46:22.251746Z","iopub.status.idle":"2025-03-23T20:46:23.441815Z","shell.execute_reply.started":"2025-03-23T20:46:22.251704Z","shell.execute_reply":"2025-03-23T20:46:23.441089Z"}},"outputs":[{"name":"stdout","text":"Dimensioni dei set:\nTrain: 10\nValidation: 10\nTest: 10\n\nDistribuzione delle etichette:\nTrain: Counter({0: 6, 1: 4})\nValidation: Counter({0: 5, 1: 5})\nTest: Counter({1: 7, 0: 3})\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"Inizializzo il Tokenizer BERT per tokenizzare le frasi e creo i dataset personalizzati.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 128\n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\nsst_training_data = ClassificationDataset(sentences = sst_train_sentences,\n                           labels = sst_train_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nsst_validation_data = ClassificationDataset(sentences = sst_val_sentences,\n                           labels = sst_val_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nsst_test_data = ClassificationDataset(sentences = sst_test_sentences,\n                           labels = sst_test_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:23.466369Z","iopub.execute_input":"2025-03-23T20:46:23.466567Z","iopub.status.idle":"2025-03-23T20:46:23.614878Z","shell.execute_reply.started":"2025-03-23T20:46:23.466549Z","shell.execute_reply":"2025-03-23T20:46:23.614126Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### Addestramento del modello","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n\n# Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Pretrained model\nlora_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n\n# LoRA config\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=128,\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"key\", \"value\"], \n    bias=\"none\",\n)\n\nlora_model = get_peft_model(lora_model, lora_config)\nlora_model.print_trainable_parameters()\n\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:23.615705Z","iopub.execute_input":"2025-03-23T20:46:23.615980Z","iopub.status.idle":"2025-03-23T20:46:24.290781Z","shell.execute_reply.started":"2025-03-23T20:46:23.615949Z","shell.execute_reply":"2025-03-23T20:46:24.290016Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,769,472 || all params: 111,253,250 || trainable%: 1.5905\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): BertForSequenceClassification(\n      (bert): BertModel(\n        (embeddings): BertEmbeddings(\n          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (token_type_embeddings): Embedding(2, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): BertEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (activation): Tanh()\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): Linear(in_features=768, out_features=2, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"for name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        param.requires_grad = True\n\nfor name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        print(f\"{name}: requires_grad = {param.requires_grad}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:24.291451Z","iopub.execute_input":"2025-03-23T20:46:24.291660Z","iopub.status.idle":"2025-03-23T20:46:24.298934Z","shell.execute_reply.started":"2025-03-23T20:46:24.291642Z","shell.execute_reply":"2025-03-23T20:46:24.298185Z"}},"outputs":[{"name":"stdout","text":"base_model.model.classifier.weight: requires_grad = True\nbase_model.model.classifier.bias: requires_grad = True\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 5e-4\nEPOCHS = 10\nBATCH_SIZE = 32\n\n# Creo i DataLoader\nsst_train_loader = DataLoader(sst_training_data, batch_size=BATCH_SIZE, shuffle=True)\nsst_val_loader = DataLoader(sst_validation_data, batch_size=BATCH_SIZE, shuffle=False)\nsst_test_loader = DataLoader(sst_test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(sst_train_loader) * EPOCHS\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n# Scheduler\nscheduler = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:24.299727Z","iopub.execute_input":"2025-03-23T20:46:24.299987Z","iopub.status.idle":"2025-03-23T20:46:24.318417Z","shell.execute_reply.started":"2025-03-23T20:46:24.299957Z","shell.execute_reply":"2025-03-23T20:46:24.317726Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"history, total_time, emissions = train_and_evaluate_model(\n    lora_model,\"sst\", sst_train_loader, sst_val_loader, optimizer, scheduler, device, epochs=EPOCHS\n) \nprint(f\"\\nBERT with LoRA Training Time: {total_time:.2f} seconds, {total_time/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:24.319331Z","iopub.execute_input":"2025-03-23T20:46:24.319582Z","iopub.status.idle":"2025-03-23T20:46:30.011205Z","shell.execute_reply.started":"2025-03-23T20:46:24.319547Z","shell.execute_reply":"2025-03-23T20:46:30.010506Z"}},"outputs":[{"name":"stderr","text":"[codecarbon INFO @ 20:46:24] [setup] RAM Tracking...\n[codecarbon INFO @ 20:46:24] [setup] CPU Tracking...\n[codecarbon WARNING @ 20:46:24] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon WARNING @ 20:46:25] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon INFO @ 20:46:25] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:46:25] [setup] GPU Tracking...\n[codecarbon INFO @ 20:46:25] Tracking Nvidia GPU via pynvml\n[codecarbon WARNING @ 20:46:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:46:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:46:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:46:25] >>> Tracker's metadata:\n[codecarbon INFO @ 20:46:25]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 20:46:25]   Python version: 3.10.12\n[codecarbon INFO @ 20:46:25]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 20:46:25]   Available RAM : 31.351 GB\n[codecarbon INFO @ 20:46:25]   CPU count: 4\n[codecarbon INFO @ 20:46:25]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:46:25]   GPU count: 1\n[codecarbon INFO @ 20:46:25]   GPU model: 1 x Tesla P100-PCIE-16GB\n[codecarbon INFO @ 20:46:28] Saving emissions data to file /kaggle/working/carbon_emissions/emissions.csv\n[codecarbon WARNING @ 20:46:28] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  8.95it/s, accuracy=0.6, loss=0.699]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.76it/s, accuracy=0.5, loss=0.685]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.66it/s, accuracy=0.5, loss=0.685]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 20.36it/s, accuracy=0.5, loss=0.697]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 1 epoche.\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.70it/s, accuracy=0.6, loss=0.665]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 20.13it/s, accuracy=0.5, loss=0.692]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 2 epoche.\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.63it/s, accuracy=0.6, loss=0.603]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 20.17it/s, accuracy=0.5, loss=0.687]\n[codecarbon INFO @ 20:46:29] Energy consumed for RAM : 0.000004 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 20:46:29] Energy consumed for all CPUs : 0.000016 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 20:46:29] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:46:30] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 20:46:30] 0.000020 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 3 epoche.\nEarly stopping attivato dopo 3 epoche senza miglioramenti\n\nEmissioni CO₂ totali: 0.0000 kg\n\nBERT with LoRA Training Time: 1.32 seconds, 0.02 minutes.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"### 3. Valutazione del modello\nValuto i modello calcolando la loss sul test set, l'accuracy e l'F1-score.","metadata":{}},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"sst_best_model_state.bin\"))\n\ntest_loss, test_acc, test_f1 = eval_model(lora_model, sst_test_loader, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.012028Z","iopub.execute_input":"2025-03-23T20:46:30.012295Z","iopub.status.idle":"2025-03-23T20:46:30.375252Z","shell.execute_reply.started":"2025-03-23T20:46:30.012261Z","shell.execute_reply":"2025-03-23T20:46:30.374083Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-34-cc014844e092>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_model.load_state_dict(torch.load(\"sst_best_model_state.bin\"))\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 20.40it/s, accuracy=0.3, loss=0.742]","output_type":"stream"},{"name":"stdout","text":"LoRA Fine-Tuning - Test loss: 0.7416, Accuracy: 0.3000, F1 score: 0.1385\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Memorizzazione dei risultati su Sentiment140\nadd_task_results(\n    task_name=\"sst\", \n    training_time=total_time,\n    emissions=emissions,\n    test_loss=test_loss,\n    test_acc=test_acc,\n    test_f1=test_f1,\n)\n\nperformance = pd.DataFrame(model_performance)\nprint(performance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.376237Z","iopub.execute_input":"2025-03-23T20:46:30.376555Z","iopub.status.idle":"2025-03-23T20:46:30.384599Z","shell.execute_reply.started":"2025-03-23T20:46:30.376521Z","shell.execute_reply":"2025-03-23T20:46:30.383871Z"}},"outputs":[{"name":"stdout","text":"     Task  Training Time  CO2 Emissions  Test Loss  Accuracy  F1 Score\n0  agnews       5.584955       0.000012   1.315260       0.6  0.709524\n1     sst       1.320940       0.000003   0.741614       0.3  0.138462\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"#### 4. Salvataggio del modulo lora","metadata":{}},{"cell_type":"code","source":"lora_model.save_pretrained(\"sst_lora_adapter\")\n\nclassifier_state_dict = {\n    \"classifier.weight\": lora_model.base_model.model.classifier.weight.cpu(),\n    \"classifier.bias\": lora_model.base_model.model.classifier.bias.cpu()\n}\n\ntorch.save(classifier_state_dict, \"sst_classifier_head.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.385355Z","iopub.execute_input":"2025-03-23T20:46:30.385624Z","iopub.status.idle":"2025-03-23T20:46:30.401461Z","shell.execute_reply.started":"2025-03-23T20:46:30.385591Z","shell.execute_reply":"2025-03-23T20:46:30.400597Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# from peft import PeftModel\n\n# base_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n# lora_model = PeftModel.from_pretrained(base_model, \"sst_lora_adapter\")\n\n# classifier_state_dict = torch.load(\"sst_classifier_head.pth\", map_location=device, weights_only=True)\n\n\n# lora_model.base_model.classifier.weight.data.copy_(classifier_state_dict[\"classifier.weight\"])\n# lora_model.base_model.classifier.bias.data.copy_(classifier_state_dict[\"classifier.bias\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.402235Z","iopub.execute_input":"2025-03-23T20:46:30.402551Z","iopub.status.idle":"2025-03-23T20:46:30.416342Z","shell.execute_reply.started":"2025-03-23T20:46:30.402521Z","shell.execute_reply":"2025-03-23T20:46:30.415588Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"## EmoInt","metadata":{}},{"cell_type":"code","source":"def load_emoint_dataset(file_path):\n    label_map = {\"anger\": 0, \"joy\": 1, \"sadness\": 2, \"fear\": 3}\n    \n    df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"id\", \"sentence\", \"label\", \"intensity\"])\n    \n    df = df[[\"sentence\", \"label\"]]\n    df[\"label\"] = df[\"label\"].map(label_map)\n    \n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    return df\n\n\nei_dataset = load_emoint_dataset(\"/kaggle/input/emoint-dataset/Emotion Intensity Dataset.txt\")\nprint(ei_dataset.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.417271Z","iopub.execute_input":"2025-03-23T20:46:30.417560Z","iopub.status.idle":"2025-03-23T20:46:30.488647Z","shell.execute_reply.started":"2025-03-23T20:46:30.417530Z","shell.execute_reply":"2025-03-23T20:46:30.487874Z"}},"outputs":[{"name":"stdout","text":"                                            sentence  label\n0  what does everyone have against sparkling wate...      1\n1  Or when they hmu on snap, and I'm like.. which...      0\n2  Can we get a shot of Lingys face at 1/4 time ?...      0\n3  I stepped into the shower and my spidey senses...      3\n4  @AaliyahLove69 I would be intimidated but I wo...      3\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\n# Divido i dati in training set, validation set e test set\nei_temp_sentences, ei_test_sentences, ei_temp_labels, ei_test_labels = train_test_split(\n                                                ei_dataset['sentence'],\n                                                ei_dataset['label'], \n                                                test_size=0.1, \n                                                random_state=42,\n                                                stratify=ei_dataset['label'])\n\nei_train_sentences, ei_val_sentences, ei_train_labels, ei_val_labels = train_test_split(\n                                                ei_temp_sentences,\n                                                ei_temp_labels,\n                                                test_size=0.1111,\n                                                random_state=42,\n                                                stratify=ei_temp_labels)\n\nei_train_sentences = ei_train_sentences.reset_index(drop=True)\nei_val_sentences = ei_val_sentences.reset_index(drop=True)\nei_test_sentences = ei_test_sentences.reset_index(drop=True)\nei_train_labels = ei_train_labels.reset_index(drop=True)\nei_val_labels = ei_val_labels.reset_index(drop=True)\nei_test_labels = ei_test_labels.reset_index(drop=True)\n\n\nei_train_sentences = ei_train_sentences[:10]\nei_val_sentences = ei_val_sentences[:10]\nei_test_sentences = ei_test_sentences[:10]\nei_train_labels = ei_train_labels[:10]\nei_val_labels  =ei_val_labels[:10]\nei_test_labels=ei_test_labels[:10]\n\nprint(\"Dimensioni dei set:\")\nprint(f\"Train: {len(ei_train_sentences)}\")\nprint(f\"Validation: {len(ei_val_sentences)}\")\nprint(f\"Test: {len(ei_test_sentences)}\")\n\n# Verifica distribuzione delle etichette\nprint(\"\\nDistribuzione delle etichette:\")\nprint(f\"Train: {Counter(ei_train_labels)}\")\nprint(f\"Validation: {Counter(ei_val_labels)}\")\nprint(f\"Test: {Counter(ei_test_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.493969Z","iopub.execute_input":"2025-03-23T20:46:30.494281Z","iopub.status.idle":"2025-03-23T20:46:30.519201Z","shell.execute_reply.started":"2025-03-23T20:46:30.494249Z","shell.execute_reply":"2025-03-23T20:46:30.518062Z"}},"outputs":[{"name":"stdout","text":"Dimensioni dei set:\nTrain: 10\nValidation: 10\nTest: 10\n\nDistribuzione delle etichette:\nTrain: Counter({1: 4, 3: 3, 0: 2, 2: 1})\nValidation: Counter({0: 4, 1: 3, 2: 2, 3: 1})\nTest: Counter({0: 3, 2: 3, 1: 3, 3: 1})\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Mappatura delle etichette numeriche in stringhe\nlabel_map = {\n    0: \"anger\",\n    1: \"joy\",\n    2: \"sadness\",\n    3: \"fear\"\n}\n\n# Conta la distribuzione delle etichette nel training set\ntrain_label_counts = Counter(ei_train_labels)\n\n# Converti le etichette numeriche in stringhe\nlabels = [label_map[label] for label in sorted(train_label_counts.keys())]\ncounts = [train_label_counts[label] for label in sorted(train_label_counts.keys())]\n\n# Creazione del grafico\nplt.figure(figsize=(5, 4))\nplt.bar(labels, counts, color='royalblue', width=0.5)  \nplt.xlabel(\"Classi\")\nplt.ylabel(\"Frequenza\")\nplt.title(\"Distribuzione delle etichette nel Training Set\")\nplt.xticks(labels, rotation=0, ha=\"center\")  # Imposta il testo orizzontale e centrato\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Mostra il grafico\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.520231Z","iopub.execute_input":"2025-03-23T20:46:30.520531Z","iopub.status.idle":"2025-03-23T20:46:30.524444Z","shell.execute_reply.started":"2025-03-23T20:46:30.520500Z","shell.execute_reply":"2025-03-23T20:46:30.523501Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 128\n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\nei_training_data = ClassificationDataset(sentences = ei_train_sentences,\n                           labels = ei_train_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nei_validation_data = ClassificationDataset(sentences = ei_val_sentences,\n                           labels = ei_val_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nei_test_data = ClassificationDataset(sentences = ei_test_sentences,\n                           labels = ei_test_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.525478Z","iopub.execute_input":"2025-03-23T20:46:30.525795Z","iopub.status.idle":"2025-03-23T20:46:30.685386Z","shell.execute_reply.started":"2025-03-23T20:46:30.525764Z","shell.execute_reply":"2025-03-23T20:46:30.684574Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n\n# Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Pretrained model\nlora_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n\n# LoRA config\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=512,\n    lora_dropout=0.3,\n    target_modules=[\"query\", \"key\", \"value\"],  \n    bias=\"none\",\n)\n\nlora_model = get_peft_model(lora_model, lora_config)\nlora_model.print_trainable_parameters()\n\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.686206Z","iopub.execute_input":"2025-03-23T20:46:30.686507Z","iopub.status.idle":"2025-03-23T20:46:31.140881Z","shell.execute_reply.started":"2025-03-23T20:46:30.686473Z","shell.execute_reply":"2025-03-23T20:46:31.140015Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,769,472 || all params: 111,254,788 || trainable%: 1.5905\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): BertForSequenceClassification(\n      (bert): BertModel(\n        (embeddings): BertEmbeddings(\n          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (token_type_embeddings): Embedding(2, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): BertEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.3, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.3, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.3, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (activation): Tanh()\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): Linear(in_features=768, out_features=4, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"for name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        param.requires_grad = True\n\nfor name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        print(f\"{name}: requires_grad = {param.requires_grad}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:31.141826Z","iopub.execute_input":"2025-03-23T20:46:31.142166Z","iopub.status.idle":"2025-03-23T20:46:31.149418Z","shell.execute_reply.started":"2025-03-23T20:46:31.142132Z","shell.execute_reply":"2025-03-23T20:46:31.148596Z"}},"outputs":[{"name":"stdout","text":"base_model.model.classifier.weight: requires_grad = True\nbase_model.model.classifier.bias: requires_grad = True\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 1e-4\nEPOCHS = 10\nBATCH_SIZE = 32\n\n# Creo i DataLoader\nei_train_loader = DataLoader(ei_training_data, batch_size=BATCH_SIZE, shuffle=True)\nei_val_loader = DataLoader(ei_validation_data, batch_size=BATCH_SIZE, shuffle=False)\nei_test_loader = DataLoader(ei_test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(ei_train_loader) * EPOCHS\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n# Scheduler\nscheduler = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0.2 * total_steps,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:31.150227Z","iopub.execute_input":"2025-03-23T20:46:31.150518Z","iopub.status.idle":"2025-03-23T20:46:31.168546Z","shell.execute_reply.started":"2025-03-23T20:46:31.150487Z","shell.execute_reply":"2025-03-23T20:46:31.167878Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"history, total_time, emissions = train_and_evaluate_model(\n    lora_model,\"ei\", ei_train_loader, ei_val_loader, optimizer, scheduler, device, epochs=EPOCHS\n) \nprint(f\"\\nBERT with LoRA Training Time: {total_time:.2f} seconds, {total_time/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:31.169441Z","iopub.execute_input":"2025-03-23T20:46:31.169710Z","iopub.status.idle":"2025-03-23T20:46:40.227369Z","shell.execute_reply.started":"2025-03-23T20:46:31.169680Z","shell.execute_reply":"2025-03-23T20:46:40.226702Z"}},"outputs":[{"name":"stderr","text":"[codecarbon INFO @ 20:46:31] [setup] RAM Tracking...\n[codecarbon INFO @ 20:46:31] [setup] CPU Tracking...\n[codecarbon WARNING @ 20:46:31] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon WARNING @ 20:46:32] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon INFO @ 20:46:32] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:46:32] [setup] GPU Tracking...\n[codecarbon INFO @ 20:46:32] Tracking Nvidia GPU via pynvml\n[codecarbon WARNING @ 20:46:32] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:46:32] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:46:32] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:46:32] >>> Tracker's metadata:\n[codecarbon INFO @ 20:46:32]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 20:46:32]   Python version: 3.10.12\n[codecarbon INFO @ 20:46:32]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 20:46:32]   Available RAM : 31.351 GB\n[codecarbon INFO @ 20:46:32]   CPU count: 4\n[codecarbon INFO @ 20:46:32]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:46:32]   GPU count: 1\n[codecarbon INFO @ 20:46:32]   GPU model: 1 x Tesla P100-PCIE-16GB\n[codecarbon INFO @ 20:46:35] Saving emissions data to file /kaggle/working/carbon_emissions/emissions.csv\n[codecarbon WARNING @ 20:46:35] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.57it/s, accuracy=0.2, loss=1.43]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.66it/s, accuracy=0.1, loss=1.6]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.51it/s, accuracy=0.3, loss=1.39]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.62it/s, accuracy=0.1, loss=1.57]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.59it/s, accuracy=0.5, loss=1.24]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.41it/s, accuracy=0.2, loss=1.55]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.54it/s, accuracy=0.5, loss=1.26]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.64it/s, accuracy=0.2, loss=1.52]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.54it/s, accuracy=0.5, loss=1.23]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.71it/s, accuracy=0.2, loss=1.51]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.61it/s, accuracy=0.7, loss=1.17]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.38it/s, accuracy=0.3, loss=1.5]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.43it/s, accuracy=0.7, loss=1.17]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.15it/s, accuracy=0.3, loss=1.49]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.55it/s, accuracy=0.7, loss=1.15]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.52it/s, accuracy=0.3, loss=1.49]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.66it/s, accuracy=0.6, loss=1.14]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.33it/s, accuracy=0.3, loss=1.48]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.61it/s, accuracy=0.5, loss=1.15]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.62it/s, accuracy=0.3, loss=1.48]\n[codecarbon INFO @ 20:46:40] Energy consumed for RAM : 0.000015 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 20:46:40] Energy consumed for all CPUs : 0.000054 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 20:46:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:46:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 20:46:40] 0.000069 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"\nEmissioni CO₂ totali: 0.0000 kg\n\nBERT with LoRA Training Time: 4.55 seconds, 0.08 minutes.\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"ei_best_model_state.bin\")) \n\ntest_loss, test_acc, test_f1 = eval_model(lora_model, ei_test_loader, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:40.228292Z","iopub.execute_input":"2025-03-23T20:46:40.228605Z","iopub.status.idle":"2025-03-23T20:46:40.592710Z","shell.execute_reply.started":"2025-03-23T20:46:40.228574Z","shell.execute_reply":"2025-03-23T20:46:40.591893Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-47-52a61a41723e>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_model.load_state_dict(torch.load(\"ei_best_model_state.bin\"))\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.48it/s, accuracy=0.2, loss=1.51]","output_type":"stream"},{"name":"stdout","text":"LoRA Fine-Tuning - Test loss: 1.5141, Accuracy: 0.2000, F1 score: 0.1091\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# Memorizzazione dei risultati su Sentiment140\nadd_task_results(\n    task_name=\"ei\", \n    training_time=total_time,\n    emissions=emissions,\n    test_loss=test_loss,\n    test_acc=test_acc,\n    test_f1=test_f1,\n)\n\nperformance = pd.DataFrame(model_performance)\nprint(performance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:40.593515Z","iopub.execute_input":"2025-03-23T20:46:40.593780Z","iopub.status.idle":"2025-03-23T20:46:40.601044Z","shell.execute_reply.started":"2025-03-23T20:46:40.593758Z","shell.execute_reply":"2025-03-23T20:46:40.600266Z"}},"outputs":[{"name":"stdout","text":"     Task  Training Time  CO2 Emissions  Test Loss  Accuracy  F1 Score\n0  agnews       5.584955       0.000012   1.315260       0.6  0.709524\n1     sst       1.320940       0.000003   0.741614       0.3  0.138462\n2      ei       4.548648       0.000010   1.514121       0.2  0.109091\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"lora_model.save_pretrained(\"ei_lora_adapter\")\n\nclassifier_state_dict = {\n    \"classifier.weight\": lora_model.base_model.model.classifier.weight.cpu(),\n    \"classifier.bias\": lora_model.base_model.model.classifier.bias.cpu()\n}\n\ntorch.save(classifier_state_dict, \"ei_classifier_head.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:40.601920Z","iopub.execute_input":"2025-03-23T20:46:40.602195Z","iopub.status.idle":"2025-03-23T20:46:40.632599Z","shell.execute_reply.started":"2025-03-23T20:46:40.602171Z","shell.execute_reply":"2025-03-23T20:46:40.631817Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# from peft import PeftModel\n\n# base_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n# lora_model = PeftModel.from_pretrained(base_model, \"ei__lora_adapter\")\n\n# classifier_state_dict = torch.load(\"ei_classifier_head.pth\", map_location=device, weights_only=True)\n\n\n# lora_model.base_model.classifier.weight.data.copy_(classifier_state_dict[\"classifier.weight\"])\n# lora_model.base_model.classifier.bias.data.copy_(classifier_state_dict[\"classifier.bias\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:40.633525Z","iopub.execute_input":"2025-03-23T20:46:40.633811Z","iopub.status.idle":"2025-03-23T20:46:40.650396Z","shell.execute_reply.started":"2025-03-23T20:46:40.633781Z","shell.execute_reply":"2025-03-23T20:46:40.649263Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"## MNLI","metadata":{}},{"cell_type":"markdown","source":"### Ottenimento del dataset e preprocessing","metadata":{}},{"cell_type":"markdown","source":"Carico il dataset MNLI, che contiene esempi che consistono in una coppia di frasi (premessa e ipotesi) etichettate con **Entailment**, **Contradiction**, **Neutral**.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n\n# Carico il datast\nmnli_dataset = load_dataset('glue', 'mnli')\nprint(mnli_dataset)\n\n# Divido i dati in training set, validation set e test set\nmnli_data = mnli_dataset['train'].shuffle(seed=42)\n\nmnli_temp_premises, mnli_test_premises, mnli_temp_hypotheses, mnli_test_hypotheses, mnli_temp_labels, mnli_test_labels = train_test_split(\n                                                  mnli_data['premise'], \n                                                  mnli_data['hypothesis'],                \n                                                  mnli_data['label'], \n                                                  test_size=3000, \n                                                  random_state=42,\n                                                  stratify=mnli_data['label'])\n\nmnli_train_premises, mnli_val_premises, mnli_train_hypotheses, mnli_val_hypotheses, mnli_train_labels, mnli_val_labels = train_test_split(\n                                                  mnli_temp_premises, \n                                                  mnli_temp_hypotheses,\n                                                  mnli_temp_labels,\n                                                  train_size=45000,\n                                                  test_size=3000, \n                                                  random_state=42,\n                                                  stratify=mnli_temp_labels)\n\nmnli_train_premises = mnli_train_premises[:10]\nmnli_val_premises = mnli_val_premises[:10]\nmnli_test_premises = mnli_test_premises[:10]\nmnli_train_hypotheses = mnli_train_hypotheses[:10]\nmnli_val_hypotheses = mnli_val_hypotheses[:10]\nmnli_test_hypotheses = mnli_test_hypotheses[:10]\nmnli_train_labels = mnli_train_labels[:10]\nmnli_val_labels  = mnli_val_labels[:10]\nmnli_test_labels=mnli_test_labels[:10]\n\nprint(\"Dimensioni dei set:\")\nprint(f\"Train: {len(mnli_train_premises)}\")\nprint(f\"Validation: {len(mnli_val_premises)}\")\nprint(f\"Test: {len(mnli_test_premises)}\")\n\n# Verifica distribuzione delle etichette\nprint(\"\\nDistribuzione delle etichette:\")\nprint(f\"Train: {Counter(mnli_train_labels)}\")\nprint(f\"Validation: {Counter(mnli_val_labels)}\")\nprint(f\"Test: {Counter(mnli_test_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:40.651093Z","iopub.execute_input":"2025-03-23T20:46:40.651385Z","iopub.status.idle":"2025-03-23T20:46:54.356343Z","shell.execute_reply.started":"2025-03-23T20:46:40.651358Z","shell.execute_reply":"2025-03-23T20:46:54.355548Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/52.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02b5141e1ce841bbb536abde8102c20f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)alidation_matched-00000-of-00001.parquet:   0%|          | 0.00/1.21M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60dee2e4df0e44828e3b4c871c03f891"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)dation_mismatched-00000-of-00001.parquet:   0%|          | 0.00/1.25M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c72095d2b0594d7c91848514a1b6c36b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test_matched-00000-of-00001.parquet:   0%|          | 0.00/1.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d382f5b4957c46a4929ac7a93d1a994c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test_mismatched-00000-of-00001.parquet:   0%|          | 0.00/1.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"252c9693af804d08a2ce731ba92d420a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b85ce3a6f76c4c9baf85504043f8bf55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51beddfc54644e28b242e4d6b946c22a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b13a65641484e23a2f1e0a8b2184113"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_matched split:   0%|          | 0/9796 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23254bca5eee41fdb76b302c5a4d9bfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_mismatched split:   0%|          | 0/9847 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9373ca4bdc648578f7d1c136c589248"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 392702\n    })\n    validation_matched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9815\n    })\n    validation_mismatched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9832\n    })\n    test_matched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9796\n    })\n    test_mismatched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9847\n    })\n})\nDimensioni dei set:\nTrain: 10\nValidation: 10\nTest: 10\n\nDistribuzione delle etichette:\nTrain: Counter({0: 5, 1: 3, 2: 2})\nValidation: Counter({1: 6, 2: 3, 0: 1})\nTest: Counter({1: 5, 2: 3, 0: 2})\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"Creo una classe Dataset personalizzata in cui viene effettuata la tokenizzaione delle recensioni e la conversione dei dati in tensori.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass NLIDataset(Dataset):\n\n    def __init__(self, premises, hypotheses , labels, tokenizer, max_len):\n        self.premises = premises\n        self.hypotheses = hypotheses\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.premises)\n    \n    def __getitem__(self,index):\n        premise = self.premises[index]\n        hyphotesis = self.hypotheses[index]\n        label = self.labels[index]\n        \n        encoding = self.tokenizer.encode_plus(\n            premise,\n            hyphotesis,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors='pt')\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding[\"token_type_ids\"].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:54.357306Z","iopub.execute_input":"2025-03-23T20:46:54.357638Z","iopub.status.idle":"2025-03-23T20:46:54.390706Z","shell.execute_reply.started":"2025-03-23T20:46:54.357603Z","shell.execute_reply":"2025-03-23T20:46:54.389909Z"}},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"Inizializzo il Tokenizer BERT per tokenizzare le frasi e creo i dataset personalizzati.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 512\n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\nmnli_training_data = NLIDataset(premises = mnli_train_premises,\n                            hypotheses = mnli_train_hypotheses,\n                            labels = mnli_train_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)\n\nmnli_validation_data = NLIDataset(premises = mnli_val_premises,\n                            hypotheses = mnli_val_hypotheses,\n                            labels = mnli_val_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)\n\nmnli_test_data = NLIDataset(premises = mnli_test_premises,\n                            hypotheses = mnli_test_hypotheses,\n                            labels = mnli_test_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:54.391846Z","iopub.execute_input":"2025-03-23T20:46:54.392195Z","iopub.status.idle":"2025-03-23T20:46:54.617729Z","shell.execute_reply.started":"2025-03-23T20:46:54.392161Z","shell.execute_reply":"2025-03-23T20:46:54.617054Z"}},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":"### Addestramento del modello","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n\n# Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Pretrained model\nlora_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n\n# LoRA config\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=128,\n    lora_dropout=0.2,\n    target_modules=[\"query\", \"key\", \"value\"], \n    bias=\"none\",\n)\n\nlora_model = get_peft_model(lora_model, lora_config)\nlora_model.print_trainable_parameters()\n\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:54.618491Z","iopub.execute_input":"2025-03-23T20:46:54.618711Z","iopub.status.idle":"2025-03-23T20:46:55.110049Z","shell.execute_reply.started":"2025-03-23T20:46:54.618691Z","shell.execute_reply":"2025-03-23T20:46:55.109190Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,769,472 || all params: 111,254,019 || trainable%: 1.5905\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): BertForSequenceClassification(\n      (bert): BertModel(\n        (embeddings): BertEmbeddings(\n          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (token_type_embeddings): Embedding(2, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): BertEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (activation): Tanh()\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): Linear(in_features=768, out_features=3, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"for name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        param.requires_grad = True\n\nfor name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        print(f\"{name}: requires_grad = {param.requires_grad}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:55.111129Z","iopub.execute_input":"2025-03-23T20:46:55.111453Z","iopub.status.idle":"2025-03-23T20:46:55.118703Z","shell.execute_reply.started":"2025-03-23T20:46:55.111419Z","shell.execute_reply":"2025-03-23T20:46:55.117858Z"}},"outputs":[{"name":"stdout","text":"base_model.model.classifier.weight: requires_grad = True\nbase_model.model.classifier.bias: requires_grad = True\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 2e-4\nEPOCHS = 10\nBATCH_SIZE = 32\n\n# Creo i DataLoader\nmnli_train_loader = DataLoader(mnli_training_data, batch_size=BATCH_SIZE, shuffle=True)\nmnli_val_loader = DataLoader(mnli_validation_data, batch_size=BATCH_SIZE, shuffle=False)\nmnli_test_loader = DataLoader(mnli_test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(mnli_train_loader) * EPOCHS\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n\n# Scheduler\nscheduler = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:55.119566Z","iopub.execute_input":"2025-03-23T20:46:55.119799Z","iopub.status.idle":"2025-03-23T20:46:55.137448Z","shell.execute_reply.started":"2025-03-23T20:46:55.119780Z","shell.execute_reply":"2025-03-23T20:46:55.136670Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"history, total_time, emissions = train_and_evaluate_model(\n    lora_model,\"mnli\", mnli_train_loader, mnli_val_loader, optimizer, scheduler, device, epochs=EPOCHS\n) \nprint(f\"\\nBERT with LoRA Training Time: {total_time:.2f} seconds, {total_time/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:55.138329Z","iopub.execute_input":"2025-03-23T20:46:55.138562Z","iopub.status.idle":"2025-03-23T20:47:02.831007Z","shell.execute_reply.started":"2025-03-23T20:46:55.138542Z","shell.execute_reply":"2025-03-23T20:47:02.830363Z"}},"outputs":[{"name":"stderr","text":"[codecarbon INFO @ 20:46:55] [setup] RAM Tracking...\n[codecarbon INFO @ 20:46:55] [setup] CPU Tracking...\n[codecarbon WARNING @ 20:46:55] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon WARNING @ 20:46:56] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon INFO @ 20:46:56] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:46:56] [setup] GPU Tracking...\n[codecarbon INFO @ 20:46:56] Tracking Nvidia GPU via pynvml\n[codecarbon WARNING @ 20:46:56] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:46:56] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:46:56] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:46:56] >>> Tracker's metadata:\n[codecarbon INFO @ 20:46:56]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 20:46:56]   Python version: 3.10.12\n[codecarbon INFO @ 20:46:56]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 20:46:56]   Available RAM : 31.351 GB\n[codecarbon INFO @ 20:46:56]   CPU count: 4\n[codecarbon INFO @ 20:46:56]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:46:56]   GPU count: 1\n[codecarbon INFO @ 20:46:56]   GPU model: 1 x Tesla P100-PCIE-16GB\n[codecarbon INFO @ 20:46:59] Saving emissions data to file /kaggle/working/carbon_emissions/emissions.csv\n[codecarbon WARNING @ 20:46:59] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  2.12it/s, accuracy=0.2, loss=1.18]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00,  5.53it/s, accuracy=0.6, loss=1.09]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  2.12it/s, accuracy=0.6, loss=0.986]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00,  5.56it/s, accuracy=0.2, loss=1.16]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 1 epoche.\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  2.12it/s, accuracy=0.4, loss=1.09]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00,  5.56it/s, accuracy=0.1, loss=1.23]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 2 epoche.\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  2.12it/s, accuracy=0.5, loss=0.972]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00,  5.52it/s, accuracy=0.1, loss=1.28]\n[codecarbon INFO @ 20:47:02] Energy consumed for RAM : 0.000011 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 20:47:02] Energy consumed for all CPUs : 0.000039 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 20:47:02] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:47:02] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 20:47:02] 0.000050 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 3 epoche.\nEarly stopping attivato dopo 3 epoche senza miglioramenti\n\nEmissioni CO₂ totali: 0.0000 kg\n\nBERT with LoRA Training Time: 3.33 seconds, 0.06 minutes.\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"### Valutazione del modello\nValuto i modello calcolando la loss sul test set, l'accuracy e l'F1-score.","metadata":{}},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"mnli_best_model_state.bin\"))\n            \ntest_loss, test_acc, test_f1 = eval_model(lora_model, mnli_test_loader, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:02.831983Z","iopub.execute_input":"2025-03-23T20:47:02.832309Z","iopub.status.idle":"2025-03-23T20:47:03.317765Z","shell.execute_reply.started":"2025-03-23T20:47:02.832270Z","shell.execute_reply":"2025-03-23T20:47:03.316881Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-58-855da658c16c>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_model.load_state_dict(torch.load(\"mnli_best_model_state.bin\"))\nEvaluating: 100%|██████████| 1/1 [00:00<00:00,  5.47it/s, accuracy=0.5, loss=1.11]","output_type":"stream"},{"name":"stdout","text":"LoRA Fine-Tuning - Test loss: 1.1082, Accuracy: 0.5000, F1 score: 0.3333\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# Memorizzazione dei risultati su Sentiment140\nadd_task_results(\n    task_name=\"mnli\", \n    training_time=total_time,\n    emissions=emissions,\n    test_loss=test_loss,\n    test_acc=test_acc,\n    test_f1=test_f1,\n)\n\nperformance = pd.DataFrame(model_performance)\nprint(performance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:03.318717Z","iopub.execute_input":"2025-03-23T20:47:03.318996Z","iopub.status.idle":"2025-03-23T20:47:03.326912Z","shell.execute_reply.started":"2025-03-23T20:47:03.318972Z","shell.execute_reply":"2025-03-23T20:47:03.326013Z"}},"outputs":[{"name":"stdout","text":"     Task  Training Time  CO2 Emissions  Test Loss  Accuracy  F1 Score\n0  agnews       5.584955       0.000012   1.315260       0.6  0.709524\n1     sst       1.320940       0.000003   0.741614       0.3  0.138462\n2      ei       4.548648       0.000010   1.514121       0.2  0.109091\n3    mnli       3.331646       0.000007   1.108231       0.5  0.333333\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"### Salvataggio dell'adapter LoRA","metadata":{}},{"cell_type":"code","source":"lora_model.save_pretrained(\"mnli_lora_adapter\")\n\nclassifier_state_dict = {\n    \"classifier.weight\": lora_model.base_model.model.classifier.weight.cpu(),\n    \"classifier.bias\": lora_model.base_model.model.classifier.bias.cpu()\n}\n\ntorch.save(classifier_state_dict, \"mnli_classifier_head.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:03.327800Z","iopub.execute_input":"2025-03-23T20:47:03.328074Z","iopub.status.idle":"2025-03-23T20:47:03.342872Z","shell.execute_reply.started":"2025-03-23T20:47:03.328040Z","shell.execute_reply":"2025-03-23T20:47:03.342077Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# from peft import PeftModel\n\n# base_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n# lora_model = PeftModel.from_pretrained(base_model, \"mnli_lora_adapter\")\n\n# classifier_state_dict = torch.load(\"mnli_classifier_head.pth\", map_location=device, weights_only=True)\n\n\n# lora_model.base_model.classifier.weight.data.copy_(classifier_state_dict[\"classifier.weight\"])\n# lora_model.base_model.classifier.bias.data.copy_(classifier_state_dict[\"classifier.bias\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:03.343723Z","iopub.execute_input":"2025-03-23T20:47:03.344014Z","iopub.status.idle":"2025-03-23T20:47:03.356242Z","shell.execute_reply.started":"2025-03-23T20:47:03.343980Z","shell.execute_reply":"2025-03-23T20:47:03.355454Z"}},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"## PAWS","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Carico il datast\npaws_dataset = load_dataset(\"google-research-datasets/paws\", \"labeled_final\")\nprint(paws_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:03.356954Z","iopub.execute_input":"2025-03-23T20:47:03.357163Z","iopub.status.idle":"2025-03-23T20:47:06.621754Z","shell.execute_reply.started":"2025-03-23T20:47:03.357144Z","shell.execute_reply":"2025-03-23T20:47:06.621035Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/9.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e62501d5a76a474eab59c833d57e1806"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/8.43M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a515e89ddfb7485eae14497fdc8cd474"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5df4c5ba93904c7295edeaf070644204"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5622dcddab5b4349a925a38272dc2179"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/49401 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79ad90d35f034ad1be530206674cec43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7893dd8b38a249c09938586f25c918b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d33a3bf3d935425faa25125949d4e137"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'sentence1', 'sentence2', 'label'],\n        num_rows: 49401\n    })\n    test: Dataset({\n        features: ['id', 'sentence1', 'sentence2', 'label'],\n        num_rows: 8000\n    })\n    validation: Dataset({\n        features: ['id', 'sentence1', 'sentence2', 'label'],\n        num_rows: 8000\n    })\n})\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\n\npaws_train_set = paws_dataset[\"train\"]\npaws_val_set = paws_dataset[\"validation\"]\npaws_test_set = paws_dataset[\"test\"]\n\npaws_train_sentences1, paws_train_sentences2, paws_train_labels = paws_train_set['sentence1'], paws_train_set['sentence2'], paws_train_set['label']\npaws_val_sentences1, paws_val_sentences2, paws_val_labels = paws_val_set['sentence1'], paws_val_set['sentence2'], paws_val_set['label']\npaws_test_sentences1, paws_test_sentences2, paws_test_labels = paws_test_set['sentence1'], paws_test_set['sentence2'], paws_test_set['label']\n\n\npaws_train_sentences1 = paws_train_sentences1[:10]\npaws_val_sentences1 = paws_val_sentences1[:10]\npaws_test_sentences1 = paws_test_sentences1[:10]\npaws_train_sentences2 = paws_train_sentences2[:10]\npaws_val_sentences2 = paws_val_sentences2[:10]\npaws_test_sentences2 = paws_test_sentences2[:10]\npaws_train_labels = paws_train_labels[:10]\npaws_val_labels  = paws_val_labels[:10]\npaws_test_labels=paws_test_labels[:10]\n\nprint(\"Dimensioni dei set:\")\nprint(f\"Train: {len(paws_train_sentences1)}\")\nprint(f\"Validation: {len(paws_val_sentences1)}\")\nprint(f\"Test: {len(paws_test_sentences1)}\")\n\n# Verifica distribuzione delle etichette\nprint(\"\\nDistribuzione delle etichette:\")\nprint(f\"Train: {Counter(paws_train_labels)}\")\nprint(f\"Validation: {Counter(paws_val_labels)}\")\nprint(f\"Test: {Counter(paws_test_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:06.622505Z","iopub.execute_input":"2025-03-23T20:47:06.622710Z","iopub.status.idle":"2025-03-23T20:47:06.796790Z","shell.execute_reply.started":"2025-03-23T20:47:06.622690Z","shell.execute_reply":"2025-03-23T20:47:06.796029Z"}},"outputs":[{"name":"stdout","text":"Dimensioni dei set:\nTrain: 10\nValidation: 10\nTest: 10\n\nDistribuzione delle etichette:\nTrain: Counter({0: 5, 1: 5})\nValidation: Counter({1: 8, 0: 2})\nTest: Counter({0: 6, 1: 4})\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 256 \n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\npaws_training_data = NLIDataset(premises = paws_train_sentences1,\n                            hypotheses = paws_train_sentences2,\n                            labels = paws_train_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)\n\npaws_validation_data = NLIDataset(premises = paws_val_sentences1,\n                            hypotheses = paws_val_sentences2,\n                            labels = paws_val_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)\n\npaws_test_data = NLIDataset(premises = paws_test_sentences1,\n                            hypotheses = paws_test_sentences2,\n                            labels = paws_test_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:06.797550Z","iopub.execute_input":"2025-03-23T20:47:06.797778Z","iopub.status.idle":"2025-03-23T20:47:06.930238Z","shell.execute_reply.started":"2025-03-23T20:47:06.797758Z","shell.execute_reply":"2025-03-23T20:47:06.929546Z"}},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":"### Addestramento del modello","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n\n# Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Pretrained model\nlora_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n\n# LoRA config\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=128,\n    lora_dropout=0.3,\n    target_modules=[\"query\", \"key\", \"value\"], \n    bias=\"none\",\n)\n\nlora_model = get_peft_model(lora_model, lora_config)\nlora_model.print_trainable_parameters()\n\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:06.931017Z","iopub.execute_input":"2025-03-23T20:47:06.931259Z","iopub.status.idle":"2025-03-23T20:47:07.374127Z","shell.execute_reply.started":"2025-03-23T20:47:06.931237Z","shell.execute_reply":"2025-03-23T20:47:07.373265Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 884,736 || all params: 110,368,514 || trainable%: 0.8016\n","output_type":"stream"},{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): BertForSequenceClassification(\n      (bert): BertModel(\n        (embeddings): BertEmbeddings(\n          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (token_type_embeddings): Embedding(2, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): BertEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.3, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.3, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.3, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (activation): Tanh()\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): Linear(in_features=768, out_features=2, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"for name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        param.requires_grad = True\n\nfor name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        print(f\"{name}: requires_grad = {param.requires_grad}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:07.374922Z","iopub.execute_input":"2025-03-23T20:47:07.375161Z","iopub.status.idle":"2025-03-23T20:47:07.382006Z","shell.execute_reply.started":"2025-03-23T20:47:07.375132Z","shell.execute_reply":"2025-03-23T20:47:07.381158Z"}},"outputs":[{"name":"stdout","text":"base_model.model.classifier.weight: requires_grad = True\nbase_model.model.classifier.bias: requires_grad = True\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Parametri principali\nlearning_rate = 5e-5\nEPOCHS = 10\nBATCH_SIZE = 32\n\n\n# Creo i DataLoader\npaws_train_loader = DataLoader(paws_training_data, batch_size=BATCH_SIZE, shuffle=True)\npaws_val_loader = DataLoader(paws_validation_data, batch_size=BATCH_SIZE, shuffle=False)\npaws_test_loader = DataLoader(paws_test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(paws_train_loader) * EPOCHS\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n\n# Scheduler\nscheduler = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:07.382965Z","iopub.execute_input":"2025-03-23T20:47:07.383245Z","iopub.status.idle":"2025-03-23T20:47:07.398681Z","shell.execute_reply.started":"2025-03-23T20:47:07.383216Z","shell.execute_reply":"2025-03-23T20:47:07.397936Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"history, total_time, emissions = train_and_evaluate_model(\n    lora_model,\"paws\", paws_train_loader, paws_val_loader, optimizer, scheduler, device, epochs=EPOCHS\n) \nprint(f\"\\nBERT with LoRA Training Time: {total_time:.2f} seconds, {total_time/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:07.404512Z","iopub.execute_input":"2025-03-23T20:47:07.404727Z","iopub.status.idle":"2025-03-23T20:47:17.679809Z","shell.execute_reply.started":"2025-03-23T20:47:07.404709Z","shell.execute_reply":"2025-03-23T20:47:17.678882Z"}},"outputs":[{"name":"stderr","text":"[codecarbon INFO @ 20:47:07] [setup] RAM Tracking...\n[codecarbon INFO @ 20:47:07] [setup] CPU Tracking...\n[codecarbon WARNING @ 20:47:07] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon WARNING @ 20:47:08] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon INFO @ 20:47:08] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:47:08] [setup] GPU Tracking...\n[codecarbon INFO @ 20:47:08] Tracking Nvidia GPU via pynvml\n[codecarbon WARNING @ 20:47:08] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:47:08] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:47:08] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:47:08] >>> Tracker's metadata:\n[codecarbon INFO @ 20:47:08]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 20:47:08]   Python version: 3.10.12\n[codecarbon INFO @ 20:47:08]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 20:47:08]   Available RAM : 31.351 GB\n[codecarbon INFO @ 20:47:08]   CPU count: 4\n[codecarbon INFO @ 20:47:08]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:47:08]   GPU count: 1\n[codecarbon INFO @ 20:47:08]   GPU model: 1 x Tesla P100-PCIE-16GB\n[codecarbon INFO @ 20:47:11] Saving emissions data to file /kaggle/working/carbon_emissions/emissions.csv\n[codecarbon WARNING @ 20:47:11] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.28it/s, accuracy=0.6, loss=0.681]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.56it/s, accuracy=0.3, loss=0.698]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.52it/s, accuracy=0.6, loss=0.666]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.58it/s, accuracy=0.5, loss=0.692]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.41it/s, accuracy=0.7, loss=0.628]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.66it/s, accuracy=0.4, loss=0.693]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 1 epoche.\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.57it/s, accuracy=0.4, loss=0.739]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.64it/s, accuracy=0.4, loss=0.693]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 2 epoche.\n\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.59it/s, accuracy=0.6, loss=0.686]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.42it/s, accuracy=0.5, loss=0.692]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.60it/s, accuracy=0.5, loss=0.665]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.59it/s, accuracy=0.5, loss=0.691]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.59it/s, accuracy=0.4, loss=0.712]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.69it/s, accuracy=0.5, loss=0.691]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 1 epoche.\n\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.59it/s, accuracy=0.6, loss=0.672]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s, accuracy=0.6, loss=0.692]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 2 epoche.\n\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.60it/s, accuracy=0.5, loss=0.736]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.67it/s, accuracy=0.5, loss=0.692]\n[codecarbon INFO @ 20:47:17] Energy consumed for RAM : 0.000019 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 20:47:17] Energy consumed for all CPUs : 0.000070 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 20:47:17] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:47:17] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 20:47:17] 0.000089 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 3 epoche.\nEarly stopping attivato dopo 3 epoche senza miglioramenti\n\nEmissioni CO₂ totali: 0.0000 kg\n\nBERT with LoRA Training Time: 5.91 seconds, 0.10 minutes.\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"paws_best_model_state.bin\"))\n\ntest_loss, test_acc, test_f1 = eval_model(lora_model, paws_test_loader, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:17.681062Z","iopub.execute_input":"2025-03-23T20:47:17.681379Z","iopub.status.idle":"2025-03-23T20:47:18.085768Z","shell.execute_reply.started":"2025-03-23T20:47:17.681347Z","shell.execute_reply":"2025-03-23T20:47:18.084864Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-69-177b40d96f6d>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_model.load_state_dict(torch.load(\"paws_best_model_state.bin\"))\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.23it/s, accuracy=0.3, loss=0.698]","output_type":"stream"},{"name":"stdout","text":"LoRA Fine-Tuning - Test loss: 0.6976, Accuracy: 0.3000, F1 score: 0.3071\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"# Memorizzazione dei risultati su Sentiment140\nadd_task_results(\n    task_name=\"paws\", \n    training_time=total_time,\n    emissions=emissions,\n    test_loss=test_loss,\n    test_acc=test_acc,\n    test_f1=test_f1,\n)\n\nperformance = pd.DataFrame(model_performance)\nprint(performance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:18.086933Z","iopub.execute_input":"2025-03-23T20:47:18.087338Z","iopub.status.idle":"2025-03-23T20:47:20.462751Z","shell.execute_reply.started":"2025-03-23T20:47:18.087302Z","shell.execute_reply":"2025-03-23T20:47:20.461938Z"}},"outputs":[{"name":"stdout","text":"           Task  Training Time  CO2 Emissions  Test Loss  Accuracy  F1 Score\n0        agnews       5.584955       0.000012   1.315260       0.6  0.709524\n1           sst       1.320940       0.000003   0.741614       0.3  0.138462\n2            ei       4.548648       0.000010   1.514121       0.2  0.109091\n3          mnli       3.331646       0.000007   1.108231       0.5  0.333333\n4  sentiment140       5.909914       0.000012   0.697641       0.3  0.307071\n","output_type":"stream"}],"execution_count":70},{"cell_type":"markdown","source":"### Salvataggio dell'adapter LoRA","metadata":{}},{"cell_type":"code","source":"lora_model.save_pretrained(\"paws_lora_adapter\")\n\nclassifier_state_dict = {\n    \"classifier.weight\": lora_model.base_model.model.classifier.weight.cpu(),\n    \"classifier.bias\": lora_model.base_model.model.classifier.bias.cpu()\n}\n\ntorch.save(classifier_state_dict, \"paws_classifier_head.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:20.463661Z","iopub.execute_input":"2025-03-23T20:47:20.463916Z","iopub.status.idle":"2025-03-23T20:47:20.482471Z","shell.execute_reply.started":"2025-03-23T20:47:20.463893Z","shell.execute_reply":"2025-03-23T20:47:20.481617Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"# from peft import PeftModel\n\n# base_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n# lora_model = PeftModel.from_pretrained(base_model, \"pwas_lora_adapter\")\n\n# classifier_state_dict = torch.load(\"paws_classifier_head.pth\", map_location=device, weights_only=True)\n\n\n# lora_model.base_model.classifier.weight.data.copy_(classifier_state_dict[\"classifier.weight\"])\n# lora_model.base_model.classifier.bias.data.copy_(classifier_state_dict[\"classifier.bias\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:20.483417Z","iopub.execute_input":"2025-03-23T20:47:20.483711Z","iopub.status.idle":"2025-03-23T20:47:20.497245Z","shell.execute_reply.started":"2025-03-23T20:47:20.483689Z","shell.execute_reply":"2025-03-23T20:47:20.496481Z"}},"outputs":[],"execution_count":72}]}