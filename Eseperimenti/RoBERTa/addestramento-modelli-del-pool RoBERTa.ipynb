{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10587017,"sourceType":"datasetVersion","datasetId":6274971},{"sourceId":10659576,"sourceType":"datasetVersion","datasetId":6601086},{"sourceId":10739670,"sourceType":"datasetVersion","datasetId":6659558},{"sourceId":10759017,"sourceType":"datasetVersion","datasetId":6673684}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Addestramento dei modelli del pool","metadata":{}},{"cell_type":"markdown","source":"#### Configurazioni generali","metadata":{}},{"cell_type":"markdown","source":"Installazione della librerie necessarie.","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets torch peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:27:50.133223Z","iopub.execute_input":"2025-04-03T08:27:50.133610Z","iopub.status.idle":"2025-04-03T08:27:54.818085Z","shell.execute_reply.started":"2025-04-03T08:27:50.133565Z","shell.execute_reply":"2025-04-03T08:27:54.816952Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install tensorflow==2.17.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:27:54.819497Z","iopub.execute_input":"2025-04-03T08:27:54.819730Z","iopub.status.idle":"2025-04-03T08:28:57.039261Z","shell.execute_reply.started":"2025-04-03T08:27:54.819710Z","shell.execute_reply":"2025-04-03T08:28:57.038137Z"}},"outputs":[{"name":"stdout","text":"Collecting tensorflow==2.17.0\n  Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.12.1)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (18.1.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.4.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.17.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.68.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.17.1)\nRequirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.37.1)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.26.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.17.0) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2024.12.14)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (2.18.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0.0,>=1.23.5->tensorflow==2.17.0) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.0) (0.1.2)\nDownloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tensorflow\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.17.1\n    Uninstalling tensorflow-2.17.1:\n      Successfully uninstalled tensorflow-2.17.1\nSuccessfully installed tensorflow-2.17.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install codecarbon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:28:57.041127Z","iopub.execute_input":"2025-04-03T08:28:57.041455Z","iopub.status.idle":"2025-04-03T08:29:09.216149Z","shell.execute_reply.started":"2025-04-03T08:28:57.041428Z","shell.execute_reply":"2025-04-03T08:29:09.215185Z"}},"outputs":[{"name":"stdout","text":"Collecting codecarbon\n  Downloading codecarbon-2.8.3-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: arrow in /usr/local/lib/python3.10/dist-packages (from codecarbon) (1.3.0)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from codecarbon) (8.1.7)\nCollecting fief-client[cli] (from codecarbon)\n  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.2.2)\nRequirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.21.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from codecarbon) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from codecarbon) (9.0.0)\nRequirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from codecarbon) (11.4.1)\nCollecting questionary (from codecarbon)\n  Downloading questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\nCollecting rapidfuzz (from codecarbon)\n  Downloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from codecarbon) (2.32.3)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from codecarbon) (13.9.4)\nRequirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from codecarbon) (0.15.1)\nRequirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon) (2.8.2)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow->codecarbon) (2.9.0.20241206)\nCollecting httpx<0.28.0,>=0.21.3 (from fief-client[cli]->codecarbon)\n  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\nCollecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon)\n  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\nCollecting yaspin (from fief-client[cli]->codecarbon)\n  Downloading yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (1.26.4)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->codecarbon) (2024.2)\nRequirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from questionary->codecarbon) (3.0.48)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->codecarbon) (2024.12.14)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (2.18.0)\nRequirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->codecarbon) (4.12.2)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->codecarbon) (1.5.4)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.7)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\nRequirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.10/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->codecarbon) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->codecarbon) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->codecarbon) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->codecarbon) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->codecarbon) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas->codecarbon) (2.4.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.13)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.17.0)\nCollecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon)\n  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.2.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas->codecarbon) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas->codecarbon) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas->codecarbon) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas->codecarbon) (2024.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas->codecarbon) (2024.2.0)\nDownloading codecarbon-2.8.3-py3-none-any.whl (516 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.7/516.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading questionary-2.1.0-py3-none-any.whl (36 kB)\nDownloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fief_client-0.20.0-py3-none-any.whl (20 kB)\nDownloading yaspin-3.1.0-py3-none-any.whl (18 kB)\nDownloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\nInstalling collected packages: termcolor, rapidfuzz, yaspin, questionary, httpx, jwcrypto, fief-client, codecarbon\n  Attempting uninstall: termcolor\n    Found existing installation: termcolor 2.5.0\n    Uninstalling termcolor-2.5.0:\n      Successfully uninstalled termcolor-2.5.0\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.28.1\n    Uninstalling httpx-0.28.1:\n      Successfully uninstalled httpx-0.28.1\nSuccessfully installed codecarbon-2.8.3 fief-client-0.20.0 httpx-0.27.2 jwcrypto-1.5.6 questionary-2.1.0 rapidfuzz-3.12.2 termcolor-2.3.0 yaspin-3.1.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Importo i moduli necessari.","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:29:09.217799Z","iopub.execute_input":"2025-04-03T08:29:09.218128Z","iopub.status.idle":"2025-04-03T08:29:14.491448Z","shell.execute_reply.started":"2025-04-03T08:29:09.218096Z","shell.execute_reply":"2025-04-03T08:29:14.490597Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Impostazione del seme casuale per la riproducibilità.","metadata":{}},{"cell_type":"code","source":"seed_value = 42\n\nos.environ['PYTHONHASHSEED'] = str(seed_value)\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\n\n# Imposto il seme casuale anche per i calcoli CUDA\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)  \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:29:14.492309Z","iopub.execute_input":"2025-04-03T08:29:14.492785Z","iopub.status.idle":"2025-04-03T08:29:14.554021Z","shell.execute_reply.started":"2025-04-03T08:29:14.492753Z","shell.execute_reply":"2025-04-03T08:29:14.553227Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## AG News","metadata":{}},{"cell_type":"markdown","source":"### Ottenimento dei dati e preprocessing\n\nCarico il dataset **AG News**, una raccolta di articoli di notizie che devono essere classificate in una delle quattro categorie predefinite: **World**, **Sports**, **Business**, e **Sci/Tech**.","metadata":{}},{"cell_type":"code","source":"# ottenimento del dataset\n\nag_dataset = load_dataset(\"ag_news\")\nprint(ag_dataset)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-04-03T08:29:14.554953Z","iopub.execute_input":"2025-04-03T08:29:14.555303Z","iopub.status.idle":"2025-04-03T08:29:18.834048Z","shell.execute_reply.started":"2025-04-03T08:29:14.555264Z","shell.execute_reply":"2025-04-03T08:29:18.833124Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe06242d3d654a61b4a983f75b2c3684"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"430157a5e8bc430b8fd8951b6efdfc70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd66dcd7e0d1440e9b1d042f2858c36b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7b4390d8b584335931d5c62dfbea8b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7272ca9a4b8248fcbd3fdc129f9e09a0"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 120000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 7600\n    })\n})\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Divido i dati di train in training set e validation set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\nag_train_data = ag_dataset[\"train\"]\nag_test_data = ag_dataset[\"test\"]\n\nag_train_sentences, ag_val_sentences, ag_train_labels, ag_val_labels = train_test_split(\n                                                  ag_train_data['text'], \n                                                  ag_train_data['label'],\n                                                  test_size=4000, \n                                                  train_size=20000,\n                                                  random_state=42,\n                                                  shuffle=True,\n                                                  stratify=ag_train_data['label'])\n\nag_test_sentences, ag_test_labels = ag_test_data['text'], ag_test_data['label']\n\n# ag_train_sentences = ag_train_sentences[:10]\n# ag_val_sentences = ag_val_sentences[:10]\n# ag_test_sentences = ag_test_sentences[:10]\n# ag_train_labels = ag_train_labels[:10]\n# ag_val_labels  =ag_val_labels[:10]\n# ag_test_labels=ag_test_labels[:10]\n\nprint(\"Dimensioni dei set:\")\nprint(f\"Train: {len(ag_train_sentences)}\")\nprint(f\"Validation: {len(ag_val_sentences)}\")\nprint(f\"Test: {len(ag_test_sentences)}\")\n\n# Verifica distribuzione delle etichette\nprint(\"\\nDistribuzione delle etichette:\")\nprint(f\"Train: {Counter(ag_train_labels)}\")\nprint(f\"Validation: {Counter(ag_val_labels)}\")\nprint(f\"Test: {Counter(ag_test_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:29:18.835009Z","iopub.execute_input":"2025-04-03T08:29:18.835645Z","iopub.status.idle":"2025-04-03T08:29:19.568443Z","shell.execute_reply.started":"2025-04-03T08:29:18.835608Z","shell.execute_reply":"2025-04-03T08:29:19.567675Z"}},"outputs":[{"name":"stdout","text":"Dimensioni dei set:\nTrain: 20000\nValidation: 4000\nTest: 7600\n\nDistribuzione delle etichette:\nTrain: Counter({1: 5000, 2: 5000, 0: 5000, 3: 5000})\nValidation: Counter({2: 1000, 1: 1000, 3: 1000, 0: 1000})\nTest: Counter({2: 1900, 3: 1900, 1: 1900, 0: 1900})\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass ClassificationDataset(Dataset):\n\n    def __init__(self, sentences, labels, tokenizer, max_len):\n        self.sentences = sentences\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    def __getitem__(self,index):\n        sentence = self.sentences[index]\n        label = self.labels[index]\n        \n        encoding = self.tokenizer.encode_plus(\n            sentence,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors='pt')\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding[\"token_type_ids\"].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:29:19.580730Z","iopub.execute_input":"2025-04-03T08:29:19.580950Z","iopub.status.idle":"2025-04-03T08:29:19.601252Z","shell.execute_reply.started":"2025-04-03T08:29:19.580933Z","shell.execute_reply":"2025-04-03T08:29:19.600395Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Inizializzo il Tokenizer BERT per tokenizzare le frasi e creo i dataset personalizzati.","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 128\n\n# Inizializza il Tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n\n#Ottieni i dataset\nag_training_data = ClassificationDataset( sentences = ag_train_sentences,\n                           labels = ag_train_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nag_validation_data = ClassificationDataset( sentences = ag_val_sentences,\n                             labels = ag_val_labels,\n                             tokenizer = tokenizer,\n                             max_len = MAX_SEQ_LEN)\n\nag_test_data = ClassificationDataset( sentences = ag_test_sentences,\n                       labels = ag_test_labels,\n                       tokenizer = tokenizer,\n                       max_len = MAX_SEQ_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:29:19.602157Z","iopub.execute_input":"2025-04-03T08:29:19.602397Z","iopub.status.idle":"2025-04-03T08:29:26.278023Z","shell.execute_reply.started":"2025-04-03T08:29:19.602371Z","shell.execute_reply":"2025-04-03T08:29:26.277090Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d80bcf6efb9f4d15b9e84177caad0e56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15663ed996e2476387a1675d645a437e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5da380b811004a1ea8758af493ba13cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f75845cd3b441e7a66a5b8dde4505ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7536ba8cb914cfa82599d6e3950a4cd"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### Addestramento del modello","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport time\nimport torch\nfrom sklearn.metrics import accuracy_score, f1_score\nimport torch.nn as nn\nfrom codecarbon import EmissionsTracker\n\n\n# Funzione di training e valutazione\ndef train_and_evaluate_model(model, dataset, train_loader, val_loader, optimizer, scheduler, device, epochs=10, patience=3):\n\n    os.makedirs(\"carbon_emissions\", exist_ok=True)\n    tracker = EmissionsTracker(output_dir=\"carbon_emissions\", output_file=\"emissions.csv\")  \n    tracker.start()  \n\n    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n    best_accuracy = 0\n    best_loss = float('inf')\n    patience_counter = 0  \n\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n\n        # Training\n        train_loss, train_acc = train_model(model, train_loader, optimizer, scheduler, device)\n        \n        # Valutazione\n        val_loss, val_acc, val_f1, _ = eval_model(model, val_loader, device)\n        \n        # Salvataggio del modello migliore\n        if val_acc > best_accuracy:\n            torch.save(model.state_dict(),  f\"{dataset}_best_model_state.bin\")\n            best_accuracy = val_acc\n\n        # Salvataggio delle metriche\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n\n        # Early stopping\n        if val_loss < best_loss:\n            best_loss = val_loss\n            patience_counter = 0 \n        else:\n            patience_counter += 1\n            print(f\"La loss sul validation set non è migliorata per {patience_counter} epoche.\")\n\n        if patience_counter >= patience:\n            print(f\"Early stopping attivato dopo {patience_counter} epoche senza miglioramenti\")\n            break\n\n    end_time = time.time()\n    total_training_time = end_time - start_time\n\n    emissions = tracker.stop()\n    print(f\"\\nEmissioni CO₂ totali: {emissions:.4f} kg\")  \n\n    return history, total_training_time, emissions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:45:27.563499Z","iopub.execute_input":"2025-04-03T08:45:27.563819Z","iopub.status.idle":"2025-04-03T08:45:27.572197Z","shell.execute_reply.started":"2025-04-03T08:45:27.563791Z","shell.execute_reply":"2025-04-03T08:45:27.571200Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Funzione di training\ndef train_model(model, data_loader, optimizer, scheduler, device):\n\n    model = model.train()\n\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    loop = tqdm(data_loader, desc=f\"Training  \", leave=True)\n\n    for batch in loop:\n\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n\n        # --- Forward pass ---\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            labels=labels \n        )\n\n        loss = outputs.loss  \n        logits = outputs.logits  \n\n        # --- Backward pass ---\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(logits, dim=1)  # Predizioni multiclasse\n\n        all_preds.extend(preds.detach().cpu().numpy())\n        all_labels.extend(labels.detach().cpu().numpy())\n\n        loop.set_postfix(loss=total_loss / (loop.n + 1), accuracy=accuracy_score(all_labels, all_preds))\n\n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    return avg_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:29:26.399893Z","iopub.execute_input":"2025-04-03T08:29:26.400231Z","iopub.status.idle":"2025-04-03T08:29:26.407645Z","shell.execute_reply.started":"2025-04-03T08:29:26.400195Z","shell.execute_reply":"2025-04-03T08:29:26.406449Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Funzione di valutazione\ndef eval_model(model, data_loader, device):\n\n    model = model.eval()\n\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        \n        loop = tqdm(data_loader, desc=f\"Evaluating\", leave=True)\n        for batch in loop:\n            \n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            token_type_ids = batch[\"token_type_ids\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n                labels=labels\n            )\n\n            loss = outputs.loss\n            logits = outputs.logits\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(logits, dim=1)\n\n            all_preds.extend(preds.detach().cpu().numpy())\n            all_labels.extend(labels.detach().cpu().numpy())\n\n            loop.set_postfix(loss=total_loss / (loop.n + 1), accuracy=accuracy_score(all_labels, all_preds))\n\n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n    f1_s = f1_score(all_labels, all_preds, average=\"macro\")\n    \n    return avg_loss, accuracy, f1, f1_s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:36:28.428797Z","iopub.execute_input":"2025-04-03T08:36:28.429163Z","iopub.status.idle":"2025-04-03T08:36:28.436589Z","shell.execute_reply.started":"2025-04-03T08:36:28.429129Z","shell.execute_reply":"2025-04-03T08:36:28.435657Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"Creo il modello con LoRA.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import RobertaForSequenceClassification\n\n\n# Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Pretrained model\nlora_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=4)\n    \n# LoRA config\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=128,\n    lora_dropout=0.2,\n    target_modules=[\"query\", \"key\", \"value\"],\n    bias=\"none\",\n)\n\nlora_model = get_peft_model(lora_model, lora_config)\nlora_model.print_trainable_parameters()\n\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:29:26.426135Z","iopub.execute_input":"2025-04-03T08:29:26.426428Z","iopub.status.idle":"2025-04-03T08:29:35.913544Z","shell.execute_reply.started":"2025-04-03T08:29:26.426405Z","shell.execute_reply":"2025-04-03T08:29:35.912743Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b4eb036a9874d8c8ee6bdce0f1eb70d"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,769,472 || all params: 126,418,180 || trainable%: 1.3997\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): RobertaForSequenceClassification(\n      (roberta): RobertaModel(\n        (embeddings): RobertaEmbeddings(\n          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n          (position_embeddings): Embedding(514, 768, padding_idx=1)\n          (token_type_embeddings): Embedding(1, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): RobertaEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x RobertaLayer(\n              (attention): RobertaAttention(\n                (self): RobertaSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): RobertaSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): RobertaIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): RobertaOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n      )\n      (classifier): RobertaClassificationHead(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (out_proj): Linear(in_features=768, out_features=4, bias=True)\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"for name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        param.requires_grad = True\n\nfor name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        print(f\"{name}: requires_grad = {param.requires_grad}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:29:35.914347Z","iopub.execute_input":"2025-04-03T08:29:35.915049Z","iopub.status.idle":"2025-04-03T08:29:35.923104Z","shell.execute_reply.started":"2025-04-03T08:29:35.915007Z","shell.execute_reply":"2025-04-03T08:29:35.922258Z"}},"outputs":[{"name":"stdout","text":"base_model.model.classifier.dense.weight: requires_grad = True\nbase_model.model.classifier.dense.bias: requires_grad = True\nbase_model.model.classifier.out_proj.weight: requires_grad = True\nbase_model.model.classifier.out_proj.bias: requires_grad = True\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Imposto i parametri principali ed effettuo l'addestramento.","metadata":{}},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 2e-4\nEPOCHS = 10\nBATCH_SIZE = 32\n\n# Creo i DataLoader\nag_train_loader = DataLoader(ag_training_data, batch_size=BATCH_SIZE, shuffle=True)\nag_val_loader = DataLoader(ag_validation_data, batch_size=BATCH_SIZE, shuffle=False)\nag_test_loader = DataLoader(ag_test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(ag_train_loader) * EPOCHS\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n\n# Scheduler\nscheduler = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:29:35.923919Z","iopub.execute_input":"2025-04-03T08:29:35.924180Z","iopub.status.idle":"2025-04-03T08:29:36.067578Z","shell.execute_reply.started":"2025-04-03T08:29:35.924158Z","shell.execute_reply":"2025-04-03T08:29:36.066875Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"history, total_time, emissions = train_and_evaluate_model(\n    lora_model,\"ag\", ag_train_loader, ag_val_loader, optimizer, scheduler, device, epochs=10\n) \nprint(f\"\\nBERT with LoRA Training Time: {total_time:.2f} seconds, {total_time/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:45:37.395087Z","iopub.execute_input":"2025-04-03T08:45:37.395383Z"}},"outputs":[{"name":"stderr","text":"[codecarbon ERROR @ 08:45:37] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n[codecarbon WARNING @ 08:45:37] Another instance of codecarbon is already running. Exiting.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training  :   2%|▏         | 10/625 [00:02<02:53,  3.55it/s, accuracy=0.944, loss=0.177][codecarbon INFO @ 08:45:40] Energy consumed for RAM : 0.003133 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:45:40] Energy consumed for all CPUs : 0.011329 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:45:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:45:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:45:40] 0.014462 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:45:40] 0.004301 g.CO2eq/s mean an estimation of 135.63439430150726 kg.CO2eq/year\nTraining  :  10%|█         | 63/625 [00:17<02:39,  3.52it/s, accuracy=0.941, loss=0.183][codecarbon INFO @ 08:45:55] Energy consumed for RAM : 0.003182 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:45:55] Energy consumed for all CPUs : 0.011506 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:45:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:45:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:45:55] 0.014688 kWh of electricity used since the beginning.\nTraining  :  19%|█▊        | 116/625 [00:32<02:23,  3.54it/s, accuracy=0.939, loss=0.183][codecarbon INFO @ 08:46:10] Energy consumed for RAM : 0.003231 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:46:10] Energy consumed for all CPUs : 0.011683 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:46:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:46:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:46:10] 0.014914 kWh of electricity used since the beginning.\nTraining  :  27%|██▋       | 169/625 [00:47<02:09,  3.52it/s, accuracy=0.939, loss=0.186][codecarbon INFO @ 08:46:25] Energy consumed for RAM : 0.003280 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:46:25] Energy consumed for all CPUs : 0.011860 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:46:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:46:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:46:25] 0.015140 kWh of electricity used since the beginning.\nTraining  :  36%|███▌      | 222/625 [01:02<01:54,  3.52it/s, accuracy=0.94, loss=0.185] [codecarbon INFO @ 08:46:40] Energy consumed for RAM : 0.003329 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:46:40] Energy consumed for all CPUs : 0.012037 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:46:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:46:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:46:40] 0.015366 kWh of electricity used since the beginning.\nTraining  :  44%|████▍     | 275/625 [01:17<01:40,  3.50it/s, accuracy=0.944, loss=0.175][codecarbon INFO @ 08:46:55] Energy consumed for RAM : 0.003378 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:46:55] Energy consumed for all CPUs : 0.012214 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:46:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:46:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:46:55] 0.015592 kWh of electricity used since the beginning.\nTraining  :  52%|█████▏    | 327/625 [01:32<01:25,  3.50it/s, accuracy=0.942, loss=0.183][codecarbon INFO @ 08:47:10] Energy consumed for RAM : 0.003427 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:47:10] Energy consumed for all CPUs : 0.012391 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:47:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:47:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:47:10] 0.015818 kWh of electricity used since the beginning.\nTraining  :  61%|██████    | 380/625 [01:47<01:10,  3.49it/s, accuracy=0.943, loss=0.179][codecarbon INFO @ 08:47:25] Energy consumed for RAM : 0.003476 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:47:25] Energy consumed for all CPUs : 0.012568 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:47:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:47:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:47:25] 0.016044 kWh of electricity used since the beginning.\nTraining  :  69%|██████▉   | 432/625 [02:02<00:55,  3.49it/s, accuracy=0.943, loss=0.179][codecarbon INFO @ 08:47:40] Energy consumed for RAM : 0.003525 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:47:40] Energy consumed for all CPUs : 0.012745 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:47:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:47:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:47:40] 0.016270 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:47:40] 0.004301 g.CO2eq/s mean an estimation of 135.6326517747847 kg.CO2eq/year\nTraining  :  77%|███████▋  | 484/625 [02:17<00:40,  3.48it/s, accuracy=0.942, loss=0.181][codecarbon INFO @ 08:47:55] Energy consumed for RAM : 0.003574 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:47:55] Energy consumed for all CPUs : 0.012922 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:47:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:47:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:47:55] 0.016496 kWh of electricity used since the beginning.\nTraining  :  86%|████████▌ | 536/625 [02:32<00:25,  3.48it/s, accuracy=0.942, loss=0.181][codecarbon INFO @ 08:48:10] Energy consumed for RAM : 0.003623 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:48:10] Energy consumed for all CPUs : 0.013099 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:48:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:48:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:48:10] 0.016722 kWh of electricity used since the beginning.\nTraining  :  94%|█████████▍| 588/625 [02:47<00:10,  3.46it/s, accuracy=0.942, loss=0.18] [codecarbon INFO @ 08:48:25] Energy consumed for RAM : 0.003672 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:48:25] Energy consumed for all CPUs : 0.013276 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:48:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:48:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:48:25] 0.016948 kWh of electricity used since the beginning.\nTraining  : 100%|██████████| 625/625 [02:58<00:00,  3.50it/s, accuracy=0.942, loss=0.18] \nEvaluating:  25%|██▍       | 31/125 [00:04<00:12,  7.34it/s, accuracy=0.92, loss=0.216] [codecarbon INFO @ 08:48:40] Energy consumed for RAM : 0.003721 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:48:40] Energy consumed for all CPUs : 0.013453 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:48:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:48:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:48:40] 0.017174 kWh of electricity used since the beginning.\nEvaluating: 100%|██████████| 125/125 [00:17<00:00,  7.28it/s, accuracy=0.929, loss=0.209]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training  :   1%|          | 4/625 [00:01<02:55,  3.53it/s, accuracy=0.977, loss=0.0714][codecarbon INFO @ 08:48:55] Energy consumed for RAM : 0.003770 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:48:55] Energy consumed for all CPUs : 0.013630 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:48:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:48:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:48:55] 0.017400 kWh of electricity used since the beginning.\nTraining  :   9%|▉         | 58/625 [00:16<02:40,  3.53it/s, accuracy=0.952, loss=0.131][codecarbon INFO @ 08:49:10] Energy consumed for RAM : 0.003819 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:49:10] Energy consumed for all CPUs : 0.013807 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:49:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:49:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:49:10] 0.017626 kWh of electricity used since the beginning.\nTraining  :  18%|█▊        | 111/625 [00:31<02:25,  3.54it/s, accuracy=0.95, loss=0.145] [codecarbon INFO @ 08:49:25] Energy consumed for RAM : 0.003868 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:49:25] Energy consumed for all CPUs : 0.013984 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:49:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:49:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:49:25] 0.017852 kWh of electricity used since the beginning.\nTraining  :  26%|██▌       | 164/625 [00:46<02:11,  3.50it/s, accuracy=0.951, loss=0.144][codecarbon INFO @ 08:49:40] Energy consumed for RAM : 0.003917 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:49:40] Energy consumed for all CPUs : 0.014161 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:49:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:49:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:49:40] 0.018078 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:49:40] 0.004301 g.CO2eq/s mean an estimation of 135.6263796825534 kg.CO2eq/year\nTraining  :  35%|███▍      | 216/625 [01:01<01:56,  3.51it/s, accuracy=0.95, loss=0.149] [codecarbon INFO @ 08:49:55] Energy consumed for RAM : 0.003966 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:49:55] Energy consumed for all CPUs : 0.014338 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:49:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:49:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:49:55] 0.018304 kWh of electricity used since the beginning.\nTraining  :  43%|████▎     | 269/625 [01:16<01:41,  3.50it/s, accuracy=0.95, loss=0.15]  [codecarbon INFO @ 08:50:10] Energy consumed for RAM : 0.004015 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:50:10] Energy consumed for all CPUs : 0.014515 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:50:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:50:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:50:10] 0.018530 kWh of electricity used since the beginning.\nTraining  :  52%|█████▏    | 322/625 [01:31<01:26,  3.50it/s, accuracy=0.95, loss=0.152] [codecarbon INFO @ 08:50:25] Energy consumed for RAM : 0.004064 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:50:25] Energy consumed for all CPUs : 0.014692 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:50:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:50:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:50:25] 0.018756 kWh of electricity used since the beginning.\nTraining  :  60%|█████▉    | 374/625 [01:46<01:11,  3.50it/s, accuracy=0.95, loss=0.15] [codecarbon INFO @ 08:50:40] Energy consumed for RAM : 0.004113 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:50:40] Energy consumed for all CPUs : 0.014869 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:50:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:50:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:50:40] 0.018982 kWh of electricity used since the beginning.\nTraining  :  68%|██████▊   | 426/625 [02:01<00:57,  3.48it/s, accuracy=0.95, loss=0.149] [codecarbon INFO @ 08:50:55] Energy consumed for RAM : 0.004162 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:50:55] Energy consumed for all CPUs : 0.015046 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:50:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:50:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:50:55] 0.019208 kWh of electricity used since the beginning.\nTraining  :  76%|███████▋  | 478/625 [02:16<00:42,  3.47it/s, accuracy=0.95, loss=0.148] [codecarbon INFO @ 08:51:10] Energy consumed for RAM : 0.004211 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:51:10] Energy consumed for all CPUs : 0.015223 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:51:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:51:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:51:10] 0.019434 kWh of electricity used since the beginning.\nTraining  :  85%|████████▍ | 530/625 [02:31<00:27,  3.43it/s, accuracy=0.951, loss=0.148][codecarbon INFO @ 08:51:25] Energy consumed for RAM : 0.004259 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:51:25] Energy consumed for all CPUs : 0.015400 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:51:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:51:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:51:25] 0.019660 kWh of electricity used since the beginning.\nTraining  :  93%|█████████▎| 582/625 [02:46<00:12,  3.45it/s, accuracy=0.951, loss=0.148][codecarbon INFO @ 08:51:40] Energy consumed for RAM : 0.004308 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:51:40] Energy consumed for all CPUs : 0.015577 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:51:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:51:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:51:40] 0.019886 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:51:40] 0.004301 g.CO2eq/s mean an estimation of 135.62961455182625 kg.CO2eq/year\nTraining  : 100%|██████████| 625/625 [02:58<00:00,  3.50it/s, accuracy=0.951, loss=0.149]\nEvaluating:  15%|█▌        | 19/125 [00:02<00:14,  7.31it/s, accuracy=0.906, loss=0.251][codecarbon INFO @ 08:51:55] Energy consumed for RAM : 0.004357 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:51:55] Energy consumed for all CPUs : 0.015754 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:51:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:51:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:51:55] 0.020112 kWh of electricity used since the beginning.\nEvaluating: 100%|██████████| 125/125 [00:17<00:00,  7.29it/s, accuracy=0.928, loss=0.215]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 1 epoche.\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training  :   0%|          | 1/625 [00:00<02:54,  3.57it/s, accuracy=0.906, loss=0.149][codecarbon INFO @ 08:52:10] Energy consumed for RAM : 0.004406 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:52:10] Energy consumed for all CPUs : 0.015931 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:52:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:52:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:52:10] 0.020337 kWh of electricity used since the beginning.\nTraining  :   9%|▊         | 54/625 [00:15<02:41,  3.53it/s, accuracy=0.957, loss=0.131][codecarbon INFO @ 08:52:25] Energy consumed for RAM : 0.004455 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:52:25] Energy consumed for all CPUs : 0.016108 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:52:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:52:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:52:25] 0.020563 kWh of electricity used since the beginning.\nTraining  :  17%|█▋        | 108/625 [00:30<02:25,  3.54it/s, accuracy=0.96, loss=0.121] [codecarbon INFO @ 08:52:40] Energy consumed for RAM : 0.004504 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:52:40] Energy consumed for all CPUs : 0.016285 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:52:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:52:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:52:40] 0.020789 kWh of electricity used since the beginning.\nTraining  :  26%|██▌       | 160/625 [00:45<02:12,  3.50it/s, accuracy=0.963, loss=0.116][codecarbon INFO @ 08:52:55] Energy consumed for RAM : 0.004553 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:52:55] Energy consumed for all CPUs : 0.016462 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:52:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:52:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:52:55] 0.021015 kWh of electricity used since the beginning.\nTraining  :  34%|███▍      | 213/625 [01:00<01:57,  3.52it/s, accuracy=0.961, loss=0.123][codecarbon INFO @ 08:53:10] Energy consumed for RAM : 0.004602 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:53:10] Energy consumed for all CPUs : 0.016639 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:53:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:53:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:53:10] 0.021241 kWh of electricity used since the beginning.\nTraining  :  43%|████▎     | 266/625 [01:15<01:42,  3.51it/s, accuracy=0.96, loss=0.126] [codecarbon INFO @ 08:53:25] Energy consumed for RAM : 0.004651 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:53:25] Energy consumed for all CPUs : 0.016816 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:53:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:53:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:53:25] 0.021467 kWh of electricity used since the beginning.\nTraining  :  51%|█████     | 318/625 [01:30<01:27,  3.52it/s, accuracy=0.959, loss=0.13] [codecarbon INFO @ 08:53:40] Energy consumed for RAM : 0.004700 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:53:40] Energy consumed for all CPUs : 0.016993 kWh. Total CPU Power : 42.5 W\nTraining  :  51%|█████     | 318/625 [01:30<01:27,  3.52it/s, accuracy=0.959, loss=0.13][codecarbon WARNING @ 08:53:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\nTraining  :  51%|█████     | 319/625 [01:30<01:27,  3.50it/s, accuracy=0.959, loss=0.13][codecarbon INFO @ 08:53:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:53:40] 0.021693 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:53:40] 0.004300 g.CO2eq/s mean an estimation of 135.61629198683565 kg.CO2eq/year\nTraining  :  59%|█████▉    | 371/625 [01:45<01:12,  3.50it/s, accuracy=0.959, loss=0.127][codecarbon INFO @ 08:53:55] Energy consumed for RAM : 0.004749 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:53:55] Energy consumed for all CPUs : 0.017170 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:53:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:53:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:53:55] 0.021919 kWh of electricity used since the beginning.\nTraining  :  68%|██████▊   | 423/625 [02:00<00:57,  3.49it/s, accuracy=0.959, loss=0.128][codecarbon INFO @ 08:54:10] Energy consumed for RAM : 0.004798 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:54:10] Energy consumed for all CPUs : 0.017347 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:54:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:54:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:54:10] 0.022145 kWh of electricity used since the beginning.\nTraining  :  76%|███████▌  | 475/625 [02:15<00:43,  3.48it/s, accuracy=0.959, loss=0.127][codecarbon INFO @ 08:54:25] Energy consumed for RAM : 0.004847 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:54:25] Energy consumed for all CPUs : 0.017524 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:54:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:54:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:54:25] 0.022371 kWh of electricity used since the beginning.\nTraining  :  84%|████████▍ | 527/625 [02:30<00:28,  3.45it/s, accuracy=0.959, loss=0.127][codecarbon INFO @ 08:54:40] Energy consumed for RAM : 0.004896 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:54:40] Energy consumed for all CPUs : 0.017701 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:54:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:54:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:54:40] 0.022597 kWh of electricity used since the beginning.\nTraining  :  93%|█████████▎| 579/625 [02:45<00:13,  3.41it/s, accuracy=0.96, loss=0.125] [codecarbon INFO @ 08:54:55] Energy consumed for RAM : 0.004945 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:54:55] Energy consumed for all CPUs : 0.017878 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:54:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:54:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:54:55] 0.022823 kWh of electricity used since the beginning.\nTraining  : 100%|██████████| 625/625 [02:58<00:00,  3.50it/s, accuracy=0.96, loss=0.125]\nEvaluating:  10%|▉         | 12/125 [00:01<00:15,  7.24it/s, accuracy=0.911, loss=0.28] [codecarbon INFO @ 08:55:10] Energy consumed for RAM : 0.004994 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:55:10] Energy consumed for all CPUs : 0.018055 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:55:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:55:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:55:10] 0.023049 kWh of electricity used since the beginning.\nEvaluating:  95%|█████████▌| 119/125 [00:16<00:00,  6.80it/s, accuracy=0.933, loss=0.22] [codecarbon INFO @ 08:55:25] Energy consumed for RAM : 0.005043 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:55:25] Energy consumed for all CPUs : 0.018232 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:55:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:55:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:55:25] 0.023275 kWh of electricity used since the beginning.\nEvaluating: 100%|██████████| 125/125 [00:17<00:00,  7.16it/s, accuracy=0.933, loss=0.22] \n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 2 epoche.\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training  :   1%|          | 4/625 [00:01<02:56,  3.52it/s, accuracy=0.977, loss=0.0526]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### Valutazione del modello\nValuto il modello calcolando la loss sul test set, l'accuracy e l'F1-score.","metadata":{}},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"ag_best_model_state.bin\"))  \n    \ntest_loss, test_acc, test_f1, test_f1score = eval_model(lora_model, ag_test_loader, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:40:04.242837Z","iopub.status.idle":"2025-04-03T08:40:04.243182Z","shell.execute_reply":"2025-04-03T08:40:04.243010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_performance = []\n\n# Funzione per memorizzare le performance sul task appena addestrato\ndef add_task_results(task_name, training_time, emissions, test_loss, test_acc, test_f1, f1):\n    model_performance.append({\n        \"Task\": task_name,\n        \"Training Time\": training_time,\n        \"CO2 Emissions\": emissions,\n        \"Test Loss\": test_loss,\n        \"Accuracy\": test_acc,\n        \"F1 \": test_f1,\n        \"F1 Score Macro\": test_f1,\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:40:04.244569Z","iopub.status.idle":"2025-04-03T08:40:04.245033Z","shell.execute_reply":"2025-04-03T08:40:04.244835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Memorizzazione dei risultati su Sentiment140\nadd_task_results(\n    task_name=\"ag\", \n    training_time=total_time,\n    emissions=emissions,\n    test_loss=test_loss,\n    test_acc=test_acc,\n    test_f1=test_f1,\n    f1 = test_f1score\n)\n\nperformance = pd.DataFrame(model_performance)\nprint(performance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:40:04.246199Z","iopub.status.idle":"2025-04-03T08:40:04.246610Z","shell.execute_reply":"2025-04-03T08:40:04.246444Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Salvataggio dell'adapter LoRA","metadata":{}},{"cell_type":"code","source":"lora_model.save_pretrained(\"ag_lora_adapter\")\n\nclassifier_state_dict = {\n    \"classifier.weight\": lora_model.base_model.model.classifier.weight.cpu(),\n    \"classifier.bias\": lora_model.base_model.model.classifier.bias.cpu()\n}\n\ntorch.save(classifier_state_dict, \"ag_classifier_head.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:40:04.247496Z","iopub.status.idle":"2025-04-03T08:40:04.247939Z","shell.execute_reply":"2025-04-03T08:40:04.247756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel\n\nbase_model = RobertaForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\nlora_model = PeftModel.from_pretrained(base_model, \"ag_lora_adapter\")\n\nclassifier_state_dict = torch.load(\"ag_classifier_head.pth\", map_location=device, weights_only=True)\n\nlora_model.base_model.classifier.weight.data.copy_(classifier_state_dict[\"classifier.weight\"])\nlora_model.base_model.classifier.bias.data.copy_(classifier_state_dict[\"classifier.bias\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:40:04.249156Z","iopub.status.idle":"2025-04-03T08:40:04.249599Z","shell.execute_reply":"2025-04-03T08:40:04.249407Z"}},"outputs":[{"name":"stderr","text":"[codecarbon INFO @ 08:40:10] Energy consumed for RAM : 0.002056 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:40:10] Energy consumed for all CPUs : 0.007435 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:40:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:40:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:40:10] 0.009491 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:40:25] Energy consumed for RAM : 0.002105 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:40:25] Energy consumed for all CPUs : 0.007612 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:40:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:40:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:40:25] 0.009717 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:40:40] Energy consumed for RAM : 0.002154 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:40:40] Energy consumed for all CPUs : 0.007789 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:40:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:40:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:40:40] 0.009943 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:40:55] Energy consumed for RAM : 0.002203 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:40:55] Energy consumed for all CPUs : 0.007966 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:40:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:40:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:40:55] 0.010169 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:41:10] Energy consumed for RAM : 0.002252 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:41:10] Energy consumed for all CPUs : 0.008143 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:41:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:41:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:41:10] 0.010395 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:41:25] Energy consumed for RAM : 0.002301 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:41:25] Energy consumed for all CPUs : 0.008320 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:41:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:41:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:41:25] 0.010621 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:41:40] Energy consumed for RAM : 0.002350 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:41:40] Energy consumed for all CPUs : 0.008497 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:41:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:41:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:41:40] 0.010847 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:41:40] 0.004301 g.CO2eq/s mean an estimation of 135.63355295160565 kg.CO2eq/year\n[codecarbon INFO @ 08:41:55] Energy consumed for RAM : 0.002399 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:41:55] Energy consumed for all CPUs : 0.008674 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:41:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:41:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:41:55] 0.011073 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:42:10] Energy consumed for RAM : 0.002448 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:42:10] Energy consumed for all CPUs : 0.008851 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:42:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:42:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:42:10] 0.011299 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:42:25] Energy consumed for RAM : 0.002497 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:42:25] Energy consumed for all CPUs : 0.009028 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:42:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:42:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:42:25] 0.011525 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:42:40] Energy consumed for RAM : 0.002546 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:42:40] Energy consumed for all CPUs : 0.009205 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:42:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:42:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:42:40] 0.011751 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:42:55] Energy consumed for RAM : 0.002595 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:42:55] Energy consumed for all CPUs : 0.009382 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:42:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:42:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:42:55] 0.011977 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:43:10] Energy consumed for RAM : 0.002644 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:43:10] Energy consumed for all CPUs : 0.009559 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:43:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:43:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:43:10] 0.012203 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:43:25] Energy consumed for RAM : 0.002693 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:43:25] Energy consumed for all CPUs : 0.009736 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:43:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:43:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:43:25] 0.012429 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:43:40] Energy consumed for RAM : 0.002742 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:43:40] Energy consumed for all CPUs : 0.009913 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:43:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:43:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:43:40] 0.012655 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:43:40] 0.004301 g.CO2eq/s mean an estimation of 135.63859700816568 kg.CO2eq/year\n[codecarbon INFO @ 08:43:55] Energy consumed for RAM : 0.002791 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:43:55] Energy consumed for all CPUs : 0.010090 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:43:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:43:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:43:55] 0.012881 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:44:10] Energy consumed for RAM : 0.002840 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:44:10] Energy consumed for all CPUs : 0.010267 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:44:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:44:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:44:10] 0.013107 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:44:25] Energy consumed for RAM : 0.002889 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:44:25] Energy consumed for all CPUs : 0.010444 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:44:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:44:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:44:25] 0.013332 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:44:40] Energy consumed for RAM : 0.002938 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:44:40] Energy consumed for all CPUs : 0.010621 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:44:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:44:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:44:40] 0.013558 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:44:55] Energy consumed for RAM : 0.002987 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:44:55] Energy consumed for all CPUs : 0.010798 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:44:55] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:44:55] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:44:55] 0.013784 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:45:10] Energy consumed for RAM : 0.003036 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:45:10] Energy consumed for all CPUs : 0.010975 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:45:10] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:45:10] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:45:10] 0.014010 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:45:25] Energy consumed for RAM : 0.003084 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:45:25] Energy consumed for all CPUs : 0.011152 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:45:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:45:25] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:45:25] 0.014236 kWh of electricity used since the beginning.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## SST-2","metadata":{}},{"cell_type":"markdown","source":"### Ottenimento dei dati e preprocessing\n\nCarico il dataset SST-2, un dataset contenente esempi che consistono in frasi tratte da recensioni di film le cui etichette sono 1 se la recensione positiva, 0 altrimenti.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nsst_dataset = load_dataset('glue','sst2')\nprint(sst_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:13.327023Z","iopub.execute_input":"2025-03-23T20:46:13.327251Z","iopub.status.idle":"2025-03-23T20:46:22.250210Z","shell.execute_reply.started":"2025-03-23T20:46:13.327231Z","shell.execute_reply":"2025-03-23T20:46:22.249376Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78dad59c864c451a9e46e7953fd31206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f29a05bb91f49f4aa0d46f581e1a060"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ac05f326fe4a06889947d3b1ac5f70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae99d077757e484eaa569f41634de82a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6134d262e2bb4ac6bc91d3c5333c2d55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46057b615f25467fb9e0c2a2c3c048c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"638f896ca237425b8dea59b476a18bbe"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 67349\n    })\n    validation: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 872\n    })\n    test: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 1821\n    })\n})\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\nsst_data = sst_dataset['train'].shuffle(seed=42)\n\nsst_temp_sentences, sst_test_sentences, sst_temp_labels, sst_test_labels = train_test_split(\n                                                  sst_data['sentence'], \n                                                  sst_data['label'], \n                                                  test_size=4000, \n                                                  random_state=42,\n                                                  stratify=sst_data['label'])\n\nsst_train_sentences, sst_val_sentences, sst_train_labels, sst_val_labels = train_test_split(\n                                                  sst_temp_sentences, \n                                                  sst_temp_labels,\n                                                  train_size=20000,\n                                                  test_size=4000, \n                                                  random_state=42,\n                                                  stratify=sst_temp_labels)\n\nsst_train_sentences = sst_train_sentences[:10]\nsst_val_sentences = sst_val_sentences[:10]\nsst_test_sentences = sst_test_sentences[:10]\nsst_train_labels = sst_train_labels[:10]\nsst_val_labels  =sst_val_labels[:10]\nsst_test_labels=sst_test_labels[:10]\n\nprint(\"Dimensioni dei set:\")\nprint(f\"Train: {len(sst_train_sentences)}\")\nprint(f\"Validation: {len(sst_val_sentences)}\")\nprint(f\"Test: {len(sst_test_sentences)}\")\n\n# Verifica distribuzione delle etichette\nprint(\"\\nDistribuzione delle etichette:\")\nprint(f\"Train: {Counter(sst_train_labels)}\")\nprint(f\"Validation: {Counter(sst_val_labels)}\")\nprint(f\"Test: {Counter(sst_test_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:22.251348Z","iopub.execute_input":"2025-03-23T20:46:22.251746Z","iopub.status.idle":"2025-03-23T20:46:23.441815Z","shell.execute_reply.started":"2025-03-23T20:46:22.251704Z","shell.execute_reply":"2025-03-23T20:46:23.441089Z"}},"outputs":[{"name":"stdout","text":"Dimensioni dei set:\nTrain: 10\nValidation: 10\nTest: 10\n\nDistribuzione delle etichette:\nTrain: Counter({0: 6, 1: 4})\nValidation: Counter({0: 5, 1: 5})\nTest: Counter({1: 7, 0: 3})\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"Inizializzo il Tokenizer BERT per tokenizzare le frasi e creo i dataset personalizzati.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 128\n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\nsst_training_data = ClassificationDataset(sentences = sst_train_sentences,\n                           labels = sst_train_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nsst_validation_data = ClassificationDataset(sentences = sst_val_sentences,\n                           labels = sst_val_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nsst_test_data = ClassificationDataset(sentences = sst_test_sentences,\n                           labels = sst_test_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:23.466369Z","iopub.execute_input":"2025-03-23T20:46:23.466567Z","iopub.status.idle":"2025-03-23T20:46:23.614878Z","shell.execute_reply.started":"2025-03-23T20:46:23.466549Z","shell.execute_reply":"2025-03-23T20:46:23.614126Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### Addestramento del modello","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n\n# Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Pretrained model\nlora_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n\n# LoRA config\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=128,\n    lora_dropout=0.1,\n    target_modules=[\"query\", \"key\", \"value\"], \n    bias=\"none\",\n)\n\nlora_model = get_peft_model(lora_model, lora_config)\nlora_model.print_trainable_parameters()\n\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:23.615705Z","iopub.execute_input":"2025-03-23T20:46:23.615980Z","iopub.status.idle":"2025-03-23T20:46:24.290781Z","shell.execute_reply.started":"2025-03-23T20:46:23.615949Z","shell.execute_reply":"2025-03-23T20:46:24.290016Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,769,472 || all params: 111,253,250 || trainable%: 1.5905\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): BertForSequenceClassification(\n      (bert): BertModel(\n        (embeddings): BertEmbeddings(\n          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (token_type_embeddings): Embedding(2, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): BertEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (activation): Tanh()\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): Linear(in_features=768, out_features=2, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"for name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        param.requires_grad = True\n\nfor name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        print(f\"{name}: requires_grad = {param.requires_grad}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:24.291451Z","iopub.execute_input":"2025-03-23T20:46:24.291660Z","iopub.status.idle":"2025-03-23T20:46:24.298934Z","shell.execute_reply.started":"2025-03-23T20:46:24.291642Z","shell.execute_reply":"2025-03-23T20:46:24.298185Z"}},"outputs":[{"name":"stdout","text":"base_model.model.classifier.weight: requires_grad = True\nbase_model.model.classifier.bias: requires_grad = True\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 5e-4\nEPOCHS = 10\nBATCH_SIZE = 32\n\n# Creo i DataLoader\nsst_train_loader = DataLoader(sst_training_data, batch_size=BATCH_SIZE, shuffle=True)\nsst_val_loader = DataLoader(sst_validation_data, batch_size=BATCH_SIZE, shuffle=False)\nsst_test_loader = DataLoader(sst_test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(sst_train_loader) * EPOCHS\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n# Scheduler\nscheduler = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:24.299727Z","iopub.execute_input":"2025-03-23T20:46:24.299987Z","iopub.status.idle":"2025-03-23T20:46:24.318417Z","shell.execute_reply.started":"2025-03-23T20:46:24.299957Z","shell.execute_reply":"2025-03-23T20:46:24.317726Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"history, total_time, emissions = train_and_evaluate_model(\n    lora_model,\"sst\", sst_train_loader, sst_val_loader, optimizer, scheduler, device, epochs=EPOCHS\n) \nprint(f\"\\nBERT with LoRA Training Time: {total_time:.2f} seconds, {total_time/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:24.319331Z","iopub.execute_input":"2025-03-23T20:46:24.319582Z","iopub.status.idle":"2025-03-23T20:46:30.011205Z","shell.execute_reply.started":"2025-03-23T20:46:24.319547Z","shell.execute_reply":"2025-03-23T20:46:30.010506Z"}},"outputs":[{"name":"stderr","text":"[codecarbon INFO @ 20:46:24] [setup] RAM Tracking...\n[codecarbon INFO @ 20:46:24] [setup] CPU Tracking...\n[codecarbon WARNING @ 20:46:24] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon WARNING @ 20:46:25] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon INFO @ 20:46:25] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:46:25] [setup] GPU Tracking...\n[codecarbon INFO @ 20:46:25] Tracking Nvidia GPU via pynvml\n[codecarbon WARNING @ 20:46:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:46:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:46:25] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:46:25] >>> Tracker's metadata:\n[codecarbon INFO @ 20:46:25]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 20:46:25]   Python version: 3.10.12\n[codecarbon INFO @ 20:46:25]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 20:46:25]   Available RAM : 31.351 GB\n[codecarbon INFO @ 20:46:25]   CPU count: 4\n[codecarbon INFO @ 20:46:25]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:46:25]   GPU count: 1\n[codecarbon INFO @ 20:46:25]   GPU model: 1 x Tesla P100-PCIE-16GB\n[codecarbon INFO @ 20:46:28] Saving emissions data to file /kaggle/working/carbon_emissions/emissions.csv\n[codecarbon WARNING @ 20:46:28] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  8.95it/s, accuracy=0.6, loss=0.699]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.76it/s, accuracy=0.5, loss=0.685]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.66it/s, accuracy=0.5, loss=0.685]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 20.36it/s, accuracy=0.5, loss=0.697]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 1 epoche.\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.70it/s, accuracy=0.6, loss=0.665]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 20.13it/s, accuracy=0.5, loss=0.692]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 2 epoche.\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  9.63it/s, accuracy=0.6, loss=0.603]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 20.17it/s, accuracy=0.5, loss=0.687]\n[codecarbon INFO @ 20:46:29] Energy consumed for RAM : 0.000004 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 20:46:29] Energy consumed for all CPUs : 0.000016 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 20:46:29] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:46:30] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 20:46:30] 0.000020 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 3 epoche.\nEarly stopping attivato dopo 3 epoche senza miglioramenti\n\nEmissioni CO₂ totali: 0.0000 kg\n\nBERT with LoRA Training Time: 1.32 seconds, 0.02 minutes.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"### 3. Valutazione del modello\nValuto i modello calcolando la loss sul test set, l'accuracy e l'F1-score.","metadata":{}},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"sst_best_model_state.bin\"))\n\ntest_loss, test_acc, test_f1 = eval_model(lora_model, sst_test_loader, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.012028Z","iopub.execute_input":"2025-03-23T20:46:30.012295Z","iopub.status.idle":"2025-03-23T20:46:30.375252Z","shell.execute_reply.started":"2025-03-23T20:46:30.012261Z","shell.execute_reply":"2025-03-23T20:46:30.374083Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-34-cc014844e092>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_model.load_state_dict(torch.load(\"sst_best_model_state.bin\"))\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 20.40it/s, accuracy=0.3, loss=0.742]","output_type":"stream"},{"name":"stdout","text":"LoRA Fine-Tuning - Test loss: 0.7416, Accuracy: 0.3000, F1 score: 0.1385\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Memorizzazione dei risultati su Sentiment140\nadd_task_results(\n    task_name=\"sst\", \n    training_time=total_time,\n    emissions=emissions,\n    test_loss=test_loss,\n    test_acc=test_acc,\n    test_f1=test_f1,\n)\n\nperformance = pd.DataFrame(model_performance)\nprint(performance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.376237Z","iopub.execute_input":"2025-03-23T20:46:30.376555Z","iopub.status.idle":"2025-03-23T20:46:30.384599Z","shell.execute_reply.started":"2025-03-23T20:46:30.376521Z","shell.execute_reply":"2025-03-23T20:46:30.383871Z"}},"outputs":[{"name":"stdout","text":"     Task  Training Time  CO2 Emissions  Test Loss  Accuracy  F1 Score\n0  agnews       5.584955       0.000012   1.315260       0.6  0.709524\n1     sst       1.320940       0.000003   0.741614       0.3  0.138462\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"#### 4. Salvataggio del modulo lora","metadata":{}},{"cell_type":"code","source":"lora_model.save_pretrained(\"sst_lora_adapter\")\n\nclassifier_state_dict = {\n    \"classifier.weight\": lora_model.base_model.model.classifier.weight.cpu(),\n    \"classifier.bias\": lora_model.base_model.model.classifier.bias.cpu()\n}\n\ntorch.save(classifier_state_dict, \"sst_classifier_head.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.385355Z","iopub.execute_input":"2025-03-23T20:46:30.385624Z","iopub.status.idle":"2025-03-23T20:46:30.401461Z","shell.execute_reply.started":"2025-03-23T20:46:30.385591Z","shell.execute_reply":"2025-03-23T20:46:30.400597Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# from peft import PeftModel\n\n# base_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n# lora_model = PeftModel.from_pretrained(base_model, \"sst_lora_adapter\")\n\n# classifier_state_dict = torch.load(\"sst_classifier_head.pth\", map_location=device, weights_only=True)\n\n\n# lora_model.base_model.classifier.weight.data.copy_(classifier_state_dict[\"classifier.weight\"])\n# lora_model.base_model.classifier.bias.data.copy_(classifier_state_dict[\"classifier.bias\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.402235Z","iopub.execute_input":"2025-03-23T20:46:30.402551Z","iopub.status.idle":"2025-03-23T20:46:30.416342Z","shell.execute_reply.started":"2025-03-23T20:46:30.402521Z","shell.execute_reply":"2025-03-23T20:46:30.415588Z"}},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":"## EmoInt","metadata":{}},{"cell_type":"code","source":"def load_emoint_dataset(file_path):\n    label_map = {\"anger\": 0, \"joy\": 1, \"sadness\": 2, \"fear\": 3}\n    \n    df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"id\", \"sentence\", \"label\", \"intensity\"])\n    \n    df = df[[\"sentence\", \"label\"]]\n    df[\"label\"] = df[\"label\"].map(label_map)\n    \n    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    return df\n\n\nei_dataset = load_emoint_dataset(\"/kaggle/input/emoint-dataset/Emotion Intensity Dataset.txt\")\nprint(ei_dataset.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.417271Z","iopub.execute_input":"2025-03-23T20:46:30.417560Z","iopub.status.idle":"2025-03-23T20:46:30.488647Z","shell.execute_reply.started":"2025-03-23T20:46:30.417530Z","shell.execute_reply":"2025-03-23T20:46:30.487874Z"}},"outputs":[{"name":"stdout","text":"                                            sentence  label\n0  what does everyone have against sparkling wate...      1\n1  Or when they hmu on snap, and I'm like.. which...      0\n2  Can we get a shot of Lingys face at 1/4 time ?...      0\n3  I stepped into the shower and my spidey senses...      3\n4  @AaliyahLove69 I would be intimidated but I wo...      3\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\n# Divido i dati in training set, validation set e test set\nei_temp_sentences, ei_test_sentences, ei_temp_labels, ei_test_labels = train_test_split(\n                                                ei_dataset['sentence'],\n                                                ei_dataset['label'], \n                                                test_size=0.1, \n                                                random_state=42,\n                                                stratify=ei_dataset['label'])\n\nei_train_sentences, ei_val_sentences, ei_train_labels, ei_val_labels = train_test_split(\n                                                ei_temp_sentences,\n                                                ei_temp_labels,\n                                                test_size=0.1111,\n                                                random_state=42,\n                                                stratify=ei_temp_labels)\n\nei_train_sentences = ei_train_sentences.reset_index(drop=True)\nei_val_sentences = ei_val_sentences.reset_index(drop=True)\nei_test_sentences = ei_test_sentences.reset_index(drop=True)\nei_train_labels = ei_train_labels.reset_index(drop=True)\nei_val_labels = ei_val_labels.reset_index(drop=True)\nei_test_labels = ei_test_labels.reset_index(drop=True)\n\n\nei_train_sentences = ei_train_sentences[:10]\nei_val_sentences = ei_val_sentences[:10]\nei_test_sentences = ei_test_sentences[:10]\nei_train_labels = ei_train_labels[:10]\nei_val_labels  =ei_val_labels[:10]\nei_test_labels=ei_test_labels[:10]\n\nprint(\"Dimensioni dei set:\")\nprint(f\"Train: {len(ei_train_sentences)}\")\nprint(f\"Validation: {len(ei_val_sentences)}\")\nprint(f\"Test: {len(ei_test_sentences)}\")\n\n# Verifica distribuzione delle etichette\nprint(\"\\nDistribuzione delle etichette:\")\nprint(f\"Train: {Counter(ei_train_labels)}\")\nprint(f\"Validation: {Counter(ei_val_labels)}\")\nprint(f\"Test: {Counter(ei_test_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.493969Z","iopub.execute_input":"2025-03-23T20:46:30.494281Z","iopub.status.idle":"2025-03-23T20:46:30.519201Z","shell.execute_reply.started":"2025-03-23T20:46:30.494249Z","shell.execute_reply":"2025-03-23T20:46:30.518062Z"}},"outputs":[{"name":"stdout","text":"Dimensioni dei set:\nTrain: 10\nValidation: 10\nTest: 10\n\nDistribuzione delle etichette:\nTrain: Counter({1: 4, 3: 3, 0: 2, 2: 1})\nValidation: Counter({0: 4, 1: 3, 2: 2, 3: 1})\nTest: Counter({0: 3, 2: 3, 1: 3, 3: 1})\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Mappatura delle etichette numeriche in stringhe\nlabel_map = {\n    0: \"anger\",\n    1: \"joy\",\n    2: \"sadness\",\n    3: \"fear\"\n}\n\n# Conta la distribuzione delle etichette nel training set\ntrain_label_counts = Counter(ei_train_labels)\n\n# Converti le etichette numeriche in stringhe\nlabels = [label_map[label] for label in sorted(train_label_counts.keys())]\ncounts = [train_label_counts[label] for label in sorted(train_label_counts.keys())]\n\n# Creazione del grafico\nplt.figure(figsize=(5, 4))\nplt.bar(labels, counts, color='royalblue', width=0.5)  \nplt.xlabel(\"Classi\")\nplt.ylabel(\"Frequenza\")\nplt.title(\"Distribuzione delle etichette nel Training Set\")\nplt.xticks(labels, rotation=0, ha=\"center\")  # Imposta il testo orizzontale e centrato\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Mostra il grafico\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.520231Z","iopub.execute_input":"2025-03-23T20:46:30.520531Z","iopub.status.idle":"2025-03-23T20:46:30.524444Z","shell.execute_reply.started":"2025-03-23T20:46:30.520500Z","shell.execute_reply":"2025-03-23T20:46:30.523501Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 128\n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\nei_training_data = ClassificationDataset(sentences = ei_train_sentences,\n                           labels = ei_train_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nei_validation_data = ClassificationDataset(sentences = ei_val_sentences,\n                           labels = ei_val_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nei_test_data = ClassificationDataset(sentences = ei_test_sentences,\n                           labels = ei_test_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.525478Z","iopub.execute_input":"2025-03-23T20:46:30.525795Z","iopub.status.idle":"2025-03-23T20:46:30.685386Z","shell.execute_reply.started":"2025-03-23T20:46:30.525764Z","shell.execute_reply":"2025-03-23T20:46:30.684574Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n\n# Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Pretrained model\nlora_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n\n# LoRA config\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=512,\n    lora_dropout=0.3,\n    target_modules=[\"query\", \"key\", \"value\"],  \n    bias=\"none\",\n)\n\nlora_model = get_peft_model(lora_model, lora_config)\nlora_model.print_trainable_parameters()\n\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:30.686206Z","iopub.execute_input":"2025-03-23T20:46:30.686507Z","iopub.status.idle":"2025-03-23T20:46:31.140881Z","shell.execute_reply.started":"2025-03-23T20:46:30.686473Z","shell.execute_reply":"2025-03-23T20:46:31.140015Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,769,472 || all params: 111,254,788 || trainable%: 1.5905\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): BertForSequenceClassification(\n      (bert): BertModel(\n        (embeddings): BertEmbeddings(\n          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (token_type_embeddings): Embedding(2, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): BertEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.3, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.3, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.3, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (activation): Tanh()\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): Linear(in_features=768, out_features=4, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"for name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        param.requires_grad = True\n\nfor name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        print(f\"{name}: requires_grad = {param.requires_grad}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:31.141826Z","iopub.execute_input":"2025-03-23T20:46:31.142166Z","iopub.status.idle":"2025-03-23T20:46:31.149418Z","shell.execute_reply.started":"2025-03-23T20:46:31.142132Z","shell.execute_reply":"2025-03-23T20:46:31.148596Z"}},"outputs":[{"name":"stdout","text":"base_model.model.classifier.weight: requires_grad = True\nbase_model.model.classifier.bias: requires_grad = True\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 1e-4\nEPOCHS = 10\nBATCH_SIZE = 32\n\n# Creo i DataLoader\nei_train_loader = DataLoader(ei_training_data, batch_size=BATCH_SIZE, shuffle=True)\nei_val_loader = DataLoader(ei_validation_data, batch_size=BATCH_SIZE, shuffle=False)\nei_test_loader = DataLoader(ei_test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(ei_train_loader) * EPOCHS\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n# Scheduler\nscheduler = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0.2 * total_steps,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:31.150227Z","iopub.execute_input":"2025-03-23T20:46:31.150518Z","iopub.status.idle":"2025-03-23T20:46:31.168546Z","shell.execute_reply.started":"2025-03-23T20:46:31.150487Z","shell.execute_reply":"2025-03-23T20:46:31.167878Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"history, total_time, emissions = train_and_evaluate_model(\n    lora_model,\"ei\", ei_train_loader, ei_val_loader, optimizer, scheduler, device, epochs=EPOCHS\n) \nprint(f\"\\nBERT with LoRA Training Time: {total_time:.2f} seconds, {total_time/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T08:36:35.185047Z","iopub.execute_input":"2025-04-03T08:36:35.185408Z","iopub.status.idle":"2025-04-03T08:36:35.210768Z","shell.execute_reply.started":"2025-04-03T08:36:35.185379Z","shell.execute_reply":"2025-04-03T08:36:35.209438Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-179294b51a68>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history, total_time, emissions = train_and_evaluate_model(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlora_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ei\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mei_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mei_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m ) \n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nBERT with LoRA Training Time: {total_time:.2f} seconds, {total_time/60:.2f} minutes.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'ei_train_loader' is not defined"],"ename":"NameError","evalue":"name 'ei_train_loader' is not defined","output_type":"error"},{"name":"stderr","text":"[codecarbon INFO @ 08:36:40] Energy consumed for RAM : 0.001371 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 08:36:40] Energy consumed for all CPUs : 0.004957 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 08:36:40] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 08:36:40] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 08:36:40] 0.006327 kWh of electricity used since the beginning.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"ei_best_model_state.bin\")) \n\ntest_loss, test_acc, test_f1 = eval_model(lora_model, ei_test_loader, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:40.228292Z","iopub.execute_input":"2025-03-23T20:46:40.228605Z","iopub.status.idle":"2025-03-23T20:46:40.592710Z","shell.execute_reply.started":"2025-03-23T20:46:40.228574Z","shell.execute_reply":"2025-03-23T20:46:40.591893Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-47-52a61a41723e>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_model.load_state_dict(torch.load(\"ei_best_model_state.bin\"))\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 19.48it/s, accuracy=0.2, loss=1.51]","output_type":"stream"},{"name":"stdout","text":"LoRA Fine-Tuning - Test loss: 1.5141, Accuracy: 0.2000, F1 score: 0.1091\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# Memorizzazione dei risultati su Sentiment140\nadd_task_results(\n    task_name=\"ei\", \n    training_time=total_time,\n    emissions=emissions,\n    test_loss=test_loss,\n    test_acc=test_acc,\n    test_f1=test_f1,\n)\n\nperformance = pd.DataFrame(model_performance)\nprint(performance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:40.593515Z","iopub.execute_input":"2025-03-23T20:46:40.593780Z","iopub.status.idle":"2025-03-23T20:46:40.601044Z","shell.execute_reply.started":"2025-03-23T20:46:40.593758Z","shell.execute_reply":"2025-03-23T20:46:40.600266Z"}},"outputs":[{"name":"stdout","text":"     Task  Training Time  CO2 Emissions  Test Loss  Accuracy  F1 Score\n0  agnews       5.584955       0.000012   1.315260       0.6  0.709524\n1     sst       1.320940       0.000003   0.741614       0.3  0.138462\n2      ei       4.548648       0.000010   1.514121       0.2  0.109091\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"lora_model.save_pretrained(\"ei_lora_adapter\")\n\nclassifier_state_dict = {\n    \"classifier.weight\": lora_model.base_model.model.classifier.weight.cpu(),\n    \"classifier.bias\": lora_model.base_model.model.classifier.bias.cpu()\n}\n\ntorch.save(classifier_state_dict, \"ei_classifier_head.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:40.601920Z","iopub.execute_input":"2025-03-23T20:46:40.602195Z","iopub.status.idle":"2025-03-23T20:46:40.632599Z","shell.execute_reply.started":"2025-03-23T20:46:40.602171Z","shell.execute_reply":"2025-03-23T20:46:40.631817Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# from peft import PeftModel\n\n# base_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n# lora_model = PeftModel.from_pretrained(base_model, \"ei__lora_adapter\")\n\n# classifier_state_dict = torch.load(\"ei_classifier_head.pth\", map_location=device, weights_only=True)\n\n\n# lora_model.base_model.classifier.weight.data.copy_(classifier_state_dict[\"classifier.weight\"])\n# lora_model.base_model.classifier.bias.data.copy_(classifier_state_dict[\"classifier.bias\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:40.633525Z","iopub.execute_input":"2025-03-23T20:46:40.633811Z","iopub.status.idle":"2025-03-23T20:46:40.650396Z","shell.execute_reply.started":"2025-03-23T20:46:40.633781Z","shell.execute_reply":"2025-03-23T20:46:40.649263Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"## MNLI","metadata":{}},{"cell_type":"markdown","source":"### Ottenimento del dataset e preprocessing","metadata":{}},{"cell_type":"markdown","source":"Carico il dataset MNLI, che contiene esempi che consistono in una coppia di frasi (premessa e ipotesi) etichettate con **Entailment**, **Contradiction**, **Neutral**.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n\n# Carico il datast\nmnli_dataset = load_dataset('glue', 'mnli')\nprint(mnli_dataset)\n\n# Divido i dati in training set, validation set e test set\nmnli_data = mnli_dataset['train'].shuffle(seed=42)\n\nmnli_temp_premises, mnli_test_premises, mnli_temp_hypotheses, mnli_test_hypotheses, mnli_temp_labels, mnli_test_labels = train_test_split(\n                                                  mnli_data['premise'], \n                                                  mnli_data['hypothesis'],                \n                                                  mnli_data['label'], \n                                                  test_size=3000, \n                                                  random_state=42,\n                                                  stratify=mnli_data['label'])\n\nmnli_train_premises, mnli_val_premises, mnli_train_hypotheses, mnli_val_hypotheses, mnli_train_labels, mnli_val_labels = train_test_split(\n                                                  mnli_temp_premises, \n                                                  mnli_temp_hypotheses,\n                                                  mnli_temp_labels,\n                                                  train_size=45000,\n                                                  test_size=3000, \n                                                  random_state=42,\n                                                  stratify=mnli_temp_labels)\n\nmnli_train_premises = mnli_train_premises[:10]\nmnli_val_premises = mnli_val_premises[:10]\nmnli_test_premises = mnli_test_premises[:10]\nmnli_train_hypotheses = mnli_train_hypotheses[:10]\nmnli_val_hypotheses = mnli_val_hypotheses[:10]\nmnli_test_hypotheses = mnli_test_hypotheses[:10]\nmnli_train_labels = mnli_train_labels[:10]\nmnli_val_labels  = mnli_val_labels[:10]\nmnli_test_labels=mnli_test_labels[:10]\n\nprint(\"Dimensioni dei set:\")\nprint(f\"Train: {len(mnli_train_premises)}\")\nprint(f\"Validation: {len(mnli_val_premises)}\")\nprint(f\"Test: {len(mnli_test_premises)}\")\n\n# Verifica distribuzione delle etichette\nprint(\"\\nDistribuzione delle etichette:\")\nprint(f\"Train: {Counter(mnli_train_labels)}\")\nprint(f\"Validation: {Counter(mnli_val_labels)}\")\nprint(f\"Test: {Counter(mnli_test_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:40.651093Z","iopub.execute_input":"2025-03-23T20:46:40.651385Z","iopub.status.idle":"2025-03-23T20:46:54.356343Z","shell.execute_reply.started":"2025-03-23T20:46:40.651358Z","shell.execute_reply":"2025-03-23T20:46:54.355548Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/52.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02b5141e1ce841bbb536abde8102c20f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)alidation_matched-00000-of-00001.parquet:   0%|          | 0.00/1.21M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60dee2e4df0e44828e3b4c871c03f891"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)dation_mismatched-00000-of-00001.parquet:   0%|          | 0.00/1.25M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c72095d2b0594d7c91848514a1b6c36b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test_matched-00000-of-00001.parquet:   0%|          | 0.00/1.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d382f5b4957c46a4929ac7a93d1a994c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test_mismatched-00000-of-00001.parquet:   0%|          | 0.00/1.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"252c9693af804d08a2ce731ba92d420a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b85ce3a6f76c4c9baf85504043f8bf55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51beddfc54644e28b242e4d6b946c22a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b13a65641484e23a2f1e0a8b2184113"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_matched split:   0%|          | 0/9796 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23254bca5eee41fdb76b302c5a4d9bfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_mismatched split:   0%|          | 0/9847 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9373ca4bdc648578f7d1c136c589248"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 392702\n    })\n    validation_matched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9815\n    })\n    validation_mismatched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9832\n    })\n    test_matched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9796\n    })\n    test_mismatched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9847\n    })\n})\nDimensioni dei set:\nTrain: 10\nValidation: 10\nTest: 10\n\nDistribuzione delle etichette:\nTrain: Counter({0: 5, 1: 3, 2: 2})\nValidation: Counter({1: 6, 2: 3, 0: 1})\nTest: Counter({1: 5, 2: 3, 0: 2})\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"Creo una classe Dataset personalizzata in cui viene effettuata la tokenizzaione delle recensioni e la conversione dei dati in tensori.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass NLIDataset(Dataset):\n\n    def __init__(self, premises, hypotheses , labels, tokenizer, max_len):\n        self.premises = premises\n        self.hypotheses = hypotheses\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.premises)\n    \n    def __getitem__(self,index):\n        premise = self.premises[index]\n        hyphotesis = self.hypotheses[index]\n        label = self.labels[index]\n        \n        encoding = self.tokenizer.encode_plus(\n            premise,\n            hyphotesis,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors='pt')\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding[\"token_type_ids\"].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:54.357306Z","iopub.execute_input":"2025-03-23T20:46:54.357638Z","iopub.status.idle":"2025-03-23T20:46:54.390706Z","shell.execute_reply.started":"2025-03-23T20:46:54.357603Z","shell.execute_reply":"2025-03-23T20:46:54.389909Z"}},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"Inizializzo il Tokenizer BERT per tokenizzare le frasi e creo i dataset personalizzati.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 512\n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\nmnli_training_data = NLIDataset(premises = mnli_train_premises,\n                            hypotheses = mnli_train_hypotheses,\n                            labels = mnli_train_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)\n\nmnli_validation_data = NLIDataset(premises = mnli_val_premises,\n                            hypotheses = mnli_val_hypotheses,\n                            labels = mnli_val_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)\n\nmnli_test_data = NLIDataset(premises = mnli_test_premises,\n                            hypotheses = mnli_test_hypotheses,\n                            labels = mnli_test_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:54.391846Z","iopub.execute_input":"2025-03-23T20:46:54.392195Z","iopub.status.idle":"2025-03-23T20:46:54.617729Z","shell.execute_reply.started":"2025-03-23T20:46:54.392161Z","shell.execute_reply":"2025-03-23T20:46:54.617054Z"}},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":"### Addestramento del modello","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n\n# Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Pretrained model\nlora_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n\n# LoRA config\nlora_config = LoraConfig(\n    r=32,\n    lora_alpha=128,\n    lora_dropout=0.2,\n    target_modules=[\"query\", \"key\", \"value\"], \n    bias=\"none\",\n)\n\nlora_model = get_peft_model(lora_model, lora_config)\nlora_model.print_trainable_parameters()\n\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:54.618491Z","iopub.execute_input":"2025-03-23T20:46:54.618711Z","iopub.status.idle":"2025-03-23T20:46:55.110049Z","shell.execute_reply.started":"2025-03-23T20:46:54.618691Z","shell.execute_reply":"2025-03-23T20:46:55.109190Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 1,769,472 || all params: 111,254,019 || trainable%: 1.5905\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): BertForSequenceClassification(\n      (bert): BertModel(\n        (embeddings): BertEmbeddings(\n          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (token_type_embeddings): Embedding(2, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): BertEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.2, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=32, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=32, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (activation): Tanh()\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): Linear(in_features=768, out_features=3, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"for name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        param.requires_grad = True\n\nfor name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        print(f\"{name}: requires_grad = {param.requires_grad}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:55.111129Z","iopub.execute_input":"2025-03-23T20:46:55.111453Z","iopub.status.idle":"2025-03-23T20:46:55.118703Z","shell.execute_reply.started":"2025-03-23T20:46:55.111419Z","shell.execute_reply":"2025-03-23T20:46:55.117858Z"}},"outputs":[{"name":"stdout","text":"base_model.model.classifier.weight: requires_grad = True\nbase_model.model.classifier.bias: requires_grad = True\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 2e-4\nEPOCHS = 10\nBATCH_SIZE = 32\n\n# Creo i DataLoader\nmnli_train_loader = DataLoader(mnli_training_data, batch_size=BATCH_SIZE, shuffle=True)\nmnli_val_loader = DataLoader(mnli_validation_data, batch_size=BATCH_SIZE, shuffle=False)\nmnli_test_loader = DataLoader(mnli_test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(mnli_train_loader) * EPOCHS\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n\n# Scheduler\nscheduler = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:55.119566Z","iopub.execute_input":"2025-03-23T20:46:55.119799Z","iopub.status.idle":"2025-03-23T20:46:55.137448Z","shell.execute_reply.started":"2025-03-23T20:46:55.119780Z","shell.execute_reply":"2025-03-23T20:46:55.136670Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"history, total_time, emissions = train_and_evaluate_model(\n    lora_model,\"mnli\", mnli_train_loader, mnli_val_loader, optimizer, scheduler, device, epochs=EPOCHS\n) \nprint(f\"\\nBERT with LoRA Training Time: {total_time:.2f} seconds, {total_time/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:46:55.138329Z","iopub.execute_input":"2025-03-23T20:46:55.138562Z","iopub.status.idle":"2025-03-23T20:47:02.831007Z","shell.execute_reply.started":"2025-03-23T20:46:55.138542Z","shell.execute_reply":"2025-03-23T20:47:02.830363Z"}},"outputs":[{"name":"stderr","text":"[codecarbon INFO @ 20:46:55] [setup] RAM Tracking...\n[codecarbon INFO @ 20:46:55] [setup] CPU Tracking...\n[codecarbon WARNING @ 20:46:55] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon WARNING @ 20:46:56] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon INFO @ 20:46:56] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:46:56] [setup] GPU Tracking...\n[codecarbon INFO @ 20:46:56] Tracking Nvidia GPU via pynvml\n[codecarbon WARNING @ 20:46:56] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:46:56] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:46:56] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:46:56] >>> Tracker's metadata:\n[codecarbon INFO @ 20:46:56]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 20:46:56]   Python version: 3.10.12\n[codecarbon INFO @ 20:46:56]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 20:46:56]   Available RAM : 31.351 GB\n[codecarbon INFO @ 20:46:56]   CPU count: 4\n[codecarbon INFO @ 20:46:56]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:46:56]   GPU count: 1\n[codecarbon INFO @ 20:46:56]   GPU model: 1 x Tesla P100-PCIE-16GB\n[codecarbon INFO @ 20:46:59] Saving emissions data to file /kaggle/working/carbon_emissions/emissions.csv\n[codecarbon WARNING @ 20:46:59] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  2.12it/s, accuracy=0.2, loss=1.18]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00,  5.53it/s, accuracy=0.6, loss=1.09]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  2.12it/s, accuracy=0.6, loss=0.986]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00,  5.56it/s, accuracy=0.2, loss=1.16]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 1 epoche.\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  2.12it/s, accuracy=0.4, loss=1.09]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00,  5.56it/s, accuracy=0.1, loss=1.23]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 2 epoche.\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  2.12it/s, accuracy=0.5, loss=0.972]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00,  5.52it/s, accuracy=0.1, loss=1.28]\n[codecarbon INFO @ 20:47:02] Energy consumed for RAM : 0.000011 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 20:47:02] Energy consumed for all CPUs : 0.000039 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 20:47:02] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:47:02] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 20:47:02] 0.000050 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 3 epoche.\nEarly stopping attivato dopo 3 epoche senza miglioramenti\n\nEmissioni CO₂ totali: 0.0000 kg\n\nBERT with LoRA Training Time: 3.33 seconds, 0.06 minutes.\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"### Valutazione del modello\nValuto i modello calcolando la loss sul test set, l'accuracy e l'F1-score.","metadata":{}},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"mnli_best_model_state.bin\"))\n            \ntest_loss, test_acc, test_f1 = eval_model(lora_model, mnli_test_loader, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:02.831983Z","iopub.execute_input":"2025-03-23T20:47:02.832309Z","iopub.status.idle":"2025-03-23T20:47:03.317765Z","shell.execute_reply.started":"2025-03-23T20:47:02.832270Z","shell.execute_reply":"2025-03-23T20:47:03.316881Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-58-855da658c16c>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_model.load_state_dict(torch.load(\"mnli_best_model_state.bin\"))\nEvaluating: 100%|██████████| 1/1 [00:00<00:00,  5.47it/s, accuracy=0.5, loss=1.11]","output_type":"stream"},{"name":"stdout","text":"LoRA Fine-Tuning - Test loss: 1.1082, Accuracy: 0.5000, F1 score: 0.3333\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# Memorizzazione dei risultati su Sentiment140\nadd_task_results(\n    task_name=\"mnli\", \n    training_time=total_time,\n    emissions=emissions,\n    test_loss=test_loss,\n    test_acc=test_acc,\n    test_f1=test_f1,\n)\n\nperformance = pd.DataFrame(model_performance)\nprint(performance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:03.318717Z","iopub.execute_input":"2025-03-23T20:47:03.318996Z","iopub.status.idle":"2025-03-23T20:47:03.326912Z","shell.execute_reply.started":"2025-03-23T20:47:03.318972Z","shell.execute_reply":"2025-03-23T20:47:03.326013Z"}},"outputs":[{"name":"stdout","text":"     Task  Training Time  CO2 Emissions  Test Loss  Accuracy  F1 Score\n0  agnews       5.584955       0.000012   1.315260       0.6  0.709524\n1     sst       1.320940       0.000003   0.741614       0.3  0.138462\n2      ei       4.548648       0.000010   1.514121       0.2  0.109091\n3    mnli       3.331646       0.000007   1.108231       0.5  0.333333\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"### Salvataggio dell'adapter LoRA","metadata":{}},{"cell_type":"code","source":"lora_model.save_pretrained(\"mnli_lora_adapter\")\n\nclassifier_state_dict = {\n    \"classifier.weight\": lora_model.base_model.model.classifier.weight.cpu(),\n    \"classifier.bias\": lora_model.base_model.model.classifier.bias.cpu()\n}\n\ntorch.save(classifier_state_dict, \"mnli_classifier_head.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:03.327800Z","iopub.execute_input":"2025-03-23T20:47:03.328074Z","iopub.status.idle":"2025-03-23T20:47:03.342872Z","shell.execute_reply.started":"2025-03-23T20:47:03.328040Z","shell.execute_reply":"2025-03-23T20:47:03.342077Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# from peft import PeftModel\n\n# base_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n# lora_model = PeftModel.from_pretrained(base_model, \"mnli_lora_adapter\")\n\n# classifier_state_dict = torch.load(\"mnli_classifier_head.pth\", map_location=device, weights_only=True)\n\n\n# lora_model.base_model.classifier.weight.data.copy_(classifier_state_dict[\"classifier.weight\"])\n# lora_model.base_model.classifier.bias.data.copy_(classifier_state_dict[\"classifier.bias\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:03.343723Z","iopub.execute_input":"2025-03-23T20:47:03.344014Z","iopub.status.idle":"2025-03-23T20:47:03.356242Z","shell.execute_reply.started":"2025-03-23T20:47:03.343980Z","shell.execute_reply":"2025-03-23T20:47:03.355454Z"}},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"## PAWS","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Carico il datast\npaws_dataset = load_dataset(\"google-research-datasets/paws\", \"labeled_final\")\nprint(paws_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:03.356954Z","iopub.execute_input":"2025-03-23T20:47:03.357163Z","iopub.status.idle":"2025-03-23T20:47:06.621754Z","shell.execute_reply.started":"2025-03-23T20:47:03.357144Z","shell.execute_reply":"2025-03-23T20:47:06.621035Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/9.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e62501d5a76a474eab59c833d57e1806"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/8.43M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a515e89ddfb7485eae14497fdc8cd474"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5df4c5ba93904c7295edeaf070644204"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5622dcddab5b4349a925a38272dc2179"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/49401 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79ad90d35f034ad1be530206674cec43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7893dd8b38a249c09938586f25c918b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d33a3bf3d935425faa25125949d4e137"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'sentence1', 'sentence2', 'label'],\n        num_rows: 49401\n    })\n    test: Dataset({\n        features: ['id', 'sentence1', 'sentence2', 'label'],\n        num_rows: 8000\n    })\n    validation: Dataset({\n        features: ['id', 'sentence1', 'sentence2', 'label'],\n        num_rows: 8000\n    })\n})\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\n\npaws_train_set = paws_dataset[\"train\"]\npaws_val_set = paws_dataset[\"validation\"]\npaws_test_set = paws_dataset[\"test\"]\n\npaws_train_sentences1, paws_train_sentences2, paws_train_labels = paws_train_set['sentence1'], paws_train_set['sentence2'], paws_train_set['label']\npaws_val_sentences1, paws_val_sentences2, paws_val_labels = paws_val_set['sentence1'], paws_val_set['sentence2'], paws_val_set['label']\npaws_test_sentences1, paws_test_sentences2, paws_test_labels = paws_test_set['sentence1'], paws_test_set['sentence2'], paws_test_set['label']\n\n\npaws_train_sentences1 = paws_train_sentences1[:10]\npaws_val_sentences1 = paws_val_sentences1[:10]\npaws_test_sentences1 = paws_test_sentences1[:10]\npaws_train_sentences2 = paws_train_sentences2[:10]\npaws_val_sentences2 = paws_val_sentences2[:10]\npaws_test_sentences2 = paws_test_sentences2[:10]\npaws_train_labels = paws_train_labels[:10]\npaws_val_labels  = paws_val_labels[:10]\npaws_test_labels=paws_test_labels[:10]\n\nprint(\"Dimensioni dei set:\")\nprint(f\"Train: {len(paws_train_sentences1)}\")\nprint(f\"Validation: {len(paws_val_sentences1)}\")\nprint(f\"Test: {len(paws_test_sentences1)}\")\n\n# Verifica distribuzione delle etichette\nprint(\"\\nDistribuzione delle etichette:\")\nprint(f\"Train: {Counter(paws_train_labels)}\")\nprint(f\"Validation: {Counter(paws_val_labels)}\")\nprint(f\"Test: {Counter(paws_test_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:06.622505Z","iopub.execute_input":"2025-03-23T20:47:06.622710Z","iopub.status.idle":"2025-03-23T20:47:06.796790Z","shell.execute_reply.started":"2025-03-23T20:47:06.622690Z","shell.execute_reply":"2025-03-23T20:47:06.796029Z"}},"outputs":[{"name":"stdout","text":"Dimensioni dei set:\nTrain: 10\nValidation: 10\nTest: 10\n\nDistribuzione delle etichette:\nTrain: Counter({0: 5, 1: 5})\nValidation: Counter({1: 8, 0: 2})\nTest: Counter({0: 6, 1: 4})\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 256 \n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\npaws_training_data = NLIDataset(premises = paws_train_sentences1,\n                            hypotheses = paws_train_sentences2,\n                            labels = paws_train_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)\n\npaws_validation_data = NLIDataset(premises = paws_val_sentences1,\n                            hypotheses = paws_val_sentences2,\n                            labels = paws_val_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)\n\npaws_test_data = NLIDataset(premises = paws_test_sentences1,\n                            hypotheses = paws_test_sentences2,\n                            labels = paws_test_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:06.797550Z","iopub.execute_input":"2025-03-23T20:47:06.797778Z","iopub.status.idle":"2025-03-23T20:47:06.930238Z","shell.execute_reply.started":"2025-03-23T20:47:06.797758Z","shell.execute_reply":"2025-03-23T20:47:06.929546Z"}},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":"### Addestramento del modello","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\nfrom transformers import BertForSequenceClassification, AutoModelForSequenceClassification\n\n# Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Pretrained model\nlora_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n\n# LoRA config\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=128,\n    lora_dropout=0.3,\n    target_modules=[\"query\", \"key\", \"value\"], \n    bias=\"none\",\n)\n\nlora_model = get_peft_model(lora_model, lora_config)\nlora_model.print_trainable_parameters()\n\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:06.931017Z","iopub.execute_input":"2025-03-23T20:47:06.931259Z","iopub.status.idle":"2025-03-23T20:47:07.374127Z","shell.execute_reply.started":"2025-03-23T20:47:06.931237Z","shell.execute_reply":"2025-03-23T20:47:07.373265Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 884,736 || all params: 110,368,514 || trainable%: 0.8016\n","output_type":"stream"},{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): BertForSequenceClassification(\n      (bert): BertModel(\n        (embeddings): BertEmbeddings(\n          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (token_type_embeddings): Embedding(2, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): BertEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.3, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.3, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.3, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (activation): Tanh()\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): Linear(in_features=768, out_features=2, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"for name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        param.requires_grad = True\n\nfor name, param in lora_model.named_parameters():\n    if \"classifier\" in name:\n        print(f\"{name}: requires_grad = {param.requires_grad}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:07.374922Z","iopub.execute_input":"2025-03-23T20:47:07.375161Z","iopub.status.idle":"2025-03-23T20:47:07.382006Z","shell.execute_reply.started":"2025-03-23T20:47:07.375132Z","shell.execute_reply":"2025-03-23T20:47:07.381158Z"}},"outputs":[{"name":"stdout","text":"base_model.model.classifier.weight: requires_grad = True\nbase_model.model.classifier.bias: requires_grad = True\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Parametri principali\nlearning_rate = 5e-5\nEPOCHS = 10\nBATCH_SIZE = 32\n\n\n# Creo i DataLoader\npaws_train_loader = DataLoader(paws_training_data, batch_size=BATCH_SIZE, shuffle=True)\npaws_val_loader = DataLoader(paws_validation_data, batch_size=BATCH_SIZE, shuffle=False)\npaws_test_loader = DataLoader(paws_test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(paws_train_loader) * EPOCHS\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n\n# Scheduler\nscheduler = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:07.382965Z","iopub.execute_input":"2025-03-23T20:47:07.383245Z","iopub.status.idle":"2025-03-23T20:47:07.398681Z","shell.execute_reply.started":"2025-03-23T20:47:07.383216Z","shell.execute_reply":"2025-03-23T20:47:07.397936Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"history, total_time, emissions = train_and_evaluate_model(\n    lora_model,\"paws\", paws_train_loader, paws_val_loader, optimizer, scheduler, device, epochs=EPOCHS\n) \nprint(f\"\\nBERT with LoRA Training Time: {total_time:.2f} seconds, {total_time/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:07.404512Z","iopub.execute_input":"2025-03-23T20:47:07.404727Z","iopub.status.idle":"2025-03-23T20:47:17.679809Z","shell.execute_reply.started":"2025-03-23T20:47:07.404709Z","shell.execute_reply":"2025-03-23T20:47:17.678882Z"}},"outputs":[{"name":"stderr","text":"[codecarbon INFO @ 20:47:07] [setup] RAM Tracking...\n[codecarbon INFO @ 20:47:07] [setup] CPU Tracking...\n[codecarbon WARNING @ 20:47:07] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon WARNING @ 20:47:08] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon INFO @ 20:47:08] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:47:08] [setup] GPU Tracking...\n[codecarbon INFO @ 20:47:08] Tracking Nvidia GPU via pynvml\n[codecarbon WARNING @ 20:47:08] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:47:08] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon WARNING @ 20:47:08] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:47:08] >>> Tracker's metadata:\n[codecarbon INFO @ 20:47:08]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 20:47:08]   Python version: 3.10.12\n[codecarbon INFO @ 20:47:08]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 20:47:08]   Available RAM : 31.351 GB\n[codecarbon INFO @ 20:47:08]   CPU count: 4\n[codecarbon INFO @ 20:47:08]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 20:47:08]   GPU count: 1\n[codecarbon INFO @ 20:47:08]   GPU model: 1 x Tesla P100-PCIE-16GB\n[codecarbon INFO @ 20:47:11] Saving emissions data to file /kaggle/working/carbon_emissions/emissions.csv\n[codecarbon WARNING @ 20:47:11] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.28it/s, accuracy=0.6, loss=0.681]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.56it/s, accuracy=0.3, loss=0.698]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.52it/s, accuracy=0.6, loss=0.666]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.58it/s, accuracy=0.5, loss=0.692]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.41it/s, accuracy=0.7, loss=0.628]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.66it/s, accuracy=0.4, loss=0.693]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 1 epoche.\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.57it/s, accuracy=0.4, loss=0.739]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.64it/s, accuracy=0.4, loss=0.693]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 2 epoche.\n\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.59it/s, accuracy=0.6, loss=0.686]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.42it/s, accuracy=0.5, loss=0.692]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.60it/s, accuracy=0.5, loss=0.665]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.59it/s, accuracy=0.5, loss=0.691]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.59it/s, accuracy=0.4, loss=0.712]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.69it/s, accuracy=0.5, loss=0.691]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 1 epoche.\n\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.59it/s, accuracy=0.6, loss=0.672]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s, accuracy=0.6, loss=0.692]\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 2 epoche.\n\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training  : 100%|██████████| 1/1 [00:00<00:00,  4.60it/s, accuracy=0.5, loss=0.736]\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.67it/s, accuracy=0.5, loss=0.692]\n[codecarbon INFO @ 20:47:17] Energy consumed for RAM : 0.000019 kWh. RAM Power : 11.756441116333008 W\n[codecarbon INFO @ 20:47:17] Energy consumed for all CPUs : 0.000070 kWh. Total CPU Power : 42.5 W\n[codecarbon WARNING @ 20:47:17] Failed to retrieve gpu total energy consumption\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/codecarbon/core/gpu.py\", line 116, in _get_total_energy_consumption\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 2039, in nvmlDeviceGetTotalEnergyConsumption\n    _nvmlCheckReturn(ret)\n  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 765, in _nvmlCheckReturn\n    raise NVMLError(ret)\npynvml.nvml.NVMLError_NotSupported: Not Supported\n[codecarbon INFO @ 20:47:17] Energy consumed for all GPUs : 0.000000 kWh. Total GPU Power : 0.0 W\n[codecarbon INFO @ 20:47:17] 0.000089 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"La loss sul validation set non è migliorata per 3 epoche.\nEarly stopping attivato dopo 3 epoche senza miglioramenti\n\nEmissioni CO₂ totali: 0.0000 kg\n\nBERT with LoRA Training Time: 5.91 seconds, 0.10 minutes.\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"paws_best_model_state.bin\"))\n\ntest_loss, test_acc, test_f1 = eval_model(lora_model, paws_test_loader, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:17.681062Z","iopub.execute_input":"2025-03-23T20:47:17.681379Z","iopub.status.idle":"2025-03-23T20:47:18.085768Z","shell.execute_reply.started":"2025-03-23T20:47:17.681347Z","shell.execute_reply":"2025-03-23T20:47:18.084864Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-69-177b40d96f6d>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_model.load_state_dict(torch.load(\"paws_best_model_state.bin\"))\nEvaluating: 100%|██████████| 1/1 [00:00<00:00, 10.23it/s, accuracy=0.3, loss=0.698]","output_type":"stream"},{"name":"stdout","text":"LoRA Fine-Tuning - Test loss: 0.6976, Accuracy: 0.3000, F1 score: 0.3071\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"# Memorizzazione dei risultati su Sentiment140\nadd_task_results(\n    task_name=\"paws\", \n    training_time=total_time,\n    emissions=emissions,\n    test_loss=test_loss,\n    test_acc=test_acc,\n    test_f1=test_f1,\n)\n\nperformance = pd.DataFrame(model_performance)\nprint(performance)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:18.086933Z","iopub.execute_input":"2025-03-23T20:47:18.087338Z","iopub.status.idle":"2025-03-23T20:47:20.462751Z","shell.execute_reply.started":"2025-03-23T20:47:18.087302Z","shell.execute_reply":"2025-03-23T20:47:20.461938Z"}},"outputs":[{"name":"stdout","text":"           Task  Training Time  CO2 Emissions  Test Loss  Accuracy  F1 Score\n0        agnews       5.584955       0.000012   1.315260       0.6  0.709524\n1           sst       1.320940       0.000003   0.741614       0.3  0.138462\n2            ei       4.548648       0.000010   1.514121       0.2  0.109091\n3          mnli       3.331646       0.000007   1.108231       0.5  0.333333\n4  sentiment140       5.909914       0.000012   0.697641       0.3  0.307071\n","output_type":"stream"}],"execution_count":70},{"cell_type":"markdown","source":"### Salvataggio dell'adapter LoRA","metadata":{}},{"cell_type":"code","source":"lora_model.save_pretrained(\"paws_lora_adapter\")\n\nclassifier_state_dict = {\n    \"classifier.weight\": lora_model.base_model.model.classifier.weight.cpu(),\n    \"classifier.bias\": lora_model.base_model.model.classifier.bias.cpu()\n}\n\ntorch.save(classifier_state_dict, \"paws_classifier_head.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:20.463661Z","iopub.execute_input":"2025-03-23T20:47:20.463916Z","iopub.status.idle":"2025-03-23T20:47:20.482471Z","shell.execute_reply.started":"2025-03-23T20:47:20.463893Z","shell.execute_reply":"2025-03-23T20:47:20.481617Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"# from peft import PeftModel\n\n# base_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n# lora_model = PeftModel.from_pretrained(base_model, \"pwas_lora_adapter\")\n\n# classifier_state_dict = torch.load(\"paws_classifier_head.pth\", map_location=device, weights_only=True)\n\n\n# lora_model.base_model.classifier.weight.data.copy_(classifier_state_dict[\"classifier.weight\"])\n# lora_model.base_model.classifier.bias.data.copy_(classifier_state_dict[\"classifier.bias\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T20:47:20.483417Z","iopub.execute_input":"2025-03-23T20:47:20.483711Z","iopub.status.idle":"2025-03-23T20:47:20.497245Z","shell.execute_reply.started":"2025-03-23T20:47:20.483689Z","shell.execute_reply":"2025-03-23T20:47:20.496481Z"}},"outputs":[],"execution_count":72}]}