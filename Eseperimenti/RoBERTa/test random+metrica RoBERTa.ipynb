{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66c48088-db16-42e5-ba79-29c5fea0cd38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f4868-d0ac-48eb-9de8-fd058cc5280a",
   "metadata": {},
   "source": [
    "# Fine-Tuning con LoRA basato sulla similarità tra task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7b109-e37b-4c77-8e88-88efbaf675f8",
   "metadata": {},
   "source": [
    "#### Configurazioni generali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a45cf5-7d41-4da4-812a-1254b4214a73",
   "metadata": {},
   "source": [
    "Installazione delle librerie necessarie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d2011b-e224-40ed-8832-f6e6e4e407e2",
   "metadata": {},
   "source": [
    "Importo i moduli necessari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94de51e-539a-4a79-8809-8e67787e37c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588ab405-1c31-4d6b-84ef-b84319beed8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/notebook/venvs/tesisti_RC/lib/python3.12/site-packages/tllib-0.4-py3.12.egg/tllib/alignment/cdan.py:134: SyntaxWarning: invalid escape sequence '\\o'\n",
      "/home/notebook/venvs/tesisti_RC/lib/python3.12/site-packages/tllib-0.4-py3.12.egg/tllib/alignment/cdan.py:134: SyntaxWarning: invalid escape sequence '\\o'\n",
      "/home/notebook/venvs/tesisti_RC/lib/python3.12/site-packages/tllib-0.4-py3.12.egg/tllib/modules/grl.py:34: SyntaxWarning: invalid escape sequence '\\m'\n",
      "/home/notebook/venvs/tesisti_RC/lib/python3.12/site-packages/tllib-0.4-py3.12.egg/tllib/modules/grl.py:34: SyntaxWarning: invalid escape sequence '\\m'\n",
      "/home/notebook/venvs/tesisti_RC/lib/python3.12/site-packages/tllib-0.4-py3.12.egg/tllib/alignment/mdd.py:266: SyntaxWarning: invalid escape sequence '\\l'\n",
      "/home/notebook/venvs/tesisti_RC/lib/python3.12/site-packages/tllib-0.4-py3.12.egg/tllib/alignment/mdd.py:266: SyntaxWarning: invalid escape sequence '\\l'\n"
     ]
    }
   ],
   "source": [
    "import tllib\n",
    "import codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c0e6c1-890a-4f1d-8913-47ec1eb7e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010e263-e97a-4bcc-afd0-1c6be65e57c6",
   "metadata": {},
   "source": [
    "Impostazione del seme casuale per la riproducibilità."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a43b9c-5dba-489e-97a2-7c3dc6eadc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# Imposto il seme casuale anche per i calcoli CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcac625-a53d-4be7-b222-a6bd42076892",
   "metadata": {},
   "source": [
    "## Creazione del pool di modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60760146-1141-41ac-9780-717d7805b78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 15:50:39.730584: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-18 15:50:39.745268: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744991439.763309  545895 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744991439.768803  545895 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744991439.782443  545895 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744991439.782457  545895 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744991439.782458  545895 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744991439.782460  545895 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-18 15:50:39.787169: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e404d3-ffee-46a6-b956-286a95ab73d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartella in cui sono memorizzati i pesi degli adapters LoRA\n",
    "ADAPTER_DIR = \"./adapters-pool/original/\"\n",
    "\n",
    "# Dizionario di configurazione per ogni task\n",
    "TASK_CONFIGS = {\n",
    "    \"ag\":        {\"num_classes\": 4},\n",
    "    \"sst\":       {\"num_classes\": 2},\n",
    "    \"mnli\":      {\"num_classes\": 3},\n",
    "    \"ei\":        {\"num_classes\": 4},\n",
    "    \"paws\":      {\"num_classes\": 2},\n",
    "}\n",
    "\n",
    "\n",
    "# Funzione per creare il pool di modelli\n",
    "def create_model_pool(adapter_dir=ADAPTER_DIR, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \n",
    "    model_pool = {}\n",
    "\n",
    "    for folder in os.listdir(adapter_dir):\n",
    "        adapter_path = os.path.join(adapter_dir, folder)\n",
    "        task_name = folder.replace(\"_lora_adapter\", \"\")\n",
    "        \n",
    "        if task_name not in TASK_CONFIGS:\n",
    "            print(f\"Task '{task_name}' non trovato in TASK_CONFIGS, verrà ignorato...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Caricamento modello per il task: {task_name}...\")\n",
    "\n",
    "        # Ottengo il numero di classi per il task\n",
    "        num_classes = TASK_CONFIGS[task_name][\"num_classes\"]   \n",
    "        \n",
    "        # Creo il modello base\n",
    "        base_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_classes)\n",
    "\n",
    "        # Carico i pesi LoRA\n",
    "        lora_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        \n",
    "        # Carico i pesi della testa di classificazione \n",
    "        classifier_head_path = os.path.join(adapter_path, \"classifier_head.pth\")\n",
    "        classifier_state_dict = torch.load(classifier_head_path, map_location=device, weights_only=True)\n",
    "\n",
    "        print(\"Chiavi del classifier_head:\", classifier_state_dict.keys())\n",
    "\n",
    "        classifier_module = lora_model.base_model.classifier\n",
    "\n",
    "        classifier_module.dense.weight.data.copy_(classifier_state_dict[\"dense.weight\"])\n",
    "        classifier_module.dense.bias.data.copy_(classifier_state_dict[\"dense.bias\"])\n",
    "        classifier_module.out_proj.weight.data.copy_(classifier_state_dict[\"out_proj.weight\"])\n",
    "        classifier_module.out_proj.bias.data.copy_(classifier_state_dict[\"out_proj.bias\"])\n",
    "\n",
    "\n",
    "        lora_model.to(device)\n",
    "        \n",
    "        model_pool[task_name] = lora_model\n",
    "        print(f\"Modello '{task_name}' caricato con successo.\\n\")\n",
    "\n",
    "    return model_pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d80a10b-07ab-4d3d-9c9f-dca35717e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento modello per il task: ag...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiavi del classifier_head: odict_keys(['dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias'])\n",
      "Modello 'ag' caricato con successo.\n",
      "\n",
      "Task '.ipynb_checkpoints' non trovato in TASK_CONFIGS, verrà ignorato...\n",
      "Caricamento modello per il task: sst...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiavi del classifier_head: odict_keys(['dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias'])\n",
      "Modello 'sst' caricato con successo.\n",
      "\n",
      "Caricamento modello per il task: ei...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiavi del classifier_head: odict_keys(['dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias'])\n",
      "Modello 'ei' caricato con successo.\n",
      "\n",
      "Caricamento modello per il task: mnli...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiavi del classifier_head: odict_keys(['dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias'])\n",
      "Modello 'mnli' caricato con successo.\n",
      "\n",
      "Caricamento modello per il task: paws...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiavi del classifier_head: odict_keys(['dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias'])\n",
      "Modello 'paws' caricato con successo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creazione del pool di modelli \n",
    "model_pool = create_model_pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca1874c4-a2b4-4396-b581-1c9087a8db7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cartella ./adapters-pool/working/ è stata ripulita.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "working_pool_path = \"./adapters-pool/working/\"\n",
    "shutil.rmtree(working_pool_path, ignore_errors=True)\n",
    "os.makedirs(working_pool_path, exist_ok=True)\n",
    "\n",
    "print(\"La cartella ./adapters-pool/working/ è stata ripulita.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06d52f41-ca28-4854-9e9d-13614342e1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copiato: ag_lora_adapter\n",
      "Copiato: .ipynb_checkpoints\n",
      "Copiato: sst_lora_adapter\n",
      "Copiato: ei_lora_adapter\n",
      "Copiato: mnli_lora_adapter\n",
      "Copiato: paws_lora_adapter\n",
      "Tutti i modelli sono stati copiati in ./adapters-pool/working/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "original_pool_path = \"./adapters-pool/original/\"\n",
    "working_pool_path = \"./adapters-pool/working/\"\n",
    "\n",
    "os.makedirs(working_pool_path, exist_ok=True)\n",
    "\n",
    "for model_name in os.listdir(original_pool_path):\n",
    "    src_path = os.path.join(original_pool_path, model_name)\n",
    "    dest_path = os.path.join(working_pool_path, model_name)\n",
    "    \n",
    "    if not os.path.exists(dest_path):\n",
    "        shutil.copytree(src_path, dest_path)\n",
    "        print(f\"Copiato: {model_name}\")\n",
    "\n",
    "print(\"Tutti i modelli sono stati copiati in ./adapters-pool/working/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e4f68-7d6f-4568-8f20-9b41d4a29751",
   "metadata": {},
   "source": [
    "## Definizione delle funzioni per il calcolo della similarità"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175bc103-43ad-477e-8e2d-59c462882478",
   "metadata": {},
   "source": [
    "#### LEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d9b57ea-2f39-4f3a-befc-a58ab528583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(model, data_loader):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Salvo le etichette vere\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Ottiengo i logits dal modello\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask, \n",
    "                            token_type_ids=token_type_ids)\n",
    "            \n",
    "            probabilities = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "            all_predictions.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    return np.array(all_predictions), np.array(all_labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49628ce0-4d49-438e-9bd7-4a4f0ea2727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tllib.ranking import log_expected_empirical_prediction as leep\n",
    "\n",
    "def calculate_leep_scores(pool_models, target_loader):\n",
    "    leep_scores = []\n",
    "    for source_name, source_model in pool_models.items():\n",
    "        \n",
    "        predictions, labels = get_model_predictions(source_model, target_loader)\n",
    "        score = leep(predictions, labels)\n",
    "        leep_scores.append({\"Source\": source_name, \"Score\": score})\n",
    "        \n",
    "        print(f\"Source: {source_name}, LEEP Score: {score:.4f}\")\n",
    "    return leep_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb73ccb-e879-48cc-bfdd-8da095f75598",
   "metadata": {},
   "source": [
    "#### LogME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1860d518-38ff-44e0-a868-b385586b490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, data_loader, device):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            label = batch['labels'].cpu().numpy()\n",
    "\n",
    "            # Ottiengo l'output del modello\n",
    "            outputs = model.roberta(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            # Estraggo la rappresentazione del token [CLS]\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(cls_embeddings)\n",
    "            labels.append(label)\n",
    "    \n",
    "    embeddings = np.vstack(embeddings)\n",
    "    labels = np.concatenate(labels)\n",
    "    labels = labels.astype(int)\n",
    "    \n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aae3154-891d-48bf-b941-2a7031eb398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tllib.ranking import log_maximum_evidence as logme\n",
    "\n",
    "def calculate_logme_scores(pool_models, target_loader, device):\n",
    "    logme_scores = []\n",
    "    for source_name, source_model in pool_models.items():\n",
    "    \n",
    "        embeddings, labels = extract_embeddings(source_model, target_loader, device)\n",
    "        score = logme(embeddings, labels)\n",
    "        logme_scores.append({\"Source\": source_name, \"Score\": score})\n",
    "        \n",
    "        print(f\"Source: {source_name}, LogME Score: {score:.4f}\")\n",
    "    return logme_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a254e621-8747-445a-9e33-59ec75535f1f",
   "metadata": {},
   "source": [
    "#### H-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b17e083-5d3f-4b77-b1a3-7d135438a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "# Funzione della libreria tllib\n",
    "def regularized_h_score(features: np.ndarray, labels: np.ndarray):\n",
    "    r\"\"\"\n",
    "    Regularized H-score in `Newer is not always better: Rethinking transferability metrics, their peculiarities, stability and performance (NeurIPS 2021) \n",
    "    <https://openreview.net/pdf?id=iz_Wwmfquno>`_.\n",
    "    \n",
    "    The  regularized H-Score :math:`\\mathcal{H}_{\\alpha}` can be described as:\n",
    "\n",
    "    .. math::\n",
    "        \\mathcal{H}_{\\alpha}=\\operatorname{tr}\\left(\\operatorname{cov}_{\\alpha}(f)^{-1}\\left(1-\\alpha \\right)\\operatorname{cov}\\left(\\mathbb{E}[f \\mid y]\\right)\\right)\n",
    "    \n",
    "    where :math:`f` is the features extracted by the model to be ranked, :math:`y` is the groud-truth label vector and :math:`\\operatorname{cov}_{\\alpha}` the  Ledoit-Wolf \n",
    "    covariance estimator with shrinkage parameter :math:`\\alpha`\n",
    "    Args:\n",
    "        features (np.ndarray):features extracted by pre-trained model.\n",
    "        labels (np.ndarray):  groud-truth labels.\n",
    "\n",
    "    Shape:\n",
    "        - features: (N, F), with number of samples N and feature dimension F.\n",
    "        - labels: (N, ) elements in [0, :math:`C_t`), with target class number :math:`C_t`.\n",
    "        - score: scalar.\n",
    "    \"\"\"\n",
    "    f = features.astype('float64')\n",
    "    f = f - np.mean(f, axis=0, keepdims=True)  # Center the features for correct Ledoit-Wolf Estimation\n",
    "    y = labels\n",
    "\n",
    "    C = int(y.max() + 1)\n",
    "    g = np.zeros_like(f)\n",
    "\n",
    "    cov = LedoitWolf(assume_centered=False).fit(f)\n",
    "    alpha = cov.shrinkage_\n",
    "    covf_alpha = cov.covariance_\n",
    "\n",
    "    for i in range(C):\n",
    "        Ef_i = np.mean(f[y == i, :], axis=0)\n",
    "        g[y == i] = Ef_i\n",
    "\n",
    "    covg = np.cov(g, rowvar=False)\n",
    "    score = np.trace(np.dot(np.linalg.pinv(covf_alpha, rcond=1e-15), (1 - alpha) * covg))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "595ed4e6-7552-4299-a2b7-3140ee87c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_h_scores(pool_models, target_loader, device):\n",
    "    h_scores = []\n",
    "    for source_name, source_model in pool_models.items():\n",
    "        \n",
    "        embeddings, labels = extract_embeddings(source_model, target_loader, device)\n",
    "        score = regularized_h_score(embeddings, labels)\n",
    "        h_scores.append({\"Source\": source_name, \"Score\": score})\n",
    "        \n",
    "        print(f\"Source: {source_name}, H-Score: {score:.4f}\")\n",
    "    \n",
    "    return h_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0804177e-228e-472b-8e4c-4140082cd618",
   "metadata": {},
   "source": [
    "#### NCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff2a5937-4fce-4e95-aff8-229ab0cdacc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_labels(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            # Ottieni i logits dal modello\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            \n",
    "            predictions = torch.argmax(torch.softmax(logits.logits, dim=1), dim=1).cpu().numpy()\n",
    "            all_predictions.extend(predictions.astype(int))\n",
    "    \n",
    "    return np.array(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9cf2e03-eb5f-4c1f-a3c7-23108a4fa970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Funzione della libreria tllib con aggiunta di smoothing e clipping\n",
    "# def negative_conditional_entropy(source_labels: np.ndarray, target_labels: np.ndarray, alpha=1e-6, clip_min=1e-20):\n",
    "#     \"\"\"\n",
    "#     Negative Conditional Entropy with smoothing to handle zero probabilities.\n",
    "\n",
    "#     Args:\n",
    "#         source_labels (np.ndarray): Predicted source labels.\n",
    "#         target_labels (np.ndarray): Ground-truth target labels.\n",
    "#         alpha (float): Smoothing factor for joint probability estimation.\n",
    "#         clip_min (float): Minimum value for probabilities to prevent log(0).\n",
    "\n",
    "#     Returns:\n",
    "#         float: Negative conditional entropy score.\n",
    "#     \"\"\"\n",
    "#     C_t = int(np.max(target_labels) + 1)\n",
    "#     C_s = int(np.max(source_labels) + 1)\n",
    "#     N = len(source_labels)\n",
    "\n",
    "#     joint = np.zeros((C_t, C_s), dtype=float)\n",
    "#     for s, t in zip(source_labels, target_labels):\n",
    "#         joint[t, s] += 1.0\n",
    "\n",
    "#     joint = (joint + alpha) / (N + alpha * C_s * C_t)\n",
    "\n",
    "#     p_z = joint.sum(axis=0, keepdims=True)\n",
    "\n",
    "#     p_target_given_source = (joint / p_z).T \n",
    "#     p_target_given_source = np.clip(p_target_given_source, a_min=clip_min, a_max=None)  # Evita log(0)\n",
    "\n",
    "#     entropy_y_given_z = np.sum(- p_target_given_source * np.log(p_target_given_source), axis=1, keepdims=True)\n",
    "    \n",
    "#     mask = p_z.reshape(-1) != 0\n",
    "#     conditional_entropy = np.sum(entropy_y_given_z * p_z.reshape((-1, 1))[mask])\n",
    "\n",
    "#     return -conditional_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3945455b-729c-425d-a1f6-3b55df15792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_conditional_entropy(source_labels: np.ndarray, target_labels: np.ndarray, alpha=1e-30, clip_min=1e-50):\n",
    "    C_t = int(np.max(target_labels) + 1)\n",
    "    C_s = int(np.max(source_labels) + 1)\n",
    "    N = len(source_labels)\n",
    "\n",
    "    # Compute joint probability with smoothing\n",
    "    joint = np.zeros((C_t, C_s), dtype=float)\n",
    "    for s, t in zip(source_labels, target_labels):\n",
    "        joint[t, s] += 1.0\n",
    "\n",
    "    joint = (joint + alpha) / (N + alpha * C_s * C_t)\n",
    "\n",
    "    # Compute marginal P(z)\n",
    "    p_z = joint.sum(axis=0, keepdims=True)\n",
    "\n",
    "    # Compute conditional P(y|z) and apply clipping\n",
    "    p_target_given_source = (joint / (p_z + clip_min)).T  \n",
    "    p_target_given_source = np.clip(p_target_given_source, a_min=clip_min, a_max=None)\n",
    "\n",
    "    # Compute entropy\n",
    "    entropy_y_given_z = np.sum(- p_target_given_source * np.log(p_target_given_source), axis=1, keepdims=True)\n",
    "    conditional_entropy = np.sum(entropy_y_given_z * p_z.T)\n",
    "\n",
    "    return -conditional_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ccaec57-acd6-4619-806f-5f0a6e4887fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nce_scores(pool_models, target_loader, device):\n",
    "    nce_scores = []\n",
    "    \n",
    "    for source_name, source_model in pool_models.items():\n",
    "        \n",
    "        source_labels = get_source_labels(source_model, target_loader, device)\n",
    "        target_labels = np.array(target_loader.dataset.labels)    \n",
    "        \n",
    "        score = negative_conditional_entropy(source_labels, target_labels)\n",
    "        nce_scores.append({\"Source\": source_name, \"Score\": score})\n",
    "        \n",
    "        print(f\"Source: {source_name}, NCE Score: {score}\")\n",
    "    \n",
    "    return nce_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a368df-3b90-46d8-8a0c-a9e75b3bf455",
   "metadata": {},
   "source": [
    "### NLEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53c17efb-66a8-414d-b362-1e0f9336aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import torch\n",
    "\n",
    "def NLEEP(probabilities, y, component_ratio=5):\n",
    "    n = len(y)\n",
    "    num_classes = len(np.unique(y))\n",
    "    num_source_classes = probabilities.shape[1] \n",
    "\n",
    "    pca_80 = PCA(n_components=0.8)\n",
    "    X_pca_80 = pca_80.fit_transform(probabilities) \n",
    "\n",
    "    n_components_num = component_ratio * num_classes\n",
    "    gmm = GaussianMixture(n_components=n_components_num, random_state=42).fit(X_pca_80)\n",
    "    prob = gmm.predict_proba(X_pca_80) \n",
    "\n",
    "    pyz = np.zeros((num_classes, n_components_num))\n",
    "    for y_ in range(num_classes):\n",
    "        indices = np.where(y == y_)[0]\n",
    "        if len(indices) > 0:\n",
    "            filter_ = np.take(prob, indices, axis=0)\n",
    "            pyz[y_] = np.sum(filter_, axis=0) / n \n",
    "\n",
    "    pz = np.sum(pyz, axis=0) + 1e-10  \n",
    "    py_z = pyz / pz\n",
    "    py_x = np.dot(prob, py_z.T)\n",
    "\n",
    "    nleep_score = np.sum(py_x[np.arange(n), y]) / n\n",
    "    return nleep_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71512ead-0d8e-4096-9667-dd92ac9a03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nleep_scores(pool_models, target_loader):\n",
    "    leep_scores = []\n",
    "    for source_name, source_model in pool_models.items():\n",
    "        \n",
    "        predictions, labels = get_model_predictions(source_model, target_loader)\n",
    "        score =  NLEEP(predictions, labels)\n",
    "        leep_scores.append({\"Source\": source_name, \"Score\": score})\n",
    "        \n",
    "        print(f\"Source: {source_name}, NLEEP Score: {score:.4f}\")\n",
    "    return leep_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bef4dd-7e06-4f2e-9e35-332131191e50",
   "metadata": {},
   "source": [
    "## Implementazione del metodo principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43611020-c999-4fd4-bae9-383051924919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "    \n",
    "def train_target_task(\n",
    "    target_name,         \n",
    "    target_loader, \n",
    "    target_num_classes,\n",
    "    model_pool,     \n",
    "    training_data, \n",
    "    validation_data,\n",
    "    test_data,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    learning_rate=5e-4, \n",
    "    epochs=4, \n",
    "    patience=1,\n",
    "    batch_size=32, \n",
    "    optimizer_class=torch.optim.AdamW,\n",
    "    similarity_metric=\"leep\",\n",
    "    min_accuracy=0.70\n",
    "):\n",
    "    \n",
    "    print(f\"Selezione del miglior modello per il task {target_name} usando {similarity_metric}...\\n\")\n",
    "\n",
    "    \n",
    "    os.makedirs(\"carbon_emissions\", exist_ok=True)\n",
    "    tracker = EmissionsTracker( project_name=f\"{target_name}_training\", output_dir=\"carbon_emissions\", output_file=\"selection_emissions.csv\", gpu_ids=[6]) \n",
    "    tracker.start()  \n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ------\n",
    "    random_generator = random.Random(time.time())\n",
    "    \n",
    "    selected_models = random_generator.sample(list(model_pool.keys()), 5)\n",
    "    print(f\"Modelli selezionati per il confronto: {selected_models}\\n\")\n",
    "    \n",
    "    model_pool_temp = {nome: model_pool[nome] for nome in selected_models}\n",
    "    # ------\n",
    "    \n",
    "   # Calcolo della metrica di similarità scelta\n",
    "    if similarity_metric == \"leep\":\n",
    "        similarity_scores = calculate_leep_scores(model_pool_temp, target_loader) # cambia model_pool_temp in model_pool \n",
    "    elif similarity_metric == \"nleep\":\n",
    "        similarity_scores = calculate_nleep_scores(model_pool_temp, target_loader)# cambia model_pool_temp in model_pool \n",
    "    elif similarity_metric == \"logme\":\n",
    "        similarity_scores = calculate_logme_scores(model_pool_temp,target_loader, device)# cambia model_pool_temp in model_pool \n",
    "    elif similarity_metric == \"h-score\":\n",
    "        similarity_scores = calculate_h_scores(model_pool_temp, target_loader, device)# cambia model_pool_temp in model_pool \n",
    "    elif similarity_metric == \"nce\":\n",
    "        similarity_scores = calculate_nce_scores(model_pool_temp, target_loader, device)# cambia model_pool_temp in model_pool \n",
    "    else:\n",
    "        raise ValueError(\"Errore. Il parametro similarity_metric deve essere: 'leep','nleep', 'logme', 'h-score' oppure 'nce'.\")\n",
    "    \n",
    "    # Seleziono il miglior modello in base alla metrica scelta\n",
    "    best_model_entry = max(similarity_scores, key=lambda x: x[\"Score\"])\n",
    "    best_model_name = best_model_entry[\"Source\"]\n",
    "    best_model = model_pool_temp[best_model_name] # cambia model_pool_temp in model_pool \n",
    "    best_score = best_model_entry[\"Score\"]\n",
    "\n",
    "    # Test random\n",
    "    # best_model_name = random.choice(list(model_pool.keys()))\n",
    "    # best_model = model_pool[best_model_name]\n",
    "    # best_score = 0\n",
    "    \n",
    "    selection_time = time.time() - start_time\n",
    "\n",
    "    selection_emissions = tracker.stop()\n",
    "    print(f\"\\nEmissioni selezione : {selection_emissions:.4f} kg\") \n",
    "    \n",
    "    print(f\"Modello selezionato: {best_model_name} ({similarity_metric}: {best_score:.4f})\\n\")\n",
    "\n",
    "    # Creo un nuovo modello per il task target\n",
    "    base_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=target_num_classes)\n",
    "    \n",
    "    # Carico gli adapter LoRA del miglior modello selezionato\n",
    "    new_model = PeftModel.from_pretrained(base_model, f\"./adapters-pool/working/{best_model_name}_lora_adapter\")\n",
    "    new_model.to(device)\n",
    "    print(f\"Pesi LoRA del modello {best_model_name} caricati per il task {target_name}.\\n\")\n",
    "    \n",
    "    for name, param in new_model.named_parameters():\n",
    "        if \"lora\" in name:   # if \"lora\" or \"classifier\" in name:  \n",
    "            param.requires_grad = True\n",
    "            \n",
    "    \n",
    "    # Imposto i parametri principali per il training \n",
    "    # Creo i DataLoader\n",
    "    train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    total_steps = len(train_loader) * epochs\n",
    "\n",
    "    # Ottimizzatore (solo parametri addestrabili)\n",
    "    optimizer = optimizer_class(filter(lambda p: p.requires_grad, new_model.parameters()), lr=learning_rate)\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer,\n",
    "                                                       num_warmup_steps = 0,\n",
    "                                                       num_training_steps = total_steps)\n",
    "    \n",
    "    # Avvio il fine-tuning\n",
    "    print(f\"Inizio addestramento sul task {target_name}...\\n\")\n",
    "    history, total_time, emission = train_and_evaluate_model(\n",
    "        new_model, target_name, train_loader, val_loader,\n",
    "        optimizer, scheduler, device, epochs, patience)\n",
    "\n",
    "    print(f\"Addestramento completato.\\n\")\n",
    "    \n",
    "    # Valutazione sul test set\n",
    "    new_model.load_state_dict(torch.load(f\"{target_name}_best_model_state.bin\"))\n",
    "    \n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_loss, test_acc, test_f1 = eval_model(\n",
    "        new_model, test_loader, device\n",
    "    )\n",
    "    print(f\"Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Salvo il nuovo modello se soddisfa il requisito di accuracy minima\n",
    "    if test_acc >= min_accuracy:\n",
    "        \n",
    "        # Salvo il modello\n",
    "        updated_pool_path = f\"./adapters-pool/working/{target_name}_lora_adapter\"\n",
    "        os.makedirs(updated_pool_path, exist_ok=True)\n",
    "        new_model.save_pretrained(updated_pool_path)\n",
    "        print(f\"Modello {target_name} salvato in {updated_pool_path}.\\n\")\n",
    "\n",
    "        # Aggiungo il nuovo modello al pool\n",
    "        model_pool[target_name] = new_model\n",
    "        print(f\"Modello {target_name} aggiunto al pool. Ora il pool contiene {len(model_pool)} modelli.\\n\")\n",
    "    else:\n",
    "        print(f\"Il modello non ha raggiunto il requisito di accuracy minima, quindi non è stato aggiunto nel pool.\\n Accuracy modello: {test_acc:.4f}, Accuracy minima: {min_accuracy:.4f}\")\n",
    "        \n",
    "        \n",
    "    return {\n",
    "        \"model\": new_model,\n",
    "        \"selected_model\": best_model_name,\n",
    "        \"similarity_score\": best_score,\n",
    "        \"selection_time\": selection_time,\n",
    "        \"selection_emission\": selection_emissions,\n",
    "        \"training_time\": total_time,\n",
    "        \"emission\": emission,\n",
    "        \"history\": history,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_f1\":test_f1,\n",
    "        \"selected_models\": selected_models \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f03885fe-3af7-4544-b1bd-4ce9c26055c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch.nn as nn\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "os.environ[\"CODECARBON_LOG_LEVEL\"] = \"WARNING\"\n",
    "\n",
    "\n",
    "# Funzione di training e valutazione\n",
    "def train_and_evaluate_model(model, dataset, train_loader, val_loader, optimizer, scheduler, device, epochs, patience):\n",
    "\n",
    "    os.makedirs(\"carbon_emissions\", exist_ok=True)\n",
    "    tracker = EmissionsTracker( project_name=f\"{dataset} training\", output_dir=\"carbon_emissions\", output_file=\"emissions.csv\", gpu_ids=[6]) \n",
    "    tracker.start()  \n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    best_accuracy = 0\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0  \n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # Training\n",
    "        train_loss, train_acc = train_model(model, train_loader, optimizer, scheduler, device)\n",
    "        \n",
    "        # Valutazione\n",
    "        val_loss, val_acc, val_f1 = eval_model(model, val_loader, device)\n",
    "        \n",
    "        # Salvataggio del modello migliore\n",
    "        if val_acc > best_accuracy:\n",
    "            print(f\"Nuovo miglior modello salvato all'epoca {epoch + 1} (Accuracy: {val_acc:.4f})\")\n",
    "            torch.save(model.state_dict(),  f\"{dataset}_best_model_state.bin\")\n",
    "            best_accuracy = val_acc\n",
    "\n",
    "        # Salvataggio delle metriche\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0 \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"La loss sul validation set non è migliorata per {patience_counter} epoche.\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping attivato dopo {patience_counter} epoche senza miglioramenti\")\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_training_time = end_time - start_time\n",
    "\n",
    "    emissions = tracker.stop()\n",
    "    print(f\"\\nEmissioni CO₂ totali: {emissions:.4f} kg\")  \n",
    "\n",
    "    return history, total_training_time, emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87124341-4293-4bc4-9cd8-c96001e9bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di training\n",
    "def train_model(model, data_loader, optimizer, scheduler, device):\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    loop = tqdm(data_loader, desc=f\"Training  \", leave=True)\n",
    "\n",
    "    for batch in loop:\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # --- Forward pass ---\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels \n",
    "        )\n",
    "\n",
    "        loss = outputs.loss  \n",
    "        logits = outputs.logits  \n",
    "\n",
    "        # --- Backward pass ---\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)  # Predizioni multiclasse\n",
    "\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "        loop.set_postfix(loss=total_loss / (loop.n + 1), accuracy=accuracy_score(all_labels, all_preds))\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2f31192-f8ef-408b-b5d8-cd3c62b1d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di valutazione\n",
    "def eval_model(model, data_loader, device):\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        loop = tqdm(data_loader, desc=f\"Evaluating\", leave=True)\n",
    "        for batch in loop:\n",
    "            \n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.detach().cpu().numpy())\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "            loop.set_postfix(Val_loss=total_loss / (loop.n + 1), Val_accuracy=accuracy_score(all_labels, all_preds))\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    \n",
    "    return avg_loss, accuracy, f1  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d466e48-1499-4649-a340-4f7925171114",
   "metadata": {},
   "source": [
    "## Caricamento dei nuovi task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe38e8cf-2077-48ed-af25-aab151f1297f",
   "metadata": {},
   "source": [
    "### Sentiment 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "808187e9-8730-47c8-a4f3-5fce7c20953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'date', 'user', 'sentiment', 'query'],\n",
      "        num_rows: 1600000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'date', 'user', 'sentiment', 'query'],\n",
      "        num_rows: 498\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Ottenimento del dataset\n",
    "sent140_dataset = load_dataset(\"stanfordnlp/sentiment140\",trust_remote_code=True)\n",
    "print(sent140_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ef7124d-17ec-426e-9a04-ccf5043fef48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensioni dei set:\n",
      "Train: 20000\n",
      "Validation: 1000\n",
      "Test: 1024\n",
      "\n",
      "Distribuzione delle etichette:\n",
      "Train: Counter({0: 10000, 1: 10000})\n",
      "Validation: Counter({1: 500, 0: 500})\n",
      "Test: Counter({1: 512, 0: 512})\n"
     ]
    }
   ],
   "source": [
    "# Divido i dati in training, validation e test set\n",
    "sent140_data = sent140_dataset[\"train\"].shuffle(seed=42)\n",
    "\n",
    "sent140_temp_sentences, sent140_test_sentences, sent140_temp_labels, sent140_test_labels = train_test_split(\n",
    "                                                sent140_data['text'], \n",
    "                                                sent140_data['sentiment'], \n",
    "                                                test_size=1024, \n",
    "                                                random_state=42,\n",
    "                                                stratify=sent140_data['sentiment'])\n",
    "\n",
    "sent140_train_sentences, sent140_val_sentences, sent140_train_labels, sent140_val_labels = train_test_split(\n",
    "                                                sent140_temp_sentences, \n",
    "                                                sent140_temp_labels, \n",
    "                                                train_size=20000,\n",
    "                                                test_size=1000,\n",
    "                                                random_state=42,\n",
    "                                                stratify=sent140_temp_labels)\n",
    "\n",
    "# Trasformazione delle etichette 0 -> 0 e 4->1\n",
    "sent140_train_labels = [1 if label == 4 else 0 for label in sent140_train_labels]\n",
    "sent140_val_labels = [1 if label == 4 else 0 for label in sent140_val_labels]\n",
    "sent140_test_labels = [1 if label == 4 else 0 for label in sent140_test_labels]\n",
    "\n",
    "print(\"Dimensioni dei set:\")\n",
    "print(f\"Train: {len(sent140_train_sentences)}\")\n",
    "print(f\"Validation: {len(sent140_val_sentences)}\")\n",
    "print(f\"Test: {len(sent140_test_sentences)}\")\n",
    "\n",
    "# Verifica distribuzione delle etichette\n",
    "print(\"\\nDistribuzione delle etichette:\")\n",
    "print(f\"Train: {Counter(sent140_train_labels)}\")\n",
    "print(f\"Validation: {Counter(sent140_val_labels)}\")\n",
    "print(f\"Test: {Counter(sent140_test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88c4ec42-3dbf-4141-8070-350f9aa88006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences, labels, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        sentence = self.sentences[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding[\"token_type_ids\"].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89cec309-dcfb-422d-869e-94d67fcd5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Inizializzo il Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Ottiengo i dataset\n",
    "sent140_training_data = ClassificationDataset(\n",
    "                           sentences = sent140_train_sentences,\n",
    "                           labels = sent140_train_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "sent140_validation_data = ClassificationDataset(\n",
    "                           sentences = sent140_val_sentences,\n",
    "                           labels = sent140_val_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "sent140_test_data = ClassificationDataset(\n",
    "                           sentences = sent140_test_sentences,\n",
    "                           labels = sent140_test_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# Creo i DataLoader\n",
    "sent140_train_loader = DataLoader(sent140_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "sent140_val_loader = DataLoader(sent140_validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "sent140_test_loader = DataLoader(sent140_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcc2048-bb56-41c9-92a5-d38bf78dd546",
   "metadata": {},
   "source": [
    "### IMDB Rewies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7efa0ca-8274-4768-8ad8-e6565455d603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Ottenimento del dataset\n",
    "imdb_dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0a7b2fa-b9d1-4152-95b9-0353a0d33913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensioni dei set:\n",
      "Train: 40000\n",
      "Validation: 5000\n",
      "Test: 5000\n",
      "\n",
      "Distribuzione delle etichette:\n",
      "Train: Counter({0: 20000, 1: 20000})\n",
      "Validation: Counter({0: 2500, 1: 2500})\n",
      "Test: Counter({1: 2500, 0: 2500})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "imdb_train_data = pd.DataFrame(imdb_dataset[\"train\"])\n",
    "imdb_test_data = pd.DataFrame(imdb_dataset[\"test\"])\n",
    "\n",
    "imdb_data = Dataset.from_pandas(pd.concat([imdb_train_data, imdb_test_data], ignore_index=True))\n",
    "imdb_data = imdb_data.shuffle(seed=42)\n",
    "\n",
    "imdb_temp_sentences, imdb_test_sentences, imdb_temp_labels, imdb_test_labels = train_test_split(\n",
    "                                                imdb_data['text'],\n",
    "                                                imdb_data['label'], \n",
    "                                                test_size=0.1, \n",
    "                                                random_state=42,\n",
    "                                                stratify=imdb_data['label'])\n",
    "\n",
    "imdb_train_sentences, imdb_val_sentences, imdb_train_labels, imdb_val_labels = train_test_split(\n",
    "                                                imdb_temp_sentences,\n",
    "                                                imdb_temp_labels,\n",
    "                                                test_size=0.1111,\n",
    "                                                random_state=42,\n",
    "                                                stratify=imdb_temp_labels)\n",
    "\n",
    "print(\"Dimensioni dei set:\")\n",
    "print(f\"Train: {len(imdb_train_sentences)}\")\n",
    "print(f\"Validation: {len(imdb_val_sentences)}\")\n",
    "print(f\"Test: {len(imdb_test_sentences)}\")\n",
    "\n",
    "# Verifica distribuzione delle etichette\n",
    "print(\"\\nDistribuzione delle etichette:\")\n",
    "print(f\"Train: {Counter(imdb_train_labels)}\")\n",
    "print(f\"Validation: {Counter(imdb_val_labels)}\")\n",
    "print(f\"Test: {Counter(imdb_test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cea8359-fc13-49c1-a597-56164fca6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Inizializzo il Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Ottiengo i dataset\n",
    "imdb_training_data = ClassificationDataset(sentences = imdb_train_sentences,\n",
    "                           labels = imdb_train_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "imdb_validation_data = ClassificationDataset(sentences = imdb_val_sentences,\n",
    "                           labels = imdb_val_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "imdb_test_data = ClassificationDataset(sentences = imdb_test_sentences,\n",
    "                           labels = imdb_test_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# Creo i DataLoader\n",
    "imdb_train_loader = DataLoader(imdb_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "imdb_val_loader = DataLoader(imdb_validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "imdb_test_loader = DataLoader(imdb_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e270aa-7d30-4718-816a-c44a0e761c1e",
   "metadata": {},
   "source": [
    "### 20 News Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d817224-5089-475f-affe-924fb5b25e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 11314\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'label_text'],\n",
      "        num_rows: 7532\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Ottenimento del dataset\n",
    "news_dataset = load_dataset(\"SetFit/20_newsgroups\")\n",
    "print(news_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c19775f-5370-4ae4-be8f-2604191f6a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensioni dei set:\n",
      "Train: 15076\n",
      "Validation: 1885\n",
      "Test: 1885\n",
      "\n",
      "Distribuzione delle etichette:\n",
      "Train: Counter({10: 799, 15: 797, 8: 796, 9: 795, 11: 793, 13: 792, 7: 792, 5: 790, 14: 789, 12: 788, 2: 788, 3: 786, 6: 780, 1: 779, 4: 771, 17: 752, 16: 728, 0: 639, 18: 620, 19: 502})\n",
      "Validation: Counter({10: 100, 8: 100, 9: 100, 15: 100, 14: 99, 13: 99, 7: 99, 11: 99, 5: 99, 3: 98, 12: 98, 2: 98, 6: 97, 1: 97, 4: 96, 17: 94, 16: 91, 0: 80, 18: 78, 19: 63})\n",
      "Test: Counter({8: 100, 10: 100, 15: 100, 11: 99, 9: 99, 7: 99, 13: 99, 14: 99, 5: 99, 2: 99, 6: 98, 3: 98, 12: 98, 1: 97, 4: 96, 17: 94, 16: 91, 0: 80, 18: 77, 19: 63})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "news_train_data = pd.DataFrame(news_dataset[\"train\"])\n",
    "news_test_data = pd.DataFrame(news_dataset[\"test\"])\n",
    "\n",
    "news_data = Dataset.from_pandas(pd.concat([news_train_data, news_test_data], ignore_index=True))\n",
    "news_data = news_data.shuffle(seed=42)\n",
    "\n",
    "news_temp_sentences, news_test_sentences, news_temp_labels, news_test_labels = train_test_split(\n",
    "                                                news_data['text'],\n",
    "                                                news_data['label'], \n",
    "                                                test_size=0.1, \n",
    "                                                random_state=42,\n",
    "                                                stratify=news_data['label'])\n",
    "\n",
    "news_train_sentences, news_val_sentences, news_train_labels, news_val_labels = train_test_split(\n",
    "                                                news_temp_sentences,\n",
    "                                                news_temp_labels,\n",
    "                                                test_size=0.1111,\n",
    "                                                random_state=42,\n",
    "                                                stratify=news_temp_labels)\n",
    "\n",
    "print(\"Dimensioni dei set:\")\n",
    "print(f\"Train: {len(news_train_sentences)}\")\n",
    "print(f\"Validation: {len(news_val_sentences)}\")\n",
    "print(f\"Test: {len(news_test_sentences)}\")\n",
    "\n",
    "# Verifica distribuzione delle etichette\n",
    "print(\"\\nDistribuzione delle etichette:\")\n",
    "print(f\"Train: {Counter(news_train_labels)}\")\n",
    "print(f\"Validation: {Counter(news_val_labels)}\")\n",
    "print(f\"Test: {Counter(news_test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c4078d1-75b4-482f-a84b-f8cfce101b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Inizializzo il Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Ottiengo i dataset\n",
    "news_training_data = ClassificationDataset(sentences = news_train_sentences,\n",
    "                           labels = news_train_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "news_validation_data = ClassificationDataset(sentences = news_val_sentences,\n",
    "                           labels = news_val_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "news_test_data = ClassificationDataset(sentences = news_test_sentences,\n",
    "                           labels = news_test_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# Creo i DataLoader\n",
    "news_train_loader = DataLoader(news_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "news_val_loader = DataLoader(news_validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "news_test_loader = DataLoader(news_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3371874-6817-4682-a72a-05a02ab9cfdf",
   "metadata": {},
   "source": [
    "### DBpedia 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6b0ecac-8173-4bff-87bf-0b3b47b124cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensioni dei set:\n",
      "Train: 28000\n",
      "Validation: 5000\n",
      "Test: 5000\n",
      "\n",
      "Distribuzione delle etichette:\n",
      "Train: Counter({0: 2000, 1: 2000, 2: 2000, 3: 2000, 4: 2000, 5: 2000, 6: 2000, 7: 2000, 8: 2000, 9: 2000, 10: 2000, 11: 2000, 12: 2000, 13: 2000})\n",
      "Validation: Counter({11: 358, 9: 358, 2: 357, 7: 357, 1: 357, 10: 357, 6: 357, 3: 357, 0: 357, 13: 357, 12: 357, 5: 357, 8: 357, 4: 357})\n",
      "Test: Counter({12: 358, 10: 358, 1: 357, 6: 357, 3: 357, 5: 357, 9: 357, 4: 357, 2: 357, 0: 357, 11: 357, 8: 357, 7: 357, 13: 357})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_545895/2065745407.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dbpedia_train_dataset = dbpedia_train_dataset.groupby(\"label\").apply(lambda x: x.sample(n=2000, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Ottenimento del dataset\n",
    "dbpedia_train_dataset = pd.read_csv('./dataset/dbpedia-ontology-dataset/train.csv')\n",
    "dbpedia_val_test_dataset = pd.read_csv('./dataset/dbpedia-ontology-dataset/test.csv')\n",
    "\n",
    "# Costruisco il training set in modo da avere 2000 esempi per ognuna delle 14 classi\n",
    "dbpedia_train_dataset = dbpedia_train_dataset.groupby(\"label\").apply(lambda x: x.sample(n=2000, random_state=42))\n",
    "dbpedia_train_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dbpedia_train_sentences, dbpedia_train_labels = dbpedia_train_dataset['content'], dbpedia_train_dataset['label']\n",
    "\n",
    "# Divido i dati di test in test e val set\n",
    "dbpedia_val_sentences, dbpedia_test_sentences, dbpedia_val_labels, dbpedia_test_labels = train_test_split(\n",
    "                                                dbpedia_val_test_dataset['content'], \n",
    "                                                dbpedia_val_test_dataset['label'], \n",
    "                                                train_size=5000,\n",
    "                                                test_size=5000,\n",
    "                                                random_state=42,\n",
    "                                                stratify=dbpedia_val_test_dataset['label']\n",
    "                                            )\n",
    "\n",
    "dbpedia_val_sentences = dbpedia_val_sentences.reset_index(drop=True)\n",
    "dbpedia_val_labels = dbpedia_val_labels.reset_index(drop=True)\n",
    "\n",
    "dbpedia_test_sentences = dbpedia_test_sentences.reset_index(drop=True)\n",
    "dbpedia_test_labels = dbpedia_test_labels.reset_index(drop=True)\n",
    "\n",
    "print(\"Dimensioni dei set:\")\n",
    "print(f\"Train: {len(dbpedia_train_sentences)}\")\n",
    "print(f\"Validation: {len(dbpedia_val_sentences)}\")\n",
    "print(f\"Test: {len(dbpedia_test_sentences)}\")\n",
    "\n",
    "print(\"\\nDistribuzione delle etichette:\")\n",
    "print(f\"Train: {Counter(dbpedia_train_labels.tolist())}\")\n",
    "print(f\"Validation: {Counter(dbpedia_val_labels.tolist())}\")\n",
    "print(f\"Test: {Counter(dbpedia_test_labels.tolist())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "503af41e-890d-4243-ab33-3d1043dc867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_SEQ_LEN = 512\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Inizializzo il Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Ottiengo i dataset\n",
    "dbpedia_training_data = ClassificationDataset(sentences = dbpedia_train_sentences,\n",
    "                           labels = dbpedia_train_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "dbpedia_validation_data = ClassificationDataset(sentences = dbpedia_val_sentences,\n",
    "                           labels = dbpedia_val_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "dbpedia_test_data = ClassificationDataset(sentences = dbpedia_test_sentences,\n",
    "                           labels = dbpedia_test_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# Creo i DataLoader\n",
    "dbpedia_train_loader = DataLoader(dbpedia_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dbpedia_val_loader = DataLoader(dbpedia_validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "dbpedia_test_loader = DataLoader(dbpedia_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5a6730-c8ca-49c6-b53e-2a362114ae3f",
   "metadata": {},
   "source": [
    "### Emotion Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa2b0cb4-f1cc-43ca-afcc-b6490b746b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 16000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Ottenimento del dataset\n",
    "emotion_dataset = load_dataset(\"dair-ai/emotion\")\n",
    "print(emotion_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1fdfc00-c4b0-45af-9054-2533514abb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensioni dei set:\n",
      "Train: 16000\n",
      "Validation: 2000\n",
      "Test: 2000\n",
      "\n",
      "Distribuzione delle etichette:\n",
      "Train: Counter({1: 5362, 0: 4666, 3: 2159, 4: 1937, 2: 1304, 5: 572})\n",
      "Validation: Counter({1: 704, 0: 550, 3: 275, 4: 212, 2: 178, 5: 81})\n",
      "Test: Counter({1: 695, 0: 581, 3: 275, 4: 224, 2: 159, 5: 66})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Divido i dati in training, validation e test set\n",
    "emotion_train_data = emotion_dataset[\"train\"].shuffle(seed=42)\n",
    "emotion_val_data = emotion_dataset[\"validation\"].shuffle(seed=42)\n",
    "emotion_test_data = emotion_dataset[\"test\"].shuffle(seed=42)\n",
    "\n",
    "\n",
    "emotion_train_sentences, emotion_train_labels = emotion_train_data['text'],emotion_train_data['label']\n",
    "emotion_val_sentences, emotion_val_labels = emotion_val_data['text'],emotion_val_data['label']\n",
    "emotion_test_sentences, emotion_test_labels = emotion_test_data['text'],emotion_test_data['label']\n",
    "\n",
    "\n",
    "print(\"Dimensioni dei set:\")\n",
    "print(f\"Train: {len(emotion_train_sentences)}\")\n",
    "print(f\"Validation: {len(emotion_val_sentences)}\")\n",
    "print(f\"Test: {len(emotion_test_sentences)}\")\n",
    "\n",
    "# Verifica distribuzione delle etichette\n",
    "print(\"\\nDistribuzione delle etichette:\")\n",
    "print(f\"Train: {Counter(emotion_train_labels)}\")\n",
    "print(f\"Validation: {Counter(emotion_val_labels)}\")\n",
    "print(f\"Test: {Counter(emotion_test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82237f48-dfb4-4a07-b2e9-8696efdabc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Inizializzo il Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Ottiengo i dataset\n",
    "emotion_training_data = ClassificationDataset(\n",
    "                            sentences = emotion_train_sentences,\n",
    "                            labels = emotion_train_labels,\n",
    "                            tokenizer = tokenizer,\n",
    "                            max_len = MAX_SEQ_LEN)\n",
    "\n",
    "emotion_validation_data = ClassificationDataset(\n",
    "                            sentences = emotion_val_sentences,\n",
    "                            labels = emotion_val_labels,\n",
    "                            tokenizer = tokenizer,\n",
    "                            max_len = MAX_SEQ_LEN)\n",
    "\n",
    "emotion_test_data = ClassificationDataset(\n",
    "                            sentences = emotion_test_sentences,\n",
    "                            labels = emotion_test_labels,\n",
    "                            tokenizer = tokenizer,\n",
    "                            max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# Creo i DataLoader\n",
    "emotion_train_loader = DataLoader(emotion_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "emotion_val_loader = DataLoader(emotion_validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "emotion_test_loader = DataLoader(emotion_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d980b-afc0-43ee-87aa-d7ef95be1e55",
   "metadata": {},
   "source": [
    "### RTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82d28210-38e1-4eac-a69a-77bb1547d72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 2490\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 277\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Ottenimento del dataset\n",
    "rte_dataset = load_dataset(\"glue\", \"rte\")\n",
    "print(rte_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb720112-8636-432f-bb92-74838d8d9351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensioni dei set:\n",
      "Train: 2213\n",
      "Validation: 277\n",
      "Test: 277\n",
      "\n",
      "Distribuzione delle etichette:\n",
      "Train: Counter({0: 1115, 1: 1098})\n",
      "Validation: Counter({0: 140, 1: 137})\n",
      "Test: Counter({0: 140, 1: 137})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "rte_train_data = pd.DataFrame(rte_dataset[\"train\"])\n",
    "rte_val_data = pd.DataFrame(rte_dataset[\"validation\"])\n",
    "\n",
    "rte_data = Dataset.from_pandas(pd.concat([rte_train_data, rte_val_data], ignore_index=True))\n",
    "rte_data = rte_data.shuffle(seed=42)\n",
    "\n",
    "rte_temp_sentences1, rte_test_sentences1, rte_temp_sentences2, rte_test_sentences2,  rte_temp_labels, rte_test_labels = train_test_split(\n",
    "                                                rte_data['sentence1'],\n",
    "                                                rte_data['sentence2'],\n",
    "                                                rte_data['label'], \n",
    "                                                test_size=0.1, \n",
    "                                                random_state=42,\n",
    "                                                stratify=rte_data['label'])\n",
    "\n",
    "rte_train_sentences1, rte_val_sentences1, rte_train_sentences2, rte_val_sentences2, rte_train_labels, rte_val_labels = train_test_split(\n",
    "                                                rte_temp_sentences1,\n",
    "                                                rte_temp_sentences2,\n",
    "                                                rte_temp_labels,\n",
    "                                                test_size=0.1111,\n",
    "                                                random_state=42,\n",
    "                                                stratify=rte_temp_labels)\n",
    "\n",
    "print(\"Dimensioni dei set:\")\n",
    "print(f\"Train: {len(rte_train_sentences1)}\")\n",
    "print(f\"Validation: {len(rte_val_sentences1)}\")\n",
    "print(f\"Test: {len(rte_test_sentences1)}\")\n",
    "\n",
    "# Verifica distribuzione delle etichette\n",
    "print(\"\\nDistribuzione delle etichette:\")\n",
    "print(f\"Train: {Counter(rte_train_labels)}\")\n",
    "print(f\"Validation: {Counter(rte_val_labels)}\")\n",
    "print(f\"Test: {Counter(rte_test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92b05801-d580-458d-921e-90b0d0f5658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NLIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences1, sentences2 , labels, tokenizer, max_len):\n",
    "        self.sentences1 = sentences1\n",
    "        self.sentences2 = sentences2\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences1)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        sentence1 = self.sentences1[index]\n",
    "        sentence2 = self.sentences2[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence1,\n",
    "            sentence2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding[\"token_type_ids\"].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b0760a5-b69e-4a2a-8b33-a2fb42c8d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Inizializzo il Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Ottiengo i dataset\n",
    "rte_training_data = NLIDataset(\n",
    "                           sentences1 = rte_train_sentences1,\n",
    "                           sentences2 = rte_train_sentences2,\n",
    "                           labels = rte_train_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "rte_validation_data = NLIDataset(\n",
    "                           sentences1 = rte_val_sentences1,\n",
    "                           sentences2 = rte_val_sentences2,\n",
    "                           labels = rte_val_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "rte_test_data = NLIDataset(\n",
    "                           sentences1 = rte_test_sentences1,\n",
    "                           sentences2 = rte_test_sentences2,\n",
    "                           labels = rte_test_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# Creo i DataLoader\n",
    "rte_train_loader = DataLoader(rte_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "rte_val_loader = DataLoader(rte_validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "rte_test_loader = DataLoader(rte_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a215fdec-8edb-4204-b422-54c9ced49332",
   "metadata": {},
   "source": [
    "### QQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76869a13-fba9-4693-8d9b-34e322056ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question1', 'question2', 'label', 'idx'],\n",
      "        num_rows: 363846\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question1', 'question2', 'label', 'idx'],\n",
      "        num_rows: 40430\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question1', 'question2', 'label', 'idx'],\n",
      "        num_rows: 390965\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Ottenimento del dataset\n",
    "qqp_dataset = load_dataset(\"glue\", \"qqp\")\n",
    "print(qqp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90fea4fe-e838-47e9-81ac-c669c235c246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensioni dei set:\n",
      "Train: 20000\n",
      "Validation: 5000\n",
      "Test: 5000\n",
      "\n",
      "Distribuzione delle etichette:\n",
      "Train: Counter({1: 10000, 0: 10000})\n",
      "Validation: Counter({0: 2500, 1: 2500})\n",
      "Test: Counter({0: 2500, 1: 2500})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "def balance_dataset(dataset, num_example):\n",
    "    class_0 = [example for example in dataset if example[\"label\"] == 0][:num_example]\n",
    "    class_1 = [example for example in dataset if example[\"label\"] == 1][:num_example]\n",
    "\n",
    "    balanced_data = class_0 + class_1\n",
    "\n",
    "    balanced_dataset = Dataset.from_list(balanced_data)\n",
    "    balanced_dataset = balanced_dataset.shuffle(seed=42)\n",
    "\n",
    "    return balanced_dataset \n",
    "\n",
    "\n",
    "def get_val_test_set(dataset, val_size, test_size):\n",
    "    class_0 = [example for example in dataset if example[\"label\"] == 0]\n",
    "    class_1 = [example for example in dataset if example[\"label\"] == 1]    \n",
    "\n",
    "    val_set = class_0[:val_size//2] + class_1[:val_size//2]\n",
    "    test_set = class_0[val_size//2:val_size//2 + test_size//2] + class_1[val_size//2:val_size//2 + test_size//2]\n",
    "\n",
    "    val_set = Dataset.from_dict({k: [example[k] for example in val_set] for k in val_set[0]})\n",
    "    test_set = Dataset.from_dict({k: [example[k] for example in test_set] for k in test_set[0]})\n",
    "    \n",
    "    val_set = val_set.shuffle(seed=42)\n",
    "    test_set = test_set.shuffle(seed=42)\n",
    "\n",
    "    return val_set, test_set\n",
    "\n",
    "\n",
    "qqp_train_data = qqp_dataset[\"train\"]\n",
    "qqp_val_test_data = qqp_dataset[\"validation\"]\n",
    "\n",
    "qqp_train_data = balance_dataset(qqp_train_data,10000)\n",
    "qqp_val_data, qqp_test_data  =  get_val_test_set(qqp_val_test_data, 5000, 5000)\n",
    "\n",
    "qqp_train_questions1, qqp_train_questions2, qqp_train_labels = qqp_train_data['question1'],  qqp_train_data['question2'], qqp_train_data['label']\n",
    "qqp_val_questions1, qqp_val_questions2, qqp_val_labels = qqp_val_data['question1'],  qqp_val_data['question2'], qqp_val_data['label']\n",
    "qqp_test_questions1, qqp_test_questions2, qqp_test_labels = qqp_test_data['question1'],  qqp_test_data['question2'], qqp_test_data['label']\n",
    "\n",
    "print(\"Dimensioni dei set:\")\n",
    "print(f\"Train: {len(qqp_train_questions1)}\")\n",
    "print(f\"Validation: {len(qqp_val_questions1)}\")\n",
    "print(f\"Test: {len(qqp_test_questions1)}\")\n",
    "\n",
    "print(\"\\nDistribuzione delle etichette:\")\n",
    "print(f\"Train: {Counter(qqp_train_labels)}\")\n",
    "print(f\"Validation: {Counter(qqp_val_labels)}\")\n",
    "print(f\"Test: {Counter(qqp_test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3da8dc79-4f79-4edc-8fbd-12a24e46147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_SEQ_LEN = 256\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Inizializzo il Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Ottiengo i dataset\n",
    "qqp_training_data = NLIDataset(\n",
    "                           sentences1 = qqp_train_questions1,\n",
    "                           sentences2 = qqp_train_questions2,\n",
    "                           labels = qqp_train_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "qqp_validation_data = NLIDataset(\n",
    "                           sentences1 = qqp_val_questions1,\n",
    "                           sentences2 = qqp_val_questions2,\n",
    "                           labels = qqp_val_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "qqp_test_data = NLIDataset(\n",
    "                           sentences1 = qqp_test_questions1,\n",
    "                           sentences2 = qqp_test_questions2,\n",
    "                           labels = qqp_test_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# Creo i DataLoader\n",
    "qqp_train_loader = DataLoader(qqp_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "qqp_val_loader = DataLoader(qqp_validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "qqp_test_loader = DataLoader(qqp_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11530701-5101-4f17-a9bf-c77ca326f5ad",
   "metadata": {},
   "source": [
    "### COLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eeb6f8aa-6ff5-4b1d-b617-4435e82f4aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 8551\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1043\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1063\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Ottenimento del dataset\n",
    "cola_dataset = load_dataset(\"glue\", \"cola\")\n",
    "print(cola_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82227a80-32f0-47f4-8ba2-2e835c5b6c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensioni dei set:\n",
      "Train: 7674\n",
      "Validation: 960\n",
      "Test: 960\n",
      "\n",
      "Distribuzione delle etichette:\n",
      "Train: Counter({1: 5394, 0: 2280})\n",
      "Validation: Counter({1: 675, 0: 285})\n",
      "Test: Counter({1: 675, 0: 285})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "cola_train_data = pd.DataFrame(cola_dataset[\"train\"])\n",
    "cola_val_data = pd.DataFrame(cola_dataset[\"validation\"])\n",
    "\n",
    "cola_data = Dataset.from_pandas(pd.concat([cola_train_data, cola_val_data], ignore_index=True))\n",
    "cola_data = cola_data.shuffle(seed=42)\n",
    "\n",
    "cola_temp_sentences, cola_test_sentences, cola_temp_labels, cola_test_labels = train_test_split(\n",
    "                                                cola_data['sentence'],\n",
    "                                                cola_data['label'], \n",
    "                                                test_size=0.1, \n",
    "                                                random_state=42,\n",
    "                                                stratify=cola_data['label'])\n",
    "\n",
    "cola_train_sentences, cola_val_sentences, cola_train_labels, cola_val_labels = train_test_split(\n",
    "                                                cola_temp_sentences, \n",
    "                                                cola_temp_labels,\n",
    "                                                test_size=0.1111,\n",
    "                                                random_state=42,\n",
    "                                                stratify=cola_temp_labels)\n",
    "\n",
    "print(\"Dimensioni dei set:\")\n",
    "print(f\"Train: {len(cola_train_sentences)}\")\n",
    "print(f\"Validation: {len(cola_val_sentences)}\")\n",
    "print(f\"Test: {len(cola_test_sentences)}\")\n",
    "\n",
    "# Verifica distribuzione delle etichette\n",
    "print(\"\\nDistribuzione delle etichette:\")\n",
    "print(f\"Train: {Counter(cola_train_labels)}\")\n",
    "print(f\"Validation: {Counter(cola_val_labels)}\")\n",
    "print(f\"Test: {Counter(cola_test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a53f298f-e20a-44b6-944b-cef91a8f5438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Inizializzo il Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Ottiengo i dataset\n",
    "cola_training_data = ClassificationDataset(sentences = cola_train_sentences,\n",
    "                           labels = cola_train_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "cola_validation_data = ClassificationDataset(sentences = cola_val_sentences,\n",
    "                           labels = cola_val_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "cola_test_data = ClassificationDataset(sentences = cola_test_sentences,\n",
    "                           labels = cola_test_labels,\n",
    "                           tokenizer = tokenizer,\n",
    "                           max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# Creo i DataLoader\n",
    "cola_train_loader = DataLoader(cola_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "cola_val_loader = DataLoader(cola_validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "cola_test_loader = DataLoader(cola_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f27056e-2de7-4e80-a7cd-7faa32794dac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Controllo similarità dei modelli del pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393aa12f-cc9f-4bb3-97f4-1105b93d85e2",
   "metadata": {},
   "source": [
    "#### AG News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "151716e9-20c3-4b4a-985c-ff12ee0a5679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ottenimento del dataset\n",
    "\n",
    "# ag_dataset = load_dataset(\"ag_news\")\n",
    "# print(ag_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd6cdac0-58e3-49e7-a2cb-894c5d1e3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from collections import Counter\n",
    "\n",
    "# ag_train_dataset = ag_dataset[\"train\"]\n",
    "# ag_test_dataset = ag_dataset[\"test\"]\n",
    "\n",
    "# ag_train_sentences, ag_val_sentences, ag_train_labels, ag_val_labels = train_test_split(\n",
    "#                                                   ag_train_dataset['text'], \n",
    "#                                                   ag_train_dataset['label'],\n",
    "#                                                   test_size=4000, \n",
    "#                                                   train_size=20000,\n",
    "#                                                   random_state=42,\n",
    "#                                                   shuffle=True,\n",
    "#                                                   stratify=ag_train_dataset['label'])\n",
    "\n",
    "# ag_test_sentences, ag_test_labels = ag_test_dataset['text'], ag_test_dataset['label']\n",
    "\n",
    "\n",
    "# print(\"Dimensioni dei set:\")\n",
    "# print(f\"Train: {len(ag_train_sentences)}\")\n",
    "# print(f\"Validation: {len(ag_val_sentences)}\")\n",
    "# print(f\"Test: {len(ag_test_sentences)}\")\n",
    "\n",
    "# # Verifica distribuzione delle etichette\n",
    "# print(\"\\nDistribuzione delle etichette:\")\n",
    "# print(f\"Train: {Counter(ag_train_labels)}\")\n",
    "# print(f\"Validation: {Counter(ag_val_labels)}\")\n",
    "# print(f\"Test: {Counter(ag_test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0210bf8e-1882-45c6-b752-5768952f10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizer\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# MAX_SEQ_LEN = 128\n",
    "\n",
    "# # Inizializza il Tokenizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# #Ottieni i dataset\n",
    "# ag_training_data = ClassificationDataset( sentences = ag_train_sentences,\n",
    "#                            labels = ag_train_labels,\n",
    "#                            tokenizer = tokenizer,\n",
    "#                            max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# ag_validation_data = ClassificationDataset( sentences = ag_val_sentences,\n",
    "#                              labels = ag_val_labels,\n",
    "#                              tokenizer = tokenizer,\n",
    "#                              max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# ag_test_data = ClassificationDataset( sentences = ag_test_sentences,\n",
    "#                        labels = ag_test_labels,\n",
    "#                        tokenizer = tokenizer,\n",
    "#                        max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# # Creo i DataLoader\n",
    "# ag_train_loader = DataLoader(ag_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# ag_val_loader = DataLoader(ag_validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# ag_test_loader = DataLoader(ag_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69a556-2f1f-4327-bc56-ca76ec9ace3a",
   "metadata": {},
   "source": [
    "#### SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79be8e58-6170-4b41-9ed0-354f35a187a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# sst_dataset = load_dataset('glue','sst2')\n",
    "# print(sst_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72f1e865-2100-46b7-b2df-63231b0acf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from collections import Counter\n",
    "\n",
    "# sst_data = sst_dataset['train'].shuffle(seed=42)\n",
    "\n",
    "# sst_temp_sentences, sst_test_sentences, sst_temp_labels, sst_test_labels = train_test_split(\n",
    "#                                                   sst_data['sentence'], \n",
    "#                                                   sst_data['label'], \n",
    "#                                                   test_size=4000, \n",
    "#                                                   random_state=42,\n",
    "#                                                   stratify=sst_data['label'])\n",
    "\n",
    "# sst_train_sentences, sst_val_sentences, sst_train_labels, sst_val_labels = train_test_split(\n",
    "#                                                   sst_temp_sentences, \n",
    "#                                                   sst_temp_labels,\n",
    "#                                                   train_size=20000,\n",
    "#                                                   test_size=4000, \n",
    "#                                                   random_state=42,\n",
    "#                                                   stratify=sst_temp_labels)\n",
    "\n",
    "\n",
    "# print(\"Dimensioni dei set:\")\n",
    "# print(f\"Train: {len(sst_train_sentences)}\")\n",
    "# print(f\"Validation: {len(sst_val_sentences)}\")\n",
    "# print(f\"Test: {len(sst_test_sentences)}\")\n",
    "\n",
    "# # Verifica distribuzione delle etichette\n",
    "# print(\"\\nDistribuzione delle etichette:\")\n",
    "# print(f\"Train: {Counter(sst_train_labels)}\")\n",
    "# print(f\"Validation: {Counter(sst_val_labels)}\")\n",
    "# print(f\"Test: {Counter(sst_test_labels)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2d50a93-1511-464e-bb70-d5b8b4b68310",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# MAX_SEQ_LEN = 128\n",
    "\n",
    "# # Inizializza il Tokenizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# #Ottieni i dataset\n",
    "# sst_training_data = ClassificationDataset(sentences = sst_train_sentences,\n",
    "#                            labels = sst_train_labels,\n",
    "#                            tokenizer = tokenizer,\n",
    "#                            max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# sst_validation_data = ClassificationDataset(sentences = sst_val_sentences,\n",
    "#                            labels = sst_val_labels,\n",
    "#                            tokenizer = tokenizer,\n",
    "#                            max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# sst_test_data = ClassificationDataset(sentences = sst_test_sentences,\n",
    "#                            labels = sst_test_labels,\n",
    "#                            tokenizer = tokenizer,\n",
    "#                            max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# # Creo i DataLoader\n",
    "# sst_train_loader = DataLoader(sst_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# sst_val_loader = DataLoader(sst_validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# sst_test_loader = DataLoader(sst_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c3c113-95b3-47ea-b124-25151044dbad",
   "metadata": {},
   "source": [
    "#### EmoInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "535fc048-af02-4fe2-8545-fecad9001b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_emoint_dataset(file_path):\n",
    "#     label_map = {\"anger\": 0, \"joy\": 1, \"sadness\": 2, \"fear\": 3}\n",
    "    \n",
    "#     df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"id\", \"sentence\", \"label\", \"intensity\"])\n",
    "    \n",
    "#     df = df[[\"sentence\", \"label\"]]\n",
    "#     df[\"label\"] = df[\"label\"].map(label_map)\n",
    "    \n",
    "#     df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n",
    "# ei_dataset = load_emoint_dataset(\"./dataset/EmotionIntensity dataset/Emotion Intensity Dataset.txt\")\n",
    "# print(ei_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b0b86bb1-7e2e-45d3-b8d7-a37625be7f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from collections import Counter\n",
    "\n",
    "# # Divido i dati in training set, validation set e test set\n",
    "# ei_temp_sentences, ei_test_sentences, ei_temp_labels, ei_test_labels = train_test_split(\n",
    "#                                                 ei_dataset['sentence'],\n",
    "#                                                 ei_dataset['label'], \n",
    "#                                                 test_size=0.1, \n",
    "#                                                 random_state=42,\n",
    "#                                                 stratify=ei_dataset['label'])\n",
    "\n",
    "# ei_train_sentences, ei_val_sentences, ei_train_labels, ei_val_labels = train_test_split(\n",
    "#                                                 ei_temp_sentences,\n",
    "#                                                 ei_temp_labels,\n",
    "#                                                 test_size=0.1111,\n",
    "#                                                 random_state=42,\n",
    "#                                                 stratify=ei_temp_labels)\n",
    "\n",
    "# ei_train_sentences = ei_train_sentences.reset_index(drop=True)\n",
    "# ei_val_sentences = ei_val_sentences.reset_index(drop=True)\n",
    "# ei_test_sentences = ei_test_sentences.reset_index(drop=True)\n",
    "# ei_train_labels = ei_train_labels.reset_index(drop=True)\n",
    "# ei_val_labels = ei_val_labels.reset_index(drop=True)\n",
    "# ei_test_labels = ei_test_labels.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# print(\"Dimensioni dei set:\")\n",
    "# print(f\"Train: {len(ei_train_sentences)}\")\n",
    "# print(f\"Validation: {len(ei_val_sentences)}\")\n",
    "# print(f\"Test: {len(ei_test_sentences)}\")\n",
    "\n",
    "# # Verifica distribuzione delle etichette\n",
    "# print(\"\\nDistribuzione delle etichette:\")\n",
    "# print(f\"Train: {Counter(ei_train_labels)}\")\n",
    "# print(f\"Validation: {Counter(ei_val_labels)}\")\n",
    "# print(f\"Test: {Counter(ei_test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2b4c5da5-5f7c-45c9-a6eb-271963ef9af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# MAX_SEQ_LEN = 128\n",
    "\n",
    "# # Inizializza il Tokenizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# #Ottieni i dataset\n",
    "# ei_training_data = ClassificationDataset(sentences = ei_train_sentences,\n",
    "#                            labels = ei_train_labels,\n",
    "#                            tokenizer = tokenizer,\n",
    "#                            max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# ei_validation_data = ClassificationDataset(sentences = ei_val_sentences,\n",
    "#                            labels = ei_val_labels,\n",
    "#                            tokenizer = tokenizer,\n",
    "#                            max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# ei_test_data = ClassificationDataset(sentences = ei_test_sentences,\n",
    "#                            labels = ei_test_labels,\n",
    "#                            tokenizer = tokenizer,\n",
    "#                            max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# # Creo i DataLoader\n",
    "# ei_train_loader = DataLoader(ei_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# ei_val_loader = DataLoader(ei_validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# ei_test_loader = DataLoader(ei_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127fa1a-b7b7-41ce-a0ad-9082542eec8d",
   "metadata": {},
   "source": [
    "#### MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cece4c74-6d53-4fd6-a042-a82a840cfd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from collections import Counter\n",
    "\n",
    "# # Carico il datast\n",
    "# mnli_dataset = load_dataset('glue', 'mnli')\n",
    "# print(mnli_dataset)\n",
    "\n",
    "\n",
    "# # Divido i dati in training set, validation set e test set\n",
    "# mnli_data = mnli_dataset['train'].shuffle(seed=42)\n",
    "\n",
    "# mnli_temp_premises, mnli_test_premises, mnli_temp_hypotheses, mnli_test_hypotheses, mnli_temp_labels, mnli_test_labels = train_test_split(\n",
    "#                                                   mnli_data['premise'], \n",
    "#                                                   mnli_data['hypothesis'],                \n",
    "#                                                   mnli_data['label'], \n",
    "#                                                   test_size=3000, \n",
    "#                                                   random_state=42,\n",
    "#                                                   stratify=mnli_data['label'])\n",
    "\n",
    "# mnli_train_premises, mnli_val_premises, mnli_train_hypotheses, mnli_val_hypotheses, mnli_train_labels, mnli_val_labels = train_test_split(\n",
    "#                                                   mnli_temp_premises, \n",
    "#                                                   mnli_temp_hypotheses,\n",
    "#                                                   mnli_temp_labels,\n",
    "#                                                   train_size=45000,\n",
    "#                                                   test_size=3000, \n",
    "#                                                   random_state=42,\n",
    "#                                                   stratify=mnli_temp_labels)\n",
    "\n",
    "# print(\"Dimensioni dei set:\")\n",
    "# print(f\"Train: {len(mnli_train_premises)}\")\n",
    "# print(f\"Validation: {len(mnli_val_premises)}\")\n",
    "# print(f\"Test: {len(mnli_test_premises)}\")\n",
    "\n",
    "# # Verifica distribuzione delle etichette\n",
    "# print(\"\\nDistribuzione delle etichette:\")\n",
    "# print(f\"Train: {Counter(mnli_train_labels)}\")\n",
    "# print(f\"Validation: {Counter(mnli_val_labels)}\")\n",
    "# print(f\"Test: {Counter(mnli_test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85bcd423-1c81-4375-b07d-24185585bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# MAX_SEQ_LEN = 512\n",
    "\n",
    "# # Inizializza il Tokenizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# #Ottieni i dataset\n",
    "# mnli_training_data = NLIDataset(sentences1 = mnli_train_premises,\n",
    "#                             sentences2 = mnli_train_hypotheses,\n",
    "#                             labels = mnli_train_labels,\n",
    "#                             tokenizer = tokenizer,\n",
    "#                             max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# mnli_validation_data = NLIDataset(sentences1 = mnli_val_premises,\n",
    "#                             sentences2 = mnli_val_hypotheses,\n",
    "#                             labels = mnli_val_labels,\n",
    "#                             tokenizer = tokenizer,\n",
    "#                             max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# mnli_test_data = NLIDataset(sentences1 = mnli_test_premises,\n",
    "#                             sentences2 = mnli_test_hypotheses,\n",
    "#                             labels = mnli_test_labels,\n",
    "#                             tokenizer = tokenizer,\n",
    "#                             max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# # Creo i DataLoader\n",
    "# mnli_train_loader = DataLoader(mnli_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# mnli_val_loader = DataLoader(mnli_validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# mnli_test_loader = DataLoader(mnli_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05dcb05-ca36-490d-9898-26dc48a36b41",
   "metadata": {},
   "source": [
    "#### PAWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c27957d9-f335-478b-b2d2-b48db1484806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # Carico il datast\n",
    "# paws_dataset = load_dataset(\"google-research-datasets/paws\", \"labeled_final\")\n",
    "# print(paws_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "192b196b-0bd0-47b1-9661-56c51a32f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from collections import Counter\n",
    "\n",
    "# paws_train_set = paws_dataset[\"train\"]\n",
    "# paws_val_set = paws_dataset[\"validation\"]\n",
    "# paws_test_set = paws_dataset[\"test\"]\n",
    "\n",
    "# paws_train_sentences1, paws_train_sentences2, paws_train_labels = paws_train_set['sentence1'], paws_train_set['sentence2'], paws_train_set['label']\n",
    "# paws_val_sentences1, paws_val_sentences2, paws_val_labels = paws_val_set['sentence1'], paws_val_set['sentence2'], paws_val_set['label']\n",
    "# paws_test_sentences1, paws_test_sentences2, paws_test_labels = paws_test_set['sentence1'], paws_test_set['sentence2'], paws_test_set['label']\n",
    "\n",
    "# print(\"Dimensioni dei set:\")\n",
    "# print(f\"Train: {len(paws_train_sentences1)}\")\n",
    "# print(f\"Validation: {len(paws_val_sentences1)}\")\n",
    "# print(f\"Test: {len(paws_test_sentences1)}\")\n",
    "\n",
    "# # Verifica distribuzione delle etichette\n",
    "# print(\"\\nDistribuzione delle etichette:\")\n",
    "# print(f\"Train: {Counter(paws_train_labels)}\")\n",
    "# print(f\"Validation: {Counter(paws_val_labels)}\")\n",
    "# print(f\"Test: {Counter(paws_test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3db22be0-c6ab-4acd-b920-52bbf1ac0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# MAX_SEQ_LEN = 256 \n",
    "\n",
    "# # Inizializza il Tokenizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# #Ottieni i dataset\n",
    "# paws_training_data = NLIDataset(sentences1 = paws_train_sentences1,\n",
    "#                             sentences2 = paws_train_sentences2,\n",
    "#                             labels = paws_train_labels,\n",
    "#                             tokenizer = tokenizer,\n",
    "#                             max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# paws_validation_data = NLIDataset(sentences1 = paws_val_sentences1,\n",
    "#                             sentences2 = paws_val_sentences2,\n",
    "#                             labels = paws_val_labels,\n",
    "#                             tokenizer = tokenizer,\n",
    "#                             max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# paws_test_data = NLIDataset(sentences1 = paws_test_sentences1,\n",
    "#                             sentences2 = paws_test_sentences2,\n",
    "#                             labels = paws_test_labels,\n",
    "#                             tokenizer = tokenizer,\n",
    "#                             max_len = MAX_SEQ_LEN)\n",
    "\n",
    "# # Creo i DataLoader\n",
    "# paws_train_loader = DataLoader(paws_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# paws_val_loader = DataLoader(paws_validation_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# paws_test_loader = DataLoader(paws_test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4727b12a-8aa9-4b3c-9c1d-3d2e92a1d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import itertools\n",
    "# import pandas as pd\n",
    "# from tllib.ranking import h_score\n",
    "\n",
    "# # Device\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for task1, task2 in itertools.combinations(model_pool.keys(), 2):\n",
    "#     print(f\"Calcolando la similarità tra {task1} e {task2}...\\n\")\n",
    "\n",
    "#     source_model = model_pool[task1]\n",
    "#     target_loader = globals()[f\"{task2}_train_loader\"]  \n",
    "    \n",
    "#     leep_score = calculate_leep_scores({task1: source_model}, target_loader)[0][\"Score\"]\n",
    "#     logme_score = calculate_logme_scores({task1: source_model}, target_loader, device)[0][\"Score\"]\n",
    "#     h_score_score = calculate_h_scores({task1: source_model}, target_loader, device)[0][\"Score\"]\n",
    "#     nce_score = calculate_nce_scores({task1: source_model}, target_loader, device)[0][\"Score\"]\n",
    "\n",
    "#     results.append([task1, task2, \"leep\", leep_score])\n",
    "#     results.append([task1, task2, \"logme\", logme_score])\n",
    "#     results.append([task1, task2, \"h-score\", h_score_score])\n",
    "#     results.append([task1, task2, \"nce\", nce_score])\n",
    "\n",
    "\n",
    "# df_similarity = pd.DataFrame(results, columns=[\"Task1\", \"Task2\", \"Metrica\", \"Punteggio\"])\n",
    "# print(df_similarity)\n",
    "\n",
    " \n",
    "# csv_filename = \"task_similarity_scores.csv\"\n",
    "# df_similarity.to_csv(csv_filename, index=False)\n",
    "# print(f\"Risultati salvati in {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c110d1-0b04-44c3-bbc1-03ada88abf5c",
   "metadata": {},
   "source": [
    "## Addestramento LoRA cumulativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16eb32d2-96ba-486d-8644-d913461da6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 15:52:19] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
      "[codecarbon WARNING @ 15:52:19] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sul task: sentiment140...\n",
      "\n",
      "Selezione del miglior modello per il task sentiment140 usando h-score...\n",
      "\n",
      "Modelli selezionati per il confronto: ['sst', 'paws', 'ei', 'mnli', 'ag']\n",
      "\n",
      "Source: sst, H-Score: 0.4868\n",
      "Source: paws, H-Score: 0.4240\n",
      "Source: ei, H-Score: 0.3794\n",
      "Source: mnli, H-Score: 0.4430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 16:03:28] Another instance of codecarbon is already running. Exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: ag, H-Score: 0.3276\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining sul task: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Addestramento\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m result = \u001b[43mtrain_target_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_num_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_accuracy\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Salvataggio dei risultati\u001b[39;00m\n\u001b[32m     53\u001b[39m writer.writerow([\n\u001b[32m     54\u001b[39m     task_name, result[\u001b[33m\"\u001b[39m\u001b[33mselected_models\u001b[39m\u001b[33m\"\u001b[39m], result[\u001b[33m\"\u001b[39m\u001b[33mselected_model\u001b[39m\u001b[33m\"\u001b[39m], similarity_metric,\n\u001b[32m     55\u001b[39m     result[\u001b[33m\"\u001b[39m\u001b[33msimilarity_score\u001b[39m\u001b[33m\"\u001b[39m],  result[\u001b[33m\"\u001b[39m\u001b[33mselection_time\u001b[39m\u001b[33m\"\u001b[39m], result[\u001b[33m\"\u001b[39m\u001b[33mtraining_time\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m     result[\u001b[33m\"\u001b[39m\u001b[33mtest_loss\u001b[39m\u001b[33m\"\u001b[39m], result[\u001b[33m\"\u001b[39m\u001b[33mtest_acc\u001b[39m\u001b[33m\"\u001b[39m], result[\u001b[33m\"\u001b[39m\u001b[33mtest_f1\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     59\u001b[39m ])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mtrain_target_task\u001b[39m\u001b[34m(target_name, target_loader, target_num_classes, model_pool, training_data, validation_data, test_data, device, learning_rate, epochs, patience, batch_size, optimizer_class, similarity_metric, min_accuracy)\u001b[39m\n\u001b[32m     64\u001b[39m selection_time = time.time() - start_time\n\u001b[32m     66\u001b[39m selection_emissions = tracker.stop()\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEmissioni selezione : \u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mselection_emissions\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[33;43m.4f\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33m kg\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModello selezionato: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Creo un nuovo modello per il task target\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "# Task target con i rispettivi DataLoader e dati\n",
    "tasks = [\n",
    "     (\"sentiment140\", sent140_train_loader, 2, sent140_training_data, sent140_validation_data, sent140_test_data,5e-4),\n",
    "     (\"imdb\", imdb_train_loader, 13, imdb_training_data, imdb_validation_data, imdb_test_data, 5e-4),\n",
    "     (\"20_news_group\", news_train_loader, 20, news_training_data, news_validation_data, news_test_data, 2e-4),\n",
    "     (\"dbpedia\", dbpedia_train_loader, 14, dbpedia_training_data, dbpedia_validation_data, dbpedia_test_data, 2e-4),\n",
    "     (\"emotion_dataset\", emotion_train_loader, 6, emotion_training_data, emotion_validation_data, emotion_test_data, 1e-4),\n",
    "     (\"rte\", rte_train_loader, 2, rte_training_data, rte_validation_data, rte_test_data, 2e-4),\n",
    "     (\"qqp\", qqp_train_loader, 2, qqp_training_data, qqp_validation_data, qqp_test_data, 1e-4),\n",
    "     (\"cola\", cola_train_loader, 2, cola_training_data, cola_validation_data, cola_test_data, 1e-4),\n",
    "]\n",
    "\n",
    "\n",
    "# Parametri di training\n",
    "epochs = 2\n",
    "patience = 1\n",
    "batch_size = 32\n",
    "optimizer_class = torch.optim.AdamW\n",
    "similarity_metric = \"h-score\"\n",
    "min_accuracy= 0.70\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "csv_filename = \"training_results.csv\"\n",
    "csv_headers = [\n",
    "    \"task_name\", \"selected models\" , \"selected_model\", \"similarity_metric\", \"similarity_score\", \n",
    "    \"selection_time\", \"training_time\", \"emission\", \"selection_emission\",\n",
    "    \"learning_rate\", \"epochs\", \"batch_size\", \"optimizer\",\n",
    "    \"test_loss\", \"test_accuracy\", \"test_f1_score\"\n",
    "]\n",
    "\n",
    "\n",
    "with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(csv_headers)\n",
    "\n",
    "    for task_name, train_loader, target_num_classes, training_data, validation_data, test_data, learning_rate in tasks:\n",
    "        print(f\"Training sul task: {task_name}...\\n\")\n",
    "        \n",
    "        # Addestramento\n",
    "        result = train_target_task(\n",
    "            task_name, train_loader, target_num_classes, model_pool, \n",
    "            training_data, validation_data, test_data, device,\n",
    "            learning_rate, epochs, patience, batch_size, \n",
    "            optimizer_class, similarity_metric, min_accuracy\n",
    "        )\n",
    "\n",
    "\n",
    "        # Salvataggio dei risultati\n",
    "        writer.writerow([\n",
    "            task_name, result[\"selected_models\"], result[\"selected_model\"], similarity_metric,\n",
    "            result[\"similarity_score\"],  result[\"selection_time\"], result[\"training_time\"],\n",
    "            result[\"emission\"], result[\"selection_emission\"], learning_rate, epochs, batch_size,\n",
    "            optimizer_class.__name__, \n",
    "            result[\"test_loss\"], result[\"test_acc\"], result[\"test_f1\"]\n",
    "        ])\n",
    "\n",
    "        print(f\"Training completato per il task: {task_name}.\\n\")\n",
    "\n",
    "\n",
    "print(f\"Risultati salvati in '{csv_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110555f2-73be-4b6d-8541-e0396100e56f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tesisti_RC (GPU 6)",
   "language": "python",
   "name": "tesisti_rc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
