{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10161839,"sourceType":"datasetVersion","datasetId":6274971}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Contronto tra il fine-tuning completo e il fine-tuning con LoRA","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"#### Configurazioni","metadata":{}},{"cell_type":"markdown","source":"Installazione della libreria `loralib` per implementare la Low-Rank Adaptation.","metadata":{}},{"cell_type":"code","source":"!pip install loralib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:12:01.375777Z","iopub.execute_input":"2025-01-22T16:12:01.376071Z","iopub.status.idle":"2025-01-22T16:12:05.973209Z","shell.execute_reply.started":"2025-01-22T16:12:01.376045Z","shell.execute_reply":"2025-01-22T16:12:05.972006Z"}},"outputs":[{"name":"stdout","text":"Collecting loralib\n  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\nDownloading loralib-0.1.2-py3-none-any.whl (10 kB)\nInstalling collected packages: loralib\nSuccessfully installed loralib-0.1.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Carico i file `lora_utilis.py` e `models.py`.","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/lora-utils/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:12:07.902855Z","iopub.execute_input":"2025-01-22T16:12:07.903225Z","iopub.status.idle":"2025-01-22T16:12:07.907501Z","shell.execute_reply.started":"2025-01-22T16:12:07.903196Z","shell.execute_reply":"2025-01-22T16:12:07.906425Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Importo i moduli necessari.","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers\nimport lora_utils, models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:12:10.083767Z","iopub.execute_input":"2025-01-22T16:12:10.084050Z","iopub.status.idle":"2025-01-22T16:12:13.978148Z","shell.execute_reply.started":"2025-01-22T16:12:10.084021Z","shell.execute_reply":"2025-01-22T16:12:13.977511Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Impostazione del seme casuale per la riproducibilità.","metadata":{}},{"cell_type":"code","source":"seed_value = 42\n\nos.environ['PYTHONHASHSEED'] = str(seed_value)\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\n\n# Imposto il seme casuale anche per i calcoli CUDA\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)  \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:12:20.765108Z","iopub.execute_input":"2025-01-22T16:12:20.765575Z","iopub.status.idle":"2025-01-22T16:12:20.819432Z","shell.execute_reply.started":"2025-01-22T16:12:20.765546Z","shell.execute_reply":"2025-01-22T16:12:20.818777Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Sentiment Analisys","metadata":{}},{"cell_type":"markdown","source":"### 1. Ottenimento dei dati e preprocessing\n\nConfronto il fine-tuning completo e quello che usa LoRA innanzitutto sul task di Sentiment Analysis (classificazione binaria) utilizzando il dataset SST-2.\n\nSST-2 contiene esempi che consistono in frasi tratte da recensioni di film le cui etichette sono 1 se la recensione positiva, 0 altrimenti.\n\nUtilizzo la libreria `datasets` di Hugging Face per caricare il dataset e dividerlo in training set, validation set e test set.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\n\ndataset = load_dataset('sst2')\n\n\ndata = dataset['train'].shuffle(seed=42)\n\ntemp_data, test_data, temp_labels, test_labels = train_test_split(data['sentence'], \n                                                  data['label'], \n                                                  test_size=1024, \n                                                  random_state=42,\n                                                  stratify=data['label'])\n\ntrain_data, val_data, train_labels, val_labels = train_test_split(data['sentence'], \n                                                  data['label'],\n                                                  train_size=10000,\n                                                  test_size=1000, \n                                                  random_state=42,\n                                                  stratify=data['label'])\n\nfrom collections import Counter\n\nprint(\"Dimensioni dei set:\")\nprint(f\"Train: {len(train_data)}\")\nprint(f\"Validation: {len(val_data)}\")\nprint(f\"Test: {len(test_data)}\")\n\n# Verifica distribuzione delle etichette\nprint(\"\\nDistribuzione delle etichette:\")\nprint(f\"Train: {Counter(train_labels)}\")\nprint(f\"Validation: {Counter(val_labels)}\")\nprint(f\"Test: {Counter(test_labels)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:12:23.671599Z","iopub.execute_input":"2025-01-22T16:12:23.671892Z","iopub.status.idle":"2025-01-22T16:12:32.000046Z","shell.execute_reply.started":"2025-01-22T16:12:23.671868Z","shell.execute_reply":"2025-01-22T16:12:31.998996Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b983c151d31a45bfb9cfa11de2add2b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2195419938f4153a05549335db668c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c45d8ef836e84401bb1e81c44bc666bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df7d9a0e4fd542ed8b0b0cf00c90b3e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"267bf6faa2c94c558d251cd34a32bd67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b14bc254c4c94d72807e5b4c7468f3d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"236e32ff19a241c094c05993ef0cb39b"}},"metadata":{}},{"name":"stdout","text":"Dimensioni dei set:\nTrain: 10000\nValidation: 1000\nTest: 1024\n\nDistribuzione delle etichette:\nTrain: Counter({1: 5578, 0: 4422})\nValidation: Counter({1: 558, 0: 442})\nTest: Counter({1: 571, 0: 453})\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Creo una classe Dataset personalizzata in cui viene effettuata la tokenizzaione delle recensioni e la conversione dei dati in tensori.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass IMDBDataset(Dataset):\n\n    def __init__(self, sentences, labels, tokenizer, max_len):\n        self.sentences = sentences\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    def __getitem__(self,index):\n        sentence = self.sentences[index]\n        label = self.labels[index]\n        \n        encoding = self.tokenizer.encode_plus(\n            sentence,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors='pt')\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding[\"token_type_ids\"].flatten(),\n            'labels': torch.tensor(label, dtype=torch.float)\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:12:32.001516Z","iopub.execute_input":"2025-01-22T16:12:32.002137Z","iopub.status.idle":"2025-01-22T16:12:32.009787Z","shell.execute_reply.started":"2025-01-22T16:12:32.002100Z","shell.execute_reply":"2025-01-22T16:12:32.009005Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"Inizializzo il Tokenizer BERT per tokenizzare le frasi e creo i dataset personalizzati.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 128\n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\ntraining_data = IMDBDataset(sentences = train_data,\n                           labels = train_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nvalidation_data = IMDBDataset(sentences = val_data,\n                           labels = val_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\ntest_data = IMDBDataset(sentences = test_data,\n                           labels = test_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:12:40.933087Z","iopub.execute_input":"2025-01-22T16:12:40.933520Z","iopub.status.idle":"2025-01-22T16:12:43.012432Z","shell.execute_reply.started":"2025-01-22T16:12:40.933480Z","shell.execute_reply":"2025-01-22T16:12:43.011309Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9c2ae3b819c40208d01c829a934c313"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d49ca8f1abf24329b18c81cb52c0700f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcf4554504594d53be572fa9129caf91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"965bbb8c282e4c288d1f4f33e11f4100"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### 2. Configurazione dei modelli","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel\n\nclass BERTClassifier(nn.Module):\n    \n    def __init__(self, lora: bool = False, r: int = 16):\n        super(BERTClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = torch.nn.Dropout(p=0.3)\n        self.linear = nn.Linear(self.bert.config.hidden_size, 1)\n\n        if lora:\n            print(\"Adding LoRA to BERT\")\n            lora_utils.add_lora_to_bert(self.bert, r=r)\n            lora_utils.mark_only_lora_as_trainable(self.bert)\n\n    \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_bert = self.bert(\n            input_ids, \n            attention_mask=attention_mask, \n            token_type_ids=token_type_ids\n        )\n        output_dropout = self.dropout(output_bert.pooler_output)\n        output = self.linear(output_dropout)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:12:44.817607Z","iopub.execute_input":"2025-01-22T16:12:44.817885Z","iopub.status.idle":"2025-01-22T16:12:45.704818Z","shell.execute_reply.started":"2025-01-22T16:12:44.817864Z","shell.execute_reply":"2025-01-22T16:12:45.703928Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:12:48.123817Z","iopub.execute_input":"2025-01-22T16:12:48.124348Z","iopub.status.idle":"2025-01-22T16:12:48.128507Z","shell.execute_reply.started":"2025-01-22T16:12:48.124304Z","shell.execute_reply":"2025-01-22T16:12:48.127504Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# MODELLO STANDARD\nfull_model = BERTClassifier(lora=False)\nfull_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:12:49.686647Z","iopub.execute_input":"2025-01-22T16:12:49.687038Z","iopub.status.idle":"2025-01-22T16:12:52.549249Z","shell.execute_reply.started":"2025-01-22T16:12:49.687005Z","shell.execute_reply":"2025-01-22T16:12:52.548368Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02b0c7602f214270bdee58cd7b4cb030"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"BERTClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=768, out_features=1, bias=True)\n)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# MODELLO CON LORA\nlora_model = BERTClassifier(lora=True, r=16)\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:12:52.550279Z","iopub.execute_input":"2025-01-22T16:12:52.550580Z","iopub.status.idle":"2025-01-22T16:12:53.288010Z","shell.execute_reply.started":"2025-01-22T16:12:52.550557Z","shell.execute_reply":"2025-01-22T16:12:53.287025Z"}},"outputs":[{"name":"stdout","text":"Adding LoRA to BERT\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"BERTClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=768, out_features=1, bias=True)\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"### 3. Addestramento dei modelli","metadata":{}},{"cell_type":"markdown","source":"Definisco una serie di funzioni per l'addestramento e la valutazione di un modello.","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\n\n# Funzione di training e valutazione\ndef train_and_evaluate_model(model, model_name, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=4):\n    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n    best_accuracy = 0\n    \n    start_time = time.time()\n\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n        print('-' * 10)\n\n        # Training\n        train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, scheduler, device)\n        print(f\"Train loss: {train_loss:.4f}, Train accuracy: {train_acc:.4f}\")\n\n        # Valutazione\n        val_loss, val_acc, val_f1, val_auc = eval_model(model, val_loader, criterion, device)\n        print(f\"Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}\")\n\n        # Salvataggio del modello migliore\n        if val_acc > best_accuracy:\n            torch.save(model.state_dict(),  f\"sa_{model_name}_best_model_state.bin\")\n            best_accuracy = val_acc\n\n        # Salva le metriche\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n    \n    end_time = time.time()\n    total_training_time = end_time - start_time\n    \n    return history, total_training_time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:12:57.657081Z","iopub.execute_input":"2025-01-22T16:12:57.657421Z","iopub.status.idle":"2025-01-22T16:12:57.663793Z","shell.execute_reply.started":"2025-01-22T16:12:57.657391Z","shell.execute_reply":"2025-01-22T16:12:57.662810Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Funzione di training \ndef train_model(model, data_loader, criterion, optimizer, scheduler, device):\n  \n    model = model.train() # imposto il modello in modalità di aggiornamento\n    \n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in data_loader:\n        \n        # Sposto i dati sul device\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch[\"labels\"].unsqueeze(1).to(device)\n\n        #  --- Forward pass ---\n        \n        optimizer.zero_grad()\n \n        outputs = model(\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n            token_type_ids = token_type_ids\n        )\n        \n        loss = criterion(outputs, labels)\n\n\n        # --- Backward pass ---\n        \n        \n        loss.backward()\n\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n        \n        preds = (torch.sigmoid(outputs) > 0.5).long()    # trasformo i dati grezzi in etichette binarie\n        \n        all_preds.extend(preds.detach().cpu().numpy())\n        all_labels.extend(labels.detach().cpu().numpy())\n\n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n   \n    return avg_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:13:02.077345Z","iopub.execute_input":"2025-01-22T16:13:02.077664Z","iopub.status.idle":"2025-01-22T16:13:02.084822Z","shell.execute_reply.started":"2025-01-22T16:13:02.077639Z","shell.execute_reply":"2025-01-22T16:13:02.083769Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Funzione di valutazione\ndef eval_model(model, data_loader, criterion, device):\n    model = model.eval()\n\n    total_loss = 0\n    total_preds = []\n    total_labels = []\n    total_probs = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_masks = batch[\"attention_mask\"].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch[\"labels\"].unsqueeze(1).to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_masks,\n                token_type_ids = token_type_ids\n            )\n            #logits = outputs.logits\n            \n            loss = criterion(outputs, labels)\n\n            total_loss += loss.item()\n            \n            probs = torch.sigmoid(outputs)\n            preds = (probs > 0.5).long()  # Trasformo in etichette binarie\n            \n            total_preds.extend(preds.detach().cpu().numpy())\n            total_labels.extend(labels.detach().cpu().numpy())\n            total_probs.extend(probs.detach().cpu().numpy())\n\n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(total_labels, total_preds)\n    f1 = f1_score(total_labels, total_preds, average=\"weighted\")\n    auc = roc_auc_score(total_labels, total_probs)\n    \n    return avg_loss, accuracy, f1, auc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:13:05.663348Z","iopub.execute_input":"2025-01-22T16:13:05.663678Z","iopub.status.idle":"2025-01-22T16:13:05.670130Z","shell.execute_reply.started":"2025-01-22T16:13:05.663654Z","shell.execute_reply":"2025-01-22T16:13:05.669247Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"Definisco le configurazion principali per il training.","metadata":{}},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 3e-5\nEPOCHS = 4\nBATCH_SIZE = 32\n\n# Creo i DataLoader\ntrain_loader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(train_loader) * EPOCHS\n\n# Funzione di loss\ncriterion = torch.nn.BCEWithLogitsLoss() \n\n# Ottimizzatore\noptimizer_bert = torch.optim.AdamW(params = full_model.parameters(), lr = learning_rate)\n\n# Scheduler\nscheduler_bert = transformers.get_linear_schedule_with_warmup(optimizer = optimizer_bert,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:13:07.934425Z","iopub.execute_input":"2025-01-22T16:13:07.934704Z","iopub.status.idle":"2025-01-22T16:13:08.537229Z","shell.execute_reply.started":"2025-01-22T16:13:07.934684Z","shell.execute_reply":"2025-01-22T16:13:08.536332Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"history_bert, total_time_bert = train_and_evaluate_model(\n    full_model, \"full_model\", train_loader, val_loader, criterion, optimizer_bert, scheduler_bert, device, epochs=4\n)\nprint(f\"\\nBERT Training Time: {total_time_bert:.2f} seconds, {total_time_bert/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:13:18.606468Z","iopub.execute_input":"2025-01-22T16:13:18.607023Z","iopub.status.idle":"2025-01-22T16:21:26.458060Z","shell.execute_reply.started":"2025-01-22T16:13:18.606989Z","shell.execute_reply":"2025-01-22T16:21:26.457070Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/4\n----------\nTrain loss: 0.3386, Train accuracy: 0.8509\nValidation loss: 0.2542, Validation accuracy: 0.9030\n\nEpoch 2/4\n----------\nTrain loss: 0.1497, Train accuracy: 0.9514\nValidation loss: 0.3192, Validation accuracy: 0.9090\n\nEpoch 3/4\n----------\nTrain loss: 0.0731, Train accuracy: 0.9796\nValidation loss: 0.3948, Validation accuracy: 0.9080\n\nEpoch 4/4\n----------\nTrain loss: 0.0394, Train accuracy: 0.9903\nValidation loss: 0.4160, Validation accuracy: 0.9130\n\nBERT Training Time: 487.85 seconds, 8.13 minutes.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 5e-4\nEPOCHS = 4\nBATCH_SIZE = 16\n\n# Creo i DataLoader\ntrain_loader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(train_loader) * EPOCHS\n\n# Funzione di loss\ncriterion = torch.nn.BCEWithLogitsLoss() \n\n# Ottimizzatore\noptimizer_lora = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n\n# Scheduler\nscheduler_lora = transformers.get_linear_schedule_with_warmup(optimizer = optimizer_lora,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:21:26.459046Z","iopub.execute_input":"2025-01-22T16:21:26.459363Z","iopub.status.idle":"2025-01-22T16:21:26.473612Z","shell.execute_reply.started":"2025-01-22T16:21:26.459334Z","shell.execute_reply":"2025-01-22T16:21:26.471404Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"history_lora, total_time_lora = train_and_evaluate_model(\n    lora_model,\"lora_model\", train_loader, val_loader, criterion, optimizer_lora, scheduler_lora, device, epochs=4\n)\nprint(f\"BERT with LoRA Training Time: {total_time_lora:.2f} seconds, {total_time_lora/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:21:26.475364Z","iopub.execute_input":"2025-01-22T16:21:26.475668Z","iopub.status.idle":"2025-01-22T16:27:30.728765Z","shell.execute_reply.started":"2025-01-22T16:21:26.475634Z","shell.execute_reply":"2025-01-22T16:27:30.727886Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/4\n----------\nTrain loss: 0.4155, Train accuracy: 0.7987\nValidation loss: 0.3120, Validation accuracy: 0.8710\n\nEpoch 2/4\n----------\nTrain loss: 0.2785, Train accuracy: 0.8889\nValidation loss: 0.2840, Validation accuracy: 0.8820\n\nEpoch 3/4\n----------\nTrain loss: 0.2465, Train accuracy: 0.9006\nValidation loss: 0.2802, Validation accuracy: 0.8890\n\nEpoch 4/4\n----------\nTrain loss: 0.2284, Train accuracy: 0.9083\nValidation loss: 0.2834, Validation accuracy: 0.8930\nBERT with LoRA Training Time: 364.23 seconds, 6.07 minutes.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### 4. Valutazione dei modelli\nValuto i modello calcolando la loss sul test set, l'accuracy, l'F1-score e ROC AUC.","metadata":{}},{"cell_type":"code","source":"full_model.load_state_dict(torch.load(\"sa_full_model_best_model_state.bin\"))\n\ntest_loss, test_acc, test_f1, test_auc = eval_model(full_model, test_loader, criterion, device)\nprint(f\"Full Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}, ROC AUC: {test_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:27:30.729663Z","iopub.execute_input":"2025-01-22T16:27:30.729941Z","iopub.status.idle":"2025-01-22T16:27:34.815776Z","shell.execute_reply.started":"2025-01-22T16:27:30.729913Z","shell.execute_reply":"2025-01-22T16:27:34.814863Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-19-45788bfe313d>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  full_model.load_state_dict(torch.load(\"sa_full_model_best_model_state.bin\"))\n","output_type":"stream"},{"name":"stdout","text":"Full Fine-Tuning - Test loss: 0.3161, Accuracy: 0.9307, F1 score: 0.9305, ROC AUC: 0.9738\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"sa_lora_model_best_model_state.bin\"))\n\nlora_test_loss, lora_test_acc, lora_test_f1, lora_test_auc = eval_model(lora_model, test_loader, criterion, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {lora_test_loss:.4f}, Accuracy: {lora_test_acc:.4f}, F1 score: {lora_test_f1:.4f}, ROC AUC: {lora_test_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:27:34.816449Z","iopub.execute_input":"2025-01-22T16:27:34.816723Z","iopub.status.idle":"2025-01-22T16:27:38.977304Z","shell.execute_reply.started":"2025-01-22T16:27:34.816700Z","shell.execute_reply":"2025-01-22T16:27:38.976407Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-20-971fd67b048f>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_model.load_state_dict(torch.load(\"sa_lora_model_best_model_state.bin\"))\n","output_type":"stream"},{"name":"stdout","text":"LoRA Fine-Tuning - Test loss: 0.2291, Accuracy: 0.9141, F1 score: 0.9138, ROC AUC: 0.9701\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Natural Language Inference","metadata":{}},{"cell_type":"markdown","source":"Proseguo confrontando i due tipi di fine-tuning sul task di Natural Language Inference utilizzando il dataset MNLI.\n\nMNLI contiene esempi che consistono in una coppia di frasi (premessa e ipotesi). Le etichette indicano la relazione tra queste due frasi.  \nLe possibili etichette sono:\n- **Entailment**: l'ipotesi è implicata dalla premessa.\n- **Contradiction**: l'ipotesi contraddice la premessa.\n- **Neutral**: nessuna relazione specifica.\n  \nUtilizzo sempre la libreria `datasets` di Hugging Face per caricare il dataset e dividerlo in training set, validation set e test set.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n\n# Carico il datast\ndataset = load_dataset('glue', 'mnli')\nprint(dataset)\n\n\n# Divido i dati in training set, validation set e test set\ndata = dataset['train'].shuffle(seed=42)\n\ntemp_premises, test_premises, temp_hypotheses, test_hypotheses, temp_labels, test_labels = train_test_split(data['premise'], \n                                                  data['hypothesis'],                \n                                                  data['label'], \n                                                  test_size=1024, \n                                                  random_state=42,\n                                                  stratify=data['label'])\n\ntrain_premises, val_premises, train_hypotheses, val_hypotheses, train_labels, val_labels = train_test_split(data['premise'], \n                                                  data['hypothesis'],\n                                                  data['label'],\n                                                  train_size=10000,\n                                                  test_size=1000, \n                                                  random_state=42,\n                                                  stratify=data['label'])\n\nprint(\"Dimensioni dei set:\")\nprint(f\"Train: {len(train_premises)}\")\nprint(f\"Validation: {len(val_premises)}\")\nprint(f\"Test: {len(test_premises)}\")\n\n# Verifica distribuzione delle etichette\nprint(\"\\nDistribuzione delle etichette:\")\nprint(f\"Train: {Counter(train_labels)}\")\nprint(f\"Validation: {Counter(val_labels)}\")\nprint(f\"Test: {Counter(test_labels)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:30:56.806174Z","iopub.execute_input":"2025-01-22T16:30:56.806528Z","iopub.status.idle":"2025-01-22T16:31:22.489239Z","shell.execute_reply.started":"2025-01-22T16:30:56.806498Z","shell.execute_reply":"2025-01-22T16:31:22.488541Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed086a31a41747719d4fd9025c6a875b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/52.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a55aceea7a04e25a0f988535fbfd8e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)alidation_matched-00000-of-00001.parquet:   0%|          | 0.00/1.21M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3489e355368746dc8e680ef26767f65b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)dation_mismatched-00000-of-00001.parquet:   0%|          | 0.00/1.25M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d761564d96443f19b15ec0371de3aac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test_matched-00000-of-00001.parquet:   0%|          | 0.00/1.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17c0adfc8070474d941127f26e5eb98b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test_mismatched-00000-of-00001.parquet:   0%|          | 0.00/1.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00ca64580f8f40a8ac38163b67f91a98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7698da5f2604518ad27aa9b979b30a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fcb47263580412ea6e7c4e3a3f7c71c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d350c176c02c4a83970927af194e63a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_matched split:   0%|          | 0/9796 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ef7e140f4274f938e0dca3fa7e3b9d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_mismatched split:   0%|          | 0/9847 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0690165f6b0e45c99f8edcd24ed55dd1"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 392702\n    })\n    validation_matched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9815\n    })\n    validation_mismatched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9832\n    })\n    test_matched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9796\n    })\n    test_mismatched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9847\n    })\n})\nDimensioni dei set:\nTrain: 10000\nValidation: 1000\nTest: 1024\n\nDistribuzione delle etichette:\nTrain: Counter({2: 3334, 1: 3333, 0: 3333})\nValidation: Counter({2: 334, 1: 333, 0: 333})\nTest: Counter({2: 342, 0: 341, 1: 341})\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"Creo una classe Dataset personalizzata in cui viene effettuata la tokenizzaione delle recensioni e la conversione dei dati in tensori.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass MNLIDataset(Dataset):\n\n    def __init__(self, premises, hypotheses , labels, tokenizer, max_len):\n        self.premises = premises\n        self.hypotheses = hypotheses\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.premises)\n    \n    def __getitem__(self,index):\n        premise = self.premises[index]\n        hyphotesis = self.hypotheses[index]\n        label = self.labels[index]\n        \n        encoding = self.tokenizer.encode_plus(\n            premise,\n            hyphotesis,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors='pt')\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding[\"token_type_ids\"].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:31:22.490503Z","iopub.execute_input":"2025-01-22T16:31:22.490875Z","iopub.status.idle":"2025-01-22T16:31:22.497627Z","shell.execute_reply.started":"2025-01-22T16:31:22.490808Z","shell.execute_reply":"2025-01-22T16:31:22.496673Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"Inizializzo il Tokenizer BERT per tokenizzare le frasi e creo i dataset personalizzati.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 256 \n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\ntraining_data = MNLIDataset(premises = train_premises,\n                            hypotheses = train_hypotheses,\n                            labels = train_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)\n\nvalidation_data = MNLIDataset(premises = train_premises,\n                            hypotheses = train_hypotheses,\n                            labels = train_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)\n\ntest_data = MNLIDataset(premises = train_premises,\n                            hypotheses = train_hypotheses,\n                            labels = train_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:31:22.498933Z","iopub.execute_input":"2025-01-22T16:31:22.499208Z","iopub.status.idle":"2025-01-22T16:31:22.718592Z","shell.execute_reply.started":"2025-01-22T16:31:22.499188Z","shell.execute_reply":"2025-01-22T16:31:22.717633Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### 2. Costruzione dei modelli","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel\n\nclass NLIBERTClassifier(nn.Module):\n    \n    def __init__(self, num_classes: int = 3, lora: bool = False, r: int = 16):\n        super(NLIBERTClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(p=0.3)\n        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n        if lora:\n            print(\"Adding LoRA to BERT\")\n            lora_utils.add_lora_to_bert(self.bert, r=r)\n            lora_utils.mark_only_lora_as_trainable(self.bert)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_bert = self.bert(\n            input_ids, \n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        output_dropout = self.dropout(output_bert.pooler_output)\n        logits = self.linear(output_dropout)\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:31:22.719658Z","iopub.execute_input":"2025-01-22T16:31:22.719911Z","iopub.status.idle":"2025-01-22T16:31:22.725303Z","shell.execute_reply.started":"2025-01-22T16:31:22.719891Z","shell.execute_reply":"2025-01-22T16:31:22.724425Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:31:34.273867Z","iopub.execute_input":"2025-01-22T16:31:34.274155Z","iopub.status.idle":"2025-01-22T16:31:34.277924Z","shell.execute_reply.started":"2025-01-22T16:31:34.274133Z","shell.execute_reply":"2025-01-22T16:31:34.277083Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# MODELLO STANDARD\nfull_model = NLIBERTClassifier(lora=False)\nfull_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:31:36.133741Z","iopub.execute_input":"2025-01-22T16:31:36.134042Z","iopub.status.idle":"2025-01-22T16:31:36.754059Z","shell.execute_reply.started":"2025-01-22T16:31:36.134018Z","shell.execute_reply":"2025-01-22T16:31:36.753127Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"NLIBERTClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# MODELLO CON LORA\nlora_model = NLIBERTClassifier(lora=True, r=32)\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:31:37.450901Z","iopub.execute_input":"2025-01-22T16:31:37.451232Z","iopub.status.idle":"2025-01-22T16:31:38.225662Z","shell.execute_reply.started":"2025-01-22T16:31:37.451206Z","shell.execute_reply":"2025-01-22T16:31:38.224825Z"}},"outputs":[{"name":"stdout","text":"Adding LoRA to BERT\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"NLIBERTClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"### 3. Addestramento dei modelli","metadata":{}},{"cell_type":"markdown","source":"Definisco una serie di funzioni per l'addestramento e la valutazione di un modello.","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\n\n# Funzione di training e valutazione\ndef train_and_evaluate_model(model, model_name, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=4):\n    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n    best_accuracy = 0\n    \n    start_time = time.time()\n\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n        print('-' * 10)\n\n        # Training\n        train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, scheduler, device)\n        print(f\"Train loss: {train_loss:.4f}, Train accuracy: {train_acc:.4f}\")\n\n        # Valutazione\n        val_loss, val_acc, val_f1, val_auc = eval_model(model, val_loader, criterion, device)\n        print(f\"Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}\")\n\n        # Salvataggio del modello migliore\n        if val_acc > best_accuracy:\n            torch.save(model.state_dict(),  f\"nli_{model_name}_best_model_state.bin\")\n            best_accuracy = val_acc\n\n        # Salva le metriche\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n    \n    end_time = time.time()\n    total_training_time = end_time - start_time\n    \n    return history, total_training_time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:31:40.557307Z","iopub.execute_input":"2025-01-22T16:31:40.557614Z","iopub.status.idle":"2025-01-22T16:31:40.563982Z","shell.execute_reply.started":"2025-01-22T16:31:40.557591Z","shell.execute_reply":"2025-01-22T16:31:40.563155Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Funzione di training \ndef train_model(model, data_loader, criterion, optimizer, scheduler, device):\n\n    model = model.train()  # Imposto il modello in modalità di aggiornamento\n\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in data_loader:\n\n        # Sposto i dati sul device\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        # --- Forward pass ---\n        optimizer.zero_grad()\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids = token_type_ids\n        )\n\n        loss = criterion(outputs, labels)\n\n        # --- Backward pass ---\n        loss.backward()\n\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs, dim=1)  # Predizioni multiclasse\n\n        all_preds.extend(preds.detach().cpu().numpy())\n        all_labels.extend(labels.detach().cpu().numpy())\n\n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    return avg_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:31:43.287157Z","iopub.execute_input":"2025-01-22T16:31:43.287473Z","iopub.status.idle":"2025-01-22T16:31:43.294092Z","shell.execute_reply.started":"2025-01-22T16:31:43.287448Z","shell.execute_reply":"2025-01-22T16:31:43.293037Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Funzione di valutazione\ndef eval_model(model, data_loader, criterion, device):\n    model = model.eval()\n\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids = token_type_ids\n            )\n\n            loss = criterion(outputs, labels)\n\n            total_loss += loss.item()\n\n            probs = torch.softmax(outputs, dim=1)  # Probabilità multiclasse\n            preds = torch.argmax(probs, dim=1)  # Predizioni multiclasse\n\n            all_preds.extend(preds.detach().cpu().numpy())\n            all_labels.extend(labels.detach().cpu().numpy())\n            all_probs.extend(probs.detach().cpu().numpy())\n\n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n    auc = roc_auc_score(all_labels, all_probs, multi_class=\"ovr\")\n    \n    return avg_loss, accuracy, f1, auc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:31:48.392198Z","iopub.execute_input":"2025-01-22T16:31:48.392555Z","iopub.status.idle":"2025-01-22T16:31:48.399109Z","shell.execute_reply.started":"2025-01-22T16:31:48.392526Z","shell.execute_reply":"2025-01-22T16:31:48.398216Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"Definisco le configurazion principali per il training.","metadata":{"execution":{"iopub.status.busy":"2025-01-20T10:34:04.784079Z","iopub.execute_input":"2025-01-20T10:34:04.784394Z","iopub.status.idle":"2025-01-20T10:34:04.790478Z","shell.execute_reply.started":"2025-01-20T10:34:04.784371Z","shell.execute_reply":"2025-01-20T10:34:04.789142Z"}}},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 3e-5\nEPOCHS = 4\nBATCH_SIZE = 32\n\n# Creo i DataLoader\ntrain_loader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(train_loader) * EPOCHS\n\n# Funzione di loss\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Ottimizzatore\noptimizer_bert = torch.optim.AdamW(params = full_model.parameters(), lr = learning_rate)\n\n# Scheduler\nscheduler_bert = transformers.get_linear_schedule_with_warmup(optimizer = optimizer_bert,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:31:52.237106Z","iopub.execute_input":"2025-01-22T16:31:52.237451Z","iopub.status.idle":"2025-01-22T16:31:52.247277Z","shell.execute_reply.started":"2025-01-22T16:31:52.237423Z","shell.execute_reply":"2025-01-22T16:31:52.246416Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"history_bert, total_time_bert = train_and_evaluate_model(\n    full_model, \"full_model\", train_loader, val_loader, criterion, optimizer_bert, scheduler_bert, device, epochs=4\n)\nprint(f\"\\nBERT Training Time: {total_time_bert:.2f} seconds, {total_time_bert/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:31:54.783972Z","iopub.execute_input":"2025-01-22T16:31:54.784279Z","iopub.status.idle":"2025-01-22T16:53:53.623441Z","shell.execute_reply.started":"2025-01-22T16:31:54.784256Z","shell.execute_reply":"2025-01-22T16:53:53.622397Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/4\n----------\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.8532, Train accuracy: 0.5946\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.4901, Validation accuracy: 0.8167\n\nEpoch 2/4\n----------\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.5057, Train accuracy: 0.8080\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.2371, Validation accuracy: 0.9298\n\nEpoch 3/4\n----------\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.2730, Train accuracy: 0.9076\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.1121, Validation accuracy: 0.9701\n\nEpoch 4/4\n----------\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.1563, Train accuracy: 0.9523\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.0707, Validation accuracy: 0.9811\n\nBERT Training Time: 1318.83 seconds, 21.98 minutes.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 5e-4\nEPOCHS = 8\nBATCH_SIZE = 16\n\n# Creo i DataLoader\ntrain_loader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(train_loader) * EPOCHS\n\n# Funzione di loss\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Ottimizzatore\noptimizer_lora = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n\n# Scheduler\nscheduler_lora = transformers.get_cosine_schedule_with_warmup(optimizer = optimizer_lora,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:53:53.624718Z","iopub.execute_input":"2025-01-22T16:53:53.625052Z","iopub.status.idle":"2025-01-22T16:53:53.633307Z","shell.execute_reply.started":"2025-01-22T16:53:53.625010Z","shell.execute_reply":"2025-01-22T16:53:53.632414Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"history_lora, total_time_lora = train_and_evaluate_model(\n    lora_model,\"lora_model\", train_loader, val_loader, criterion, optimizer_lora, scheduler_lora, device, epochs=EPOCHS\n)\nprint(f\"BERT with LoRA Training Time: {total_time_lora:.2f} seconds, {total_time_lora/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:53:53.635806Z","iopub.execute_input":"2025-01-22T16:53:53.636205Z","iopub.status.idle":"2025-01-22T17:29:10.549137Z","shell.execute_reply.started":"2025-01-22T16:53:53.636164Z","shell.execute_reply":"2025-01-22T17:29:10.548159Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/8\n----------\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 1.0021, Train accuracy: 0.4758\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.7936, Validation accuracy: 0.6518\n\nEpoch 2/8\n----------\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.7844, Train accuracy: 0.6569\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.6511, Validation accuracy: 0.7334\n\nEpoch 3/8\n----------\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.6960, Train accuracy: 0.7108\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.5896, Validation accuracy: 0.7588\n\nEpoch 4/8\n----------\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.6497, Train accuracy: 0.7382\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.5537, Validation accuracy: 0.7803\n\nEpoch 5/8\n----------\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.6209, Train accuracy: 0.7505\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.5262, Validation accuracy: 0.7930\n\nEpoch 6/8\n----------\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.5908, Train accuracy: 0.7606\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.5074, Validation accuracy: 0.8007\n\nEpoch 7/8\n----------\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.5685, Train accuracy: 0.7728\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.5005, Validation accuracy: 0.8060\n\nEpoch 8/8\n----------\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.5693, Train accuracy: 0.7718\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.4991, Validation accuracy: 0.8058\nBERT with LoRA Training Time: 2116.90 seconds, 35.28 minutes.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"### 4. Valutazione dei modelli\nValuto i modello calcolando la loss sul test set, l'accuracy, l'F1-score e ROC AUC.","metadata":{}},{"cell_type":"code","source":"full_model.load_state_dict(torch.load(\"nli_full_model_best_model_state.bin\"))\n\ntest_loss, test_acc, test_f1, test_auc = eval_model(full_model, test_loader, criterion, device)\nprint(f\"Full Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}, ROC AUC: {test_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:29:10.550357Z","iopub.execute_input":"2025-01-22T17:29:10.550665Z","iopub.status.idle":"2025-01-22T17:30:28.826977Z","shell.execute_reply.started":"2025-01-22T17:29:10.550634Z","shell.execute_reply":"2025-01-22T17:30:28.826090Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-35-294bd2746b76>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  full_model.load_state_dict(torch.load(\"nli_full_model_best_model_state.bin\"))\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Full Fine-Tuning - Test loss: 0.0708, Accuracy: 0.9811, F1 score: 0.9811, ROC AUC: 0.9973\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"nli_lora_model_best_model_state.bin\"))\n\nlora_test_loss, lora_test_acc, lora_test_f1, lora_test_auc = eval_model(lora_model, test_loader, criterion, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {lora_test_loss:.4f}, Accuracy: {lora_test_acc:.4f}, F1 score: {lora_test_f1:.4f}, ROC AUC: {lora_test_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T17:30:28.827869Z","iopub.execute_input":"2025-01-22T17:30:28.828182Z","iopub.status.idle":"2025-01-22T17:31:47.110799Z","shell.execute_reply.started":"2025-01-22T17:30:28.828148Z","shell.execute_reply":"2025-01-22T17:31:47.109828Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-36-74b2e2c97fca>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_model.load_state_dict(torch.load(\"nli_lora_model_best_model_state.bin\"))\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"LoRA Fine-Tuning - Test loss: 0.5005, Accuracy: 0.8060, F1 score: 0.8057, ROC AUC: 0.9314\n","output_type":"stream"}],"execution_count":36}]}