{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715},{"sourceId":10161839,"sourceType":"datasetVersion","datasetId":6274971},{"sourceId":10410576,"sourceType":"datasetVersion","datasetId":6451663}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Contronto tra il fine-tuning completo e il fine-tuning con  LoRA","metadata":{}},{"cell_type":"markdown","source":"#### Configurazioni","metadata":{}},{"cell_type":"markdown","source":"Installazione della libreria `loralib` per implementare la Low-Rank Adaptation.","metadata":{}},{"cell_type":"code","source":"!pip install loralib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:26.716948Z","iopub.execute_input":"2025-01-19T20:35:26.717206Z","iopub.status.idle":"2025-01-19T20:35:31.742511Z","shell.execute_reply.started":"2025-01-19T20:35:26.717183Z","shell.execute_reply":"2025-01-19T20:35:31.741488Z"}},"outputs":[{"name":"stdout","text":"Collecting loralib\n  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\nDownloading loralib-0.1.2-py3-none-any.whl (10 kB)\nInstalling collected packages: loralib\nSuccessfully installed loralib-0.1.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Carico i file `lora_utilis.py` e `models.py`.","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/lora-utils/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:31.743345Z","iopub.execute_input":"2025-01-19T20:35:31.743582Z","iopub.status.idle":"2025-01-19T20:35:31.747763Z","shell.execute_reply.started":"2025-01-19T20:35:31.743562Z","shell.execute_reply":"2025-01-19T20:35:31.746780Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Importo i moduli necessari.","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers\nimport lora_utils, models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:31.748711Z","iopub.execute_input":"2025-01-19T20:35:31.749017Z","iopub.status.idle":"2025-01-19T20:35:35.848684Z","shell.execute_reply.started":"2025-01-19T20:35:31.748983Z","shell.execute_reply":"2025-01-19T20:35:35.847786Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Impostazione del seme casuale per la riproducibilità.","metadata":{}},{"cell_type":"code","source":"seed_value = 42\n\nos.environ['PYTHONHASHSEED'] = str(seed_value)\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\n\n# Imposto il seme casuale anche per i calcoli CUDA\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)  \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:35.850902Z","iopub.execute_input":"2025-01-19T20:35:35.851326Z","iopub.status.idle":"2025-01-19T20:35:35.914213Z","shell.execute_reply.started":"2025-01-19T20:35:35.851303Z","shell.execute_reply":"2025-01-19T20:35:35.913512Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Sentiment Analisys","metadata":{}},{"cell_type":"markdown","source":"### 1. Ottenimento dei dati e preprocessing\n\nConfronto il fine-tuning completo e quello basato su LoRA sul task di Sentiment Analysis utilizzando il dataset IMDB Reviews.  \n\nIl dataset IMDB è costituito da 50.000 recensioni di film, etichettate con **positive** se la recensione è positiva o **negative**, altrimenti.  \n\nUtilizzo la libreria `kagglehub` per scaricare il dataset da Kaggle.","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Scarico l'ultima versione del dataset\npath = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n\ndataset_path = path + \"/IMDB Dataset.csv\"\ndataset = pd.read_csv(dataset_path)\n\n# Stampa di verifica\nprint(dataset.loc[0:4])\nprint(dataset.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:35.915499Z","iopub.execute_input":"2025-01-19T20:35:35.915779Z","iopub.status.idle":"2025-01-19T20:35:37.382190Z","shell.execute_reply.started":"2025-01-19T20:35:35.915748Z","shell.execute_reply":"2025-01-19T20:35:37.381422Z"}},"outputs":[{"name":"stdout","text":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive\nreview       0\nsentiment    0\ndtype: int64\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Definisco una funzione per l'ottenimento dei dati e la loro divisione in training, validation e test set.","metadata":{}},{"cell_type":"code","source":"LABELS = {\"negative\": 0, \"positive\": 1}\nclasses = list(LABELS.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:37.382858Z","iopub.execute_input":"2025-01-19T20:35:37.383101Z","iopub.status.idle":"2025-01-19T20:35:37.387070Z","shell.execute_reply.started":"2025-01-19T20:35:37.383052Z","shell.execute_reply":"2025-01-19T20:35:37.385973Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def get_data(dataset_path, n_train=5000, n_val=500, n_test=512): \n\n    # Leggo il dataset\n    dataset = pd.read_csv(dataset_path)\n\n    # Converto le etichette in numeri\n    dataset['sentiment'] = dataset[\"sentiment\"].map(LABELS)\n\n    # Divido gli esempi in negativi e positivi\n    neg = dataset[ dataset['sentiment'] == LABELS['negative'] ]\n    pos = dataset[ dataset['sentiment'] == LABELS['positive'] ]\n\n    # Verifico che ci siano abbastanza esempi\n    if len(neg) < n_train + n_val + n_test or len(pos) < n_train + n_val + n_test:\n        raise ValueError(\"Non ci sono abbastanza esempi per le dimensioni del train, validation e test set specificate.\")\n    \n    # Creo una permutazione degli esempi negativi e positivi\n    neg = neg.sample(frac=1, random_state=42).reset_index(drop=True)\n    pos = pos.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    # Seleziono gli elementi da inserire nei set\n    neg_train, pos_train = neg[:n_train], pos[:n_train]\n    neg_val, pos_val = neg[n_train:n_train+n_val], pos[n_train:n_train+n_val]\n    neg_test, pos_test = neg[n_train+n_val:n_train+n_val+n_test], pos[n_train+n_val:n_train+n_val+n_test]\n\n    # Concateno gli esempi negativi e positivi per formare un solo insieme\n    train_data = pd.concat([neg_train, pos_train])\n    val_data = pd.concat([neg_val, pos_val])\n    test_data = pd.concat([neg_test, pos_test])\n\n    # Mescolo i dati negli insiemi\n    train_data = train_data.sample(frac=1, random_state=42).reset_index(drop=True)\n    val_data = val_data.sample(frac=1, random_state=42).reset_index(drop=True)\n    test_data = test_data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    # Ottengo le features e le labels\n    sentences_train, labels_train = train_data['review'] , train_data['sentiment']\n    sentences_val, labels_val = val_data['review'] , val_data['sentiment']\n    sentences_test, labels_test = test_data['review'] , test_data['sentiment']\n\n    return sentences_train, labels_train, sentences_val, labels_val, sentences_test, labels_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:37.387693Z","iopub.execute_input":"2025-01-19T20:35:37.387873Z","iopub.status.idle":"2025-01-19T20:35:37.406775Z","shell.execute_reply.started":"2025-01-19T20:35:37.387857Z","shell.execute_reply":"2025-01-19T20:35:37.406085Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Creo una classe Dataset personalizzata in cui viene effettuata la tokenizzaione delle recensioni e la conversione dei dati in tensori.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass IMDBDataset(Dataset):\n\n    def __init__(self, sentences, labels, tokenizer, max_len):\n        self.sentences = sentences\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    def __getitem__(self,index):\n        sentence = self.sentences[index]\n        label = self.labels[index]\n        \n        encoding = self.tokenizer.encode_plus(\n            sentence,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors='pt')\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding[\"token_type_ids\"].flatten(),\n            'labels': torch.tensor(label, dtype=torch.float)         # usa torch.tensor visto che label è uno scalare \n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:37.407392Z","iopub.execute_input":"2025-01-19T20:35:37.407573Z","iopub.status.idle":"2025-01-19T20:35:37.429670Z","shell.execute_reply.started":"2025-01-19T20:35:37.407556Z","shell.execute_reply":"2025-01-19T20:35:37.428998Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Ottengo i dati grezzi, li divido in training, validation e test set con la funzione `get_data()`. Inizializzo il Tokenizer BERT per tokenizzare le frasi e creo i dataset personalizzati.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 128\n\n# Ottieni i dati grezzi\nsentences_train, labels_train, sentences_val, labels_val, sentences_test, labels_test = get_data(dataset_path)\n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\ntraining_data = IMDBDataset(sentences = sentences_train,\n                           labels = labels_train,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nvalidation_data = IMDBDataset(sentences = sentences_val.values,\n                           labels = labels_val.values,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\ntest_data = IMDBDataset(sentences = sentences_test.values,\n                           labels = labels_test.values,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:37.430580Z","iopub.execute_input":"2025-01-19T20:35:37.430850Z","iopub.status.idle":"2025-01-19T20:35:39.230962Z","shell.execute_reply.started":"2025-01-19T20:35:37.430829Z","shell.execute_reply":"2025-01-19T20:35:39.230172Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf4907b7597d4316b74c8cac834b9b72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e06871af03e14423a542087a9053b98d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e97b1f0a62554656ac8e67621b2cb2a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2c7a3acf12e42d2955a6902d2c36c02"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### 2. Configurazione dei modelli","metadata":{}},{"cell_type":"markdown","source":"Definisco un classificatore basato su BERT. Utilizzo il modello BERT pre-addestrato e aggiungo un livello di pooling e un layer lineare con un solo neurone di output per la classificazione binaria. Se il parametro lora è attivo, integra LoRA nel modello.","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel\n\nclass BERTClassifier(nn.Module):\n    \n    def __init__(self, lora: bool = False, r: int = 16):\n        super(BERTClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.avg_pooling = nn.AdaptiveAvgPool1d(1) \n        self.linear = nn.Linear(self.bert.config.hidden_size, 1)\n\n        if lora:\n            print(\"Adding LoRA to BERT\")\n            lora_utils.add_lora_to_bert(self.bert, r=r)\n            lora_utils.mark_only_lora_as_trainable(self.bert)\n\n    \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_bert = self.bert(\n            input_ids, \n            attention_mask=attention_mask, \n            token_type_ids=token_type_ids\n        )\n        last_hidden_state = output_bert.last_hidden_state  \n        avg_pooled = self.avg_pooling(last_hidden_state.transpose(1, 2)).squeeze(-1)\n        logits = self.linear(avg_pooled)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:39.231712Z","iopub.execute_input":"2025-01-19T20:35:39.231916Z","iopub.status.idle":"2025-01-19T20:35:40.313976Z","shell.execute_reply.started":"2025-01-19T20:35:39.231898Z","shell.execute_reply":"2025-01-19T20:35:40.312958Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:40.314660Z","iopub.execute_input":"2025-01-19T20:35:40.315040Z","iopub.status.idle":"2025-01-19T20:35:40.319012Z","shell.execute_reply.started":"2025-01-19T20:35:40.315020Z","shell.execute_reply":"2025-01-19T20:35:40.317978Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# MODELLO BERT \nfull_model = BERTClassifier(lora=False)\nfull_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:40.319884Z","iopub.execute_input":"2025-01-19T20:35:40.320110Z","iopub.status.idle":"2025-01-19T20:35:43.607896Z","shell.execute_reply.started":"2025-01-19T20:35:40.320079Z","shell.execute_reply":"2025-01-19T20:35:43.606922Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23322202c0024c50a4334207c267c913"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"BERTClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (avg_pooling): AdaptiveAvgPool1d(output_size=1)\n  (linear): Linear(in_features=768, out_features=1, bias=True)\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# MODELLO CON LORA\nlora_model = BERTClassifier(lora=True, r=16)\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:43.611519Z","iopub.execute_input":"2025-01-19T20:35:43.611779Z","iopub.status.idle":"2025-01-19T20:35:44.274010Z","shell.execute_reply.started":"2025-01-19T20:35:43.611758Z","shell.execute_reply":"2025-01-19T20:35:44.273107Z"}},"outputs":[{"name":"stdout","text":"Adding LoRA to BERT\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"BERTClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (avg_pooling): AdaptiveAvgPool1d(output_size=1)\n  (linear): Linear(in_features=768, out_features=1, bias=True)\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"### 3. Addestramento dei modelli","metadata":{}},{"cell_type":"markdown","source":"Definisco una serie di funzioni per l'addestramento e la valutazione di un modello.","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\n\n# Funzione di training e valutazione\ndef train_and_evaluate_model(model, model_name, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=4):\n    \n    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n    best_accuracy = 0\n\n    start_time = time.time()\n\n    for epoch in range(epochs):\n    \n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n        print('-' * 10)\n\n        # Training\n        train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, scheduler, device)\n        print(f\"Train loss: {train_loss:.4f}, Train accuracy: {train_acc:.4f}\")\n \n        # Valutazione sul validation set\n        val_loss, val_acc, val_f1, val_auc = eval_model(model, val_loader, criterion, device)\n        print(f\"Validation loss: {val_loss:.4f}, Validation accuracy: {val_acc:.4f}\")\n \n        # Salvataggio del modello migliore\n        if val_acc > best_accuracy:\n            torch.save(model.state_dict(), f\"imbd_best_{model_name}_state.bin\")\n            best_accuracy = val_acc\n\n        # Salvo le metriche\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n    \n    end_time = time.time()\n    total_training_time = end_time - start_time\n    \n    return history, total_training_time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:44.275860Z","iopub.execute_input":"2025-01-19T20:35:44.276097Z","iopub.status.idle":"2025-01-19T20:35:44.773049Z","shell.execute_reply.started":"2025-01-19T20:35:44.276078Z","shell.execute_reply":"2025-01-19T20:35:44.772202Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Funzione di training \ndef train_model(model, data_loader, criterion, optimizer, scheduler, device):\n  \n    model = model.train() # imposto il modello in modalità di aggiornamento\n    \n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in data_loader:\n        \n        # Sposto i dati sul device\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch[\"labels\"].unsqueeze(1).to(device)\n\n        #  --- Forward pass ---\n        \n        # Azzero il gradiente\n        optimizer.zero_grad()\n \n        # Effettuo la previsione per il batch corrente\n        outputs = model(\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n            token_type_ids = token_type_ids\n        )# output contiene i valori grezzi non normalizzati prodotti dal modello per ogni classe.\n\n        #outputs = outputs.logits  # Estrai i logits\n\n        # Calcolo la loss\n        loss = criterion(outputs, labels)\n\n\n        # --- Backward pass ---\n        \n        # Calcolo i gradienti della loss\n        loss.backward()\n\n        # Effettuo il clipping dei gradienti\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Aggiorno i pesi\n        optimizer.step()\n\n        # Aggiorno il learning rate\n        scheduler.step()\n\n        # Salvo la loss, le previsioni e le etichette\n        total_loss += loss.item()\n        \n        preds = (torch.sigmoid(outputs) > 0.5).long()    # trasformo i dati grezzi in etichette binarie\n        \n        all_preds.extend(preds.detach().cpu().numpy())\n        all_labels.extend(labels.detach().cpu().numpy())\n\n    # Calcolo la loss e le metriche\n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n   \n    return avg_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:44.774129Z","iopub.execute_input":"2025-01-19T20:35:44.774651Z","iopub.status.idle":"2025-01-19T20:35:44.781429Z","shell.execute_reply.started":"2025-01-19T20:35:44.774619Z","shell.execute_reply":"2025-01-19T20:35:44.780494Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Funzione di valutazione\ndef eval_model(model, data_loader, criterion, device):\n    \n    model = model.eval()    # imposto il modello in modalità valutavione\n\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            \n            # Sposto i dati sul device\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['labels'].unsqueeze(1).to(device)\n\n            # Effettuo la previsione per il batch corrente\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids = token_type_ids\n            )\n            #outputs = outputs.logits  # Estrai i logits\n            \n            # Calcolo la loss\n            loss = criterion(outputs, labels)\n\n            # Aggiorno la loss, salvo le previsioni e le etichette\n            total_loss += loss.item()\n    \n            probs = torch.sigmoid(outputs)  # Calcolo le probabilità\n\n            preds = (probs > 0.5).long()  # Trasformo in etichette binarie\n            \n            all_preds.extend(preds.detach().cpu().numpy())\n            all_labels.extend(labels.detach().cpu().numpy())\n            all_probs.extend(probs.detach().cpu().numpy())\n\n        # Calcolo loss e metriche\n        avg_loss = total_loss / len(data_loader)\n        accuracy = accuracy_score(all_labels, all_preds)\n        f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n        roc_auc = roc_auc_score(all_labels, all_probs, average='weighted', multi_class='ovr')\n    \n    return avg_loss, accuracy, f1, roc_auc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:44.782473Z","iopub.execute_input":"2025-01-19T20:35:44.782794Z","iopub.status.idle":"2025-01-19T20:35:44.803157Z","shell.execute_reply.started":"2025-01-19T20:35:44.782765Z","shell.execute_reply":"2025-01-19T20:35:44.802358Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"Definisco le configurazion principali per il training.","metadata":{}},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 3e-5\nEPOCHS = 4\nBATCH_SIZE = 32\n\n\n# Creo i DataLoader\ntrain_loader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(train_loader) * EPOCHS\n\n# Funzione di loss\ncriterion = torch.nn.BCEWithLogitsLoss() # Applica automaticamente la sigmoide\n\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(params = full_model.parameters(), lr = learning_rate)\n\n# Scheduler\nscheduler = transformers.get_linear_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:44.804047Z","iopub.execute_input":"2025-01-19T20:35:44.804357Z","iopub.status.idle":"2025-01-19T20:35:45.450635Z","shell.execute_reply.started":"2025-01-19T20:35:44.804325Z","shell.execute_reply":"2025-01-19T20:35:45.449954Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"history_bert, total_time_bert = train_and_evaluate_model(\n    full_model, \"full_model\", train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=4\n)\nprint(f\"\\nBERT Training Time: {total_time_bert:.2f} seconds, {total_time_bert/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:35:45.451553Z","iopub.execute_input":"2025-01-19T20:35:45.452040Z","iopub.status.idle":"2025-01-19T20:46:55.909913Z","shell.execute_reply.started":"2025-01-19T20:35:45.452010Z","shell.execute_reply":"2025-01-19T20:46:55.909160Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/4\n----------\nTrain loss: 0.3672, Train accuracy: 0.8342\nValidation loss: 0.3072, Validation accuracy: 0.8660\n\nEpoch 2/4\n----------\nTrain loss: 0.1908, Train accuracy: 0.9284\nValidation loss: 0.3086, Validation accuracy: 0.8900\n\nEpoch 3/4\n----------\nTrain loss: 0.0862, Train accuracy: 0.9722\nValidation loss: 0.4209, Validation accuracy: 0.8870\n\nEpoch 4/4\n----------\nTrain loss: 0.0362, Train accuracy: 0.9902\nValidation loss: 0.5490, Validation accuracy: 0.8820\n\nBERT Training Time: 670.45 seconds, 11.17 minutes.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 5e-4\nEPOCHS = 4\nBATCH_SIZE = 16\n\n\n# Creo i DataLoader\ntrain_loader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(validation_data, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(train_loader) * EPOCHS\n\n# Funzione di loss\ncriterion = torch.nn.BCEWithLogitsLoss() # Applica automaticamente la sigmoide\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n# Scheduler\nscheduler = transformers.get_linear_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:46:55.910761Z","iopub.execute_input":"2025-01-19T20:46:55.910978Z","iopub.status.idle":"2025-01-19T20:46:55.918189Z","shell.execute_reply.started":"2025-01-19T20:46:55.910956Z","shell.execute_reply":"2025-01-19T20:46:55.917536Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"history_lora, total_time_lora = train_and_evaluate_model(\n    lora_model,\"lora_model\", train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=4\n)\nprint(f\"BERT with LoRA Training Time: {total_time_lora:.2f} seconds, {total_time_lora/60:.2f} minutes.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T20:48:40.339660Z","iopub.execute_input":"2025-01-19T20:48:40.340010Z","iopub.status.idle":"2025-01-19T20:57:50.460854Z","shell.execute_reply.started":"2025-01-19T20:48:40.339988Z","shell.execute_reply":"2025-01-19T20:57:50.460040Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/4\n----------\nTrain loss: 0.4022, Train accuracy: 0.8144\nValidation loss: 0.3351, Validation accuracy: 0.8580\n\nEpoch 2/4\n----------\nTrain loss: 0.3150, Train accuracy: 0.8626\nValidation loss: 0.3248, Validation accuracy: 0.8580\n\nEpoch 3/4\n----------\nTrain loss: 0.2919, Train accuracy: 0.8774\nValidation loss: 0.3186, Validation accuracy: 0.8660\n\nEpoch 4/4\n----------\nTrain loss: 0.2771, Train accuracy: 0.8855\nValidation loss: 0.3171, Validation accuracy: 0.8680\nBERT with LoRA Training Time: 550.12 seconds, 9.17 minutes.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### 4. Valutazione dei modelli\nValuto i modello calcolando la loss sul test set, l'accuracy, l'F1-score e ROC AUC.","metadata":{}},{"cell_type":"code","source":"full_model.load_state_dict(torch.load(\"imbd_best_full_model_state.bin\"))\n\ntest_loss, test_acc, test_f1, test_auc = eval_model(full_model, test_loader, criterion, device)\nprint(f\"Full Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}, ROC AUC: {test_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:13:38.397247Z","iopub.execute_input":"2025-01-19T21:13:38.397553Z","iopub.status.idle":"2025-01-19T21:13:46.747986Z","shell.execute_reply.started":"2025-01-19T21:13:38.397527Z","shell.execute_reply":"2025-01-19T21:13:46.747124Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-24-036bcb6f0380>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  full_model.load_state_dict(torch.load(\"imbd_best_full_model_state.bin\"))\n","output_type":"stream"},{"name":"stdout","text":"Full Fine-Tuning - Test loss: 0.3159, Accuracy: 0.8857, F1 score: 0.8856, ROC AUC: 0.9537\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"imbd_best_lora_model_state.bin\"))\n\nlora_test_loss, lora_test_acc, lora_test_f1, lora_test_auc = eval_model(lora_model, test_loader, criterion, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {lora_test_loss:.4f}, Accuracy: {lora_test_acc:.4f}, F1 score: {lora_test_f1:.4f}, ROC AUC: {lora_test_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:13:48.906905Z","iopub.execute_input":"2025-01-19T21:13:48.907257Z","iopub.status.idle":"2025-01-19T21:13:57.380002Z","shell.execute_reply.started":"2025-01-19T21:13:48.907228Z","shell.execute_reply":"2025-01-19T21:13:57.379269Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-25-10f68d27c2db>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_model.load_state_dict(torch.load(\"imbd_best_lora_model_state.bin\"))\n","output_type":"stream"},{"name":"stdout","text":"LoRA Fine-Tuning - Test loss: 0.3004, Accuracy: 0.8730, F1 score: 0.8730, ROC AUC: 0.9469\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## Toxicity Detection","metadata":{}},{"cell_type":"markdown","source":"### 1. Ottenimento dei dati e preprocessing  \n\nConfronto il fine-tuning completo e quello basato su LoRA sul task di classificazione multi-label utilizzando il dataset **Toxic Comment Classification**.  \n\nIl dataset contiene commenti testuali etichettati con sei classi: **toxic**, **severe_toxic**, **obscene**, **threat**, **insult**, e **identity_hate**.","metadata":{}},{"cell_type":"markdown","source":"Definisco una funzione per l'ottenimento dei dati e la loro divisione in training, validation e test set.","metadata":{}},{"cell_type":"code","source":"classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nN_CLASSES = 6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:14:04.455615Z","iopub.execute_input":"2025-01-19T21:14:04.455918Z","iopub.status.idle":"2025-01-19T21:14:04.459773Z","shell.execute_reply.started":"2025-01-19T21:14:04.455894Z","shell.execute_reply":"2025-01-19T21:14:04.458855Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def get_data(dataset_path, n_train=5000, n_val=500, n_test=1024): \n\n    # Leggo il dataset\n    dataset = pd.read_csv(dataset_path)\n\n    # Effettuo un mescolamento casuale dei dati\n    dataset = dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    # Estraggo il testo e le etichette dal dataset\n    sentences = dataset[\"comment_text\"].fillna(\"null\").str.lower()\n    labels = dataset[classes].values.astype(np.float32)\n    \n    # Seleziono gli elementi da inserire nei set\n    train_sentences, train_labels = sentences[:n_train], labels[:n_train]\n    val_sentences, val_labels = sentences[n_train:n_train+n_val].reset_index(drop=True), labels[n_train:n_train+n_val]\n    test_sentences,test_labels = sentences[n_train+n_val:n_train+n_val+n_test].reset_index(drop=True), labels[n_train+n_val:n_train+n_val+n_test]\n\n    return train_sentences, train_labels, val_sentences, val_labels, test_sentences,test_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:14:05.485604Z","iopub.execute_input":"2025-01-19T21:14:05.485934Z","iopub.status.idle":"2025-01-19T21:14:05.491504Z","shell.execute_reply.started":"2025-01-19T21:14:05.485906Z","shell.execute_reply":"2025-01-19T21:14:05.490604Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"Creo una classe Dataset personalizzata in cui viene effettuata la tokenizzaione delle recensioni e la conversione dei dati in tensori.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass ToxicityDataset(Dataset):\n\n    def __init__(self, sentences, labels, tokenizer, max_len):\n        self.sentences = sentences\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, index):\n        sentence = self.sentences[index]\n        label = self.labels[index]\n        \n        encoding = self.tokenizer.encode_plus(\n            sentence,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            return_token_type_ids=True,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding[\"token_type_ids\"].flatten(),\n            'labels': torch.FloatTensor(label)\n        }     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:14:07.983184Z","iopub.execute_input":"2025-01-19T21:14:07.983508Z","iopub.status.idle":"2025-01-19T21:14:07.989291Z","shell.execute_reply.started":"2025-01-19T21:14:07.983483Z","shell.execute_reply":"2025-01-19T21:14:07.988513Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"Ottengo i dati grezzi, li divido in training, validation e test set con la funzione `get_data()`. Inizializzo il Tokenizer BERT per tokenizzare le frasi e creo i dataset personalizzati.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 128\n\n# Ottengo i dati divisi in training set, validation set e test set\ndataset_path = \"/kaggle/input/toxisity-detection-dataset/train.csv\"\n\ntrain_sentences, train_labels, val_sentences, val_labels, test_sentences,test_labels = get_data(dataset_path)\n\n\n# Inizializzo il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Ottengo i dataset\ntrain_dataset = ToxicityDataset(sentences = train_sentences,\n                                labels = train_labels, \n                                tokenizer = tokenizer, \n                                max_len = MAX_SEQ_LEN)\n\nvalidation_dataset = ToxicityDataset(sentences = val_sentences,\n                                labels = val_labels, \n                                tokenizer = tokenizer, \n                                max_len = MAX_SEQ_LEN)\n\ntest_dataset = ToxicityDataset(sentences = test_sentences,\n                                labels = test_labels, \n                                tokenizer = tokenizer, \n                                max_len = MAX_SEQ_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:14:10.696429Z","iopub.execute_input":"2025-01-19T21:14:10.696759Z","iopub.status.idle":"2025-01-19T21:14:12.945350Z","shell.execute_reply.started":"2025-01-19T21:14:10.696734Z","shell.execute_reply":"2025-01-19T21:14:12.944372Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"Controllo che il caricamento dei dati sia avvenuto correttamente.","metadata":{}},{"cell_type":"markdown","source":"### 2. Configurazione dei modelli \n\nDefinisco un classificatore basato su BERT per il task multi-label. Utilizzo il modello BERT pre-addestrato, seguito da un livello di dropout e un layer lineare con **N_CLASSES** neuroni di output. Se attivato, il modello integra LoRA per una fine-tuning efficiente.","metadata":{}},{"cell_type":"code","source":"from torch import nn\nfrom transformers import BertModel\n\nclass BERTClassifierMultilabel(nn.Module):\n\n    def __init__(self, lora: bool = False, r: int = 16):\n        super(BERTClassifierMultilabel, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = torch.nn.Dropout(p=0.3)\n        self.linear = torch.nn.Linear(self.bert.config.hidden_size, N_CLASSES)\n\n        if lora:\n            print(\"Adding LoRA to BERT\")\n            lora_utils.add_lora_to_bert(self.bert, r=r)\n            lora_utils.mark_only_lora_as_trainable(self.bert)\n\n\n    \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_bert = self.bert(\n            input_ids, \n            attention_mask=attention_mask, \n            token_type_ids=token_type_ids\n        )\n        output_dropout = self.dropout(output_bert.pooler_output)\n        output = self.linear(output_dropout)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:14:14.847362Z","iopub.execute_input":"2025-01-19T21:14:14.847650Z","iopub.status.idle":"2025-01-19T21:14:14.853522Z","shell.execute_reply.started":"2025-01-19T21:14:14.847629Z","shell.execute_reply":"2025-01-19T21:14:14.852608Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:14:17.525187Z","iopub.execute_input":"2025-01-19T21:14:17.525537Z","iopub.status.idle":"2025-01-19T21:14:17.529667Z","shell.execute_reply.started":"2025-01-19T21:14:17.525508Z","shell.execute_reply":"2025-01-19T21:14:17.528651Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# MODELLO STANDARD\nfull_model = BERTClassifierMultilabel(lora=False)\nfull_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:14:19.975943Z","iopub.execute_input":"2025-01-19T21:14:19.976299Z","iopub.status.idle":"2025-01-19T21:14:20.286485Z","shell.execute_reply.started":"2025-01-19T21:14:19.976270Z","shell.execute_reply":"2025-01-19T21:14:20.285580Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"BERTClassifierMultilabel(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=768, out_features=6, bias=True)\n)"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"# MODELLO CON LORA\nlora_model = BERTClassifierMultilabel(lora=True, r=16)\nlora_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:14:24.566113Z","iopub.execute_input":"2025-01-19T21:14:24.566412Z","iopub.status.idle":"2025-01-19T21:14:25.172323Z","shell.execute_reply.started":"2025-01-19T21:14:24.566388Z","shell.execute_reply":"2025-01-19T21:14:25.171488Z"}},"outputs":[{"name":"stdout","text":"Adding LoRA to BERT\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"BERTClassifierMultilabel(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=768, out_features=6, bias=True)\n)"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"### 3. Addestramento dei modelli","metadata":{}},{"cell_type":"markdown","source":"Definisco una serie di funzioni per l'addestramento e la valutazione di un modello.","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\n# Funzione di training e valutazione sul validation set\n\ndef train_and_evaluate_model(model, model_name, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=4):\n\n    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n    best_accuracy = 0\n\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        \n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n        print('-' * 30)\n\n        # Training\n        train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, scheduler, device)\n        print(f\"Train loss: {train_loss:.3f}, Train accuracy: {train_acc:.4f}\")\n\n        # Valutazione\n        val_loss, val_acc, val_f1, val_auc = eval_model(model, val_loader, criterion, device)\n        print(f\"Validation loss: {val_loss:.3f}, Validation accuracy: {val_acc:.3f}\")\n\n        # Salvataggio del modello migliore\n        if val_acc > best_accuracy:\n            torch.save(model.state_dict(),  f\"toxicity_best_{model_name}_state.bin\")\n            best_accuracy = val_acc\n\n        # Salvaggio delle metriche\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n    \n    end_time = time.time()\n    total_training_time = end_time - start_time\n    \n    return history, total_training_time\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:14:36.897032Z","iopub.execute_input":"2025-01-19T21:14:36.897395Z","iopub.status.idle":"2025-01-19T21:14:36.904781Z","shell.execute_reply.started":"2025-01-19T21:14:36.897368Z","shell.execute_reply":"2025-01-19T21:14:36.903772Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def train_model(model, data_loader, criterion, optimizer, scheduler, device):\n    \n    model.train()  # imposto il modello in modalità di aggiornamento\n\n    total_loss = 0 \n    all_preds = []\n    all_labels = []\n\n    for batch in data_loader:\n        \n        # Sposto i dati sul device\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # --- Forward pass ---\n        \n        # Azzero il gradiente\n        optimizer.zero_grad()\n        \n        # Effettuo la previsione per il batch corrente\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids = token_type_ids\n        )\n        # output contiene i valori grezzi non normalizzati prodotti dal modello per ogni classe.\n        \n        # Calcolo la loss\n        loss = criterion(outputs, labels)\n\n        \n        # --- Backward pass ---\n\n        # Calcolo i gradienti della loss\n        loss.backward()\n\n        # Effettuo il clipping dei gradienti\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Aggiorno i pesi\n        optimizer.step()\n\n        # Aggiorno il learning rate\n        scheduler.step()\n\n        \n        # Salvo la loss, le previsioni e le etichette\n        total_loss += loss.item()\n\n        preds = (torch.sigmoid(outputs) > 0.5).long() #  trasformo i dati grezzi in etichette binarie\n\n        all_preds.extend(preds.detach().cpu().numpy()) # disconnetto il tensore dalla computazione del gradiente, lo sposto sulla cpu, lo trasformo in array numpy e aggiungo il risultato a all_preds\n        all_labels.extend(labels.detach().cpu().numpy())\n\n\n    # Calcolo la loss e le metriche\n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n    return avg_loss, accuracy\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:14:43.088896Z","iopub.execute_input":"2025-01-19T21:14:43.089236Z","iopub.status.idle":"2025-01-19T21:14:43.095930Z","shell.execute_reply.started":"2025-01-19T21:14:43.089206Z","shell.execute_reply":"2025-01-19T21:14:43.094939Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def eval_model(model, data_loader, criterion, device):\n\n    model.eval()   # imposto il modello in modalità valutavione\n\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n\n            # Sposto i dati sul device\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['labels'].to(device)\n\n            # Effettuo la previsione per il batch corrente\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids = token_type_ids\n            )\n            \n            # Calcolo la loss\n            loss = criterion(outputs, labels)\n\n            # Aggiorno la loss, e salvo le previsioni le etichette\n            total_loss += loss.item()\n    \n            probs = torch.sigmoid(outputs)  # Calcolo le probabilità\n\n            preds = (probs > 0.5).long()  # Trasformo in etichette binarie\n            \n            all_preds.extend(preds.detach().cpu().numpy())\n            all_labels.extend(labels.detach().cpu().numpy())\n            all_probs.extend(probs.detach().cpu().numpy())\n\n        # Calcolo loss e metriche\n        avg_loss = total_loss / len(data_loader)\n        accuracy = accuracy_score(all_labels, all_preds)\n        f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n        roc_auc = roc_auc_score(all_labels, all_probs, average='weighted', multi_class='ovr')\n    \n    return avg_loss, accuracy, f1, roc_auc\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:14:48.556082Z","iopub.execute_input":"2025-01-19T21:14:48.556381Z","iopub.status.idle":"2025-01-19T21:14:48.563268Z","shell.execute_reply.started":"2025-01-19T21:14:48.556359Z","shell.execute_reply":"2025-01-19T21:14:48.562313Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"Definisco le configurazion principali per il training.","metadata":{}},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 3e-5\nEPOCHS = 4\nBATCH_SIZE = 16\n\n\n# Creo i DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(train_loader) * EPOCHS\n\n\n# Funzione di loss\ncriterion = torch.nn.BCEWithLogitsLoss() # Applica automaticamente la sigmoide\n\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(params = full_model.parameters(), lr = learning_rate)\n\n# Scheduler\nscheduler = transformers.get_linear_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:14:52.479193Z","iopub.execute_input":"2025-01-19T21:14:52.479575Z","iopub.status.idle":"2025-01-19T21:14:52.488189Z","shell.execute_reply.started":"2025-01-19T21:14:52.479533Z","shell.execute_reply":"2025-01-19T21:14:52.487157Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"history_bert, total_time_bert = train_and_evaluate_model(\n    full_model, \"full_model\", train_loader, train_loader, criterion, optimizer, scheduler, device, epochs=4\n)\nprint(f\"\\nBERT Training Time: {total_time_bert:.2f} seconds, {total_time_bert/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:14:55.825247Z","iopub.execute_input":"2025-01-19T21:14:55.825561Z","iopub.status.idle":"2025-01-19T21:21:12.936980Z","shell.execute_reply.started":"2025-01-19T21:14:55.825537Z","shell.execute_reply":"2025-01-19T21:21:12.936132Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/4\n------------------------------\nTrain loss: 0.107, Train accuracy: 0.8962\nValidation loss: 0.048, Validation accuracy: 0.929\n\nEpoch 2/4\n------------------------------\nTrain loss: 0.046, Train accuracy: 0.9274\nValidation loss: 0.034, Validation accuracy: 0.948\n\nEpoch 3/4\n------------------------------\nTrain loss: 0.034, Train accuracy: 0.9440\nValidation loss: 0.026, Validation accuracy: 0.959\n\nEpoch 4/4\n------------------------------\nTrain loss: 0.027, Train accuracy: 0.9582\nValidation loss: 0.025, Validation accuracy: 0.969\n\nBERT Training Time: 377.11 seconds, 6.29 minutes.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Parametri principali\nlearning_rate = 5e-4\nEPOCHS = 6\nBATCH_SIZE = 16\n\n# Creo i DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\ntotal_steps = len(train_loader) * EPOCHS\n\n\n# Funzione di loss\ncriterion = torch.nn.BCEWithLogitsLoss() # Applica automaticamente la sigmoide\n\n# Ottimizzatore\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr = learning_rate)\n\n# Scheduler\nscheduler = transformers.get_linear_schedule_with_warmup(optimizer = optimizer,\n                                                       num_warmup_steps = 0,\n                                                       num_training_steps = total_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:21:12.938277Z","iopub.execute_input":"2025-01-19T21:21:12.938588Z","iopub.status.idle":"2025-01-19T21:21:12.946724Z","shell.execute_reply.started":"2025-01-19T21:21:12.938567Z","shell.execute_reply":"2025-01-19T21:21:12.946068Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"history_lora, total_time_lora = train_and_evaluate_model(\n    lora_model,\"lora_model\", train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=EPOCHS\n)\nprint(f\"BERT with LoRA Training Time: {total_time_lora:.2f} seconds, {total_time_lora/60:.2f} minutes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:21:12.948772Z","iopub.execute_input":"2025-01-19T21:21:12.948978Z","iopub.status.idle":"2025-01-19T21:26:24.876279Z","shell.execute_reply.started":"2025-01-19T21:21:12.948960Z","shell.execute_reply":"2025-01-19T21:26:24.875513Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/6\n------------------------------\nTrain loss: 0.114, Train accuracy: 0.8936\nValidation loss: 0.073, Validation accuracy: 0.894\n\nEpoch 2/6\n------------------------------\nTrain loss: 0.059, Train accuracy: 0.9104\nValidation loss: 0.065, Validation accuracy: 0.894\n\nEpoch 3/6\n------------------------------\nTrain loss: 0.051, Train accuracy: 0.9156\nValidation loss: 0.065, Validation accuracy: 0.896\n\nEpoch 4/6\n------------------------------\nTrain loss: 0.046, Train accuracy: 0.9210\nValidation loss: 0.065, Validation accuracy: 0.888\n\nEpoch 5/6\n------------------------------\nTrain loss: 0.043, Train accuracy: 0.9254\nValidation loss: 0.065, Validation accuracy: 0.892\n\nEpoch 6/6\n------------------------------\nTrain loss: 0.041, Train accuracy: 0.9274\nValidation loss: 0.065, Validation accuracy: 0.888\nBERT with LoRA Training Time: 311.91 seconds, 5.20 minutes.\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"### 4. Valutazione dei modelli\nValuto i modello calcolando la loss sul test set, l'accuracy, l'F1-score e ROC AUC.","metadata":{}},{"cell_type":"code","source":"full_model.load_state_dict(torch.load(\"toxicity_best_full_model_state.bin\"))\n\ntest_loss, test_acc, test_f1, test_auc = eval_model(full_model, test_loader, criterion, device)\nprint(f\"Full Fine-Tuning - Test loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}, F1 score: {test_f1:.4f}, ROC AUC: {test_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:26:24.877140Z","iopub.execute_input":"2025-01-19T21:26:24.877374Z","iopub.status.idle":"2025-01-19T21:26:30.217845Z","shell.execute_reply.started":"2025-01-19T21:26:24.877343Z","shell.execute_reply":"2025-01-19T21:26:30.216941Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-41-5adac543eef7>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  full_model.load_state_dict(torch.load(\"toxicity_best_full_model_state.bin\"))\n","output_type":"stream"},{"name":"stdout","text":"Full Fine-Tuning - Test loss: 0.0646, Accuracy: 0.9062, F1 score: 0.7164, ROC AUC: 0.9785\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"lora_model.load_state_dict(torch.load(\"toxicity_best_lora_model_state.bin\"))\n\nlora_test_loss, lora_test_acc, lora_test_f1, lora_test_auc = eval_model(lora_model, test_loader, criterion, device)\nprint(f\"LoRA Fine-Tuning - Test loss: {lora_test_loss:.4f}, Accuracy: {lora_test_acc:.4f}, F1 score: {lora_test_f1:.4f}, ROC AUC: {lora_test_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T21:26:30.218715Z","iopub.execute_input":"2025-01-19T21:26:30.219011Z","iopub.status.idle":"2025-01-19T21:26:35.648652Z","shell.execute_reply.started":"2025-01-19T21:26:30.218974Z","shell.execute_reply":"2025-01-19T21:26:35.647685Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-42-d1c6458ef167>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_model.load_state_dict(torch.load(\"toxicity_best_lora_model_state.bin\"))\n","output_type":"stream"},{"name":"stdout","text":"LoRA Fine-Tuning - Test loss: 0.0621, Accuracy: 0.9082, F1 score: 0.7009, ROC AUC: 0.9739\n","output_type":"stream"}],"execution_count":42}]}