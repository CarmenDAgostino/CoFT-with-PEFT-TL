{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10161839,"sourceType":"datasetVersion","datasetId":6274971}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Contronto tra il fine-tuning completo e il fine-tuning che utilizza LoRA\r\n\r\n\r\n**Configurazioni**  \r\nInstallazione delle librerie necessarie.","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets torch loralib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:58:58.465042Z","iopub.execute_input":"2024-12-11T10:58:58.465391Z","iopub.status.idle":"2024-12-11T10:59:07.098495Z","shell.execute_reply.started":"2024-12-11T10:58:58.465360Z","shell.execute_reply":"2024-12-11T10:59:07.097397Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: loralib in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"Carico i file `lora_utilis.py` e `models.py`.","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/lora-utils/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:59:07.100912Z","iopub.execute_input":"2024-12-11T10:59:07.101331Z","iopub.status.idle":"2024-12-11T10:59:07.106155Z","shell.execute_reply.started":"2024-12-11T10:59:07.101286Z","shell.execute_reply":"2024-12-11T10:59:07.105204Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"Importo i moduli necessari.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\nimport lora_utils, models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:59:07.107425Z","iopub.execute_input":"2024-12-11T10:59:07.107708Z","iopub.status.idle":"2024-12-11T10:59:07.120822Z","shell.execute_reply.started":"2024-12-11T10:59:07.107684Z","shell.execute_reply":"2024-12-11T10:59:07.120038Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Sentiment analisys","metadata":{}},{"cell_type":"markdown","source":"### 1. Ottenimento dei dati e preprocessing\r\nConfronto il fine-tuning completo e quello che usa LoRA innanzitutto sul task di classificazione binaria del sentimento utilizzando il dataset SST-2.\r\n\r\nSST-2 contiene esempi che consistono in frasi tratte da recensioni di film le cui etichette sono 1 se la recensione positiva, 0 altrimenti.\r\n\r\nUtilizzo la libreria `datasets` di Hugging Face per caricare il dataset e dividerlo in training sat, velidation set e test set.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset('sst2')\n\nprint(dataset)\n\ntrain_data = dataset['train'].shuffle(seed=42).select(range(10000))\nval_data = dataset['validation']\ntest_data = dataset['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:14:57.106402Z","iopub.execute_input":"2024-12-11T11:14:57.107300Z","iopub.status.idle":"2024-12-11T11:14:59.965336Z","shell.execute_reply.started":"2024-12-11T11:14:57.107263Z","shell.execute_reply":"2024-12-11T11:14:59.964464Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4feae0a444e140a38f1bf22d9024b465"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b0b84f40624b48a7194e11f4cb5124"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f2f577f5bf649e78078cfb05d898212"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32e3882f975546728994cd18482bc6d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d9333e8206d46df820918c2f4df201d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70b6c205d5894f2c82ea6607d29d2d8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6218945d9c4e43cf8ef381454d8c21fb"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['idx', 'sentence', 'label'],\n        num_rows: 67349\n    })\n    validation: Dataset({\n        features: ['idx', 'sentence', 'label'],\n        num_rows: 872\n    })\n    test: Dataset({\n        features: ['idx', 'sentence', 'label'],\n        num_rows: 1821\n    })\n})\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"from collections import Counter\n\nlabels = [example['label'] for example in val_data]\nprint(Counter(labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:32:26.698239Z","iopub.execute_input":"2024-12-11T11:32:26.699011Z","iopub.status.idle":"2024-12-11T11:32:26.731841Z","shell.execute_reply.started":"2024-12-11T11:32:26.698975Z","shell.execute_reply":"2024-12-11T11:32:26.730810Z"}},"outputs":[{"name":"stdout","text":"Counter({1: 444, 0: 428})\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# Tokenizzazione\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_function(batch):\n    return tokenizer(batch[\"sentence\"], padding=\"max_length\", truncation=True, max_length=512)\n\ntrain_data = train_data.map(tokenize_function, batched=True)\nval_data = val_data.map(tokenize_function, batched=True)\ntest_data = test_data.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T09:25:50.102360Z","iopub.execute_input":"2024-12-11T09:25:50.103045Z","iopub.status.idle":"2024-12-11T09:25:53.490077Z","shell.execute_reply.started":"2024-12-11T09:25:50.103010Z","shell.execute_reply":"2024-12-11T09:25:53.489302Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cdca98c5ea94b1aae958c4b795fb14d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9b984a658484b2385dd2044d45e40ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27b186e0281b4a3795c40f5ed33fce36"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Conversione dei dati in tensori\n\ndef to_tensor(dataset, shuffle=True):\n    return torch.utils.data.DataLoader(\n        dataset.with_format(\"torch\"), batch_size=16, shuffle=shuffle\n    )\n\ntrain_loader = to_tensor(train_data)\nval_loader = to_tensor(val_data)\ntest_loader = to_tensor(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T09:25:57.621242Z","iopub.execute_input":"2024-12-11T09:25:57.622137Z","iopub.status.idle":"2024-12-11T09:25:57.630403Z","shell.execute_reply.started":"2024-12-11T09:25:57.622100Z","shell.execute_reply":"2024-12-11T09:25:57.629410Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### 2. Configurazione dei modelli\r\n\r\nCarico il modello BERT pre-addestrato e ne costruisco un altro che utilizza LoRA.  *\r\n\r\nModello stan*dard.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nfull_model = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", num_labels=2\n)\nfull_model.to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T09:26:01.160675Z","iopub.execute_input":"2024-12-11T09:26:01.161027Z","iopub.status.idle":"2024-12-11T09:26:15.751277Z","shell.execute_reply.started":"2024-12-11T09:26:01.160998Z","shell.execute_reply":"2024-12-11T09:26:15.750394Z"},"scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30bec46cf03b40f196809ca8c59776b1"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"*Modello con LoRA*.","metadata":{}},{"cell_type":"code","source":"lora_model = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", num_labels=2\n)\nlora_utils.add_lora_to_bert(lora_model.bert, r=8)\nlora_utils.mark_only_lora_as_trainable(lora_model.bert)\nlora_model.to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T09:26:24.740963Z","iopub.execute_input":"2024-12-11T09:26:24.742096Z","iopub.status.idle":"2024-12-11T09:26:25.337465Z","shell.execute_reply.started":"2024-12-11T09:26:24.742059Z","shell.execute_reply":"2024-12-11T09:26:25.336612Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"### 3. Addestramento dei modelli","metadata":{}},{"cell_type":"code","source":"import time\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LinearLR\n\n\n# funzione di perdita e ottimizzatore\ncriterion = torch.nn.CrossEntropyLoss()\nfull_optimizer = AdamW(full_model.parameters(), lr=2e-5)\nlora_optimizer = AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr=2e-4)\n\n\nepochs = 5\n\n# Aggiungi il Learning Rate Scheduler lineare\nfull_scheduler = LinearLR(full_optimizer, start_factor=1.0, end_factor=0.1, total_iters=epochs)  # Modifica total_iters in base alle epoche\nlora_scheduler = LinearLR(lora_optimizer, start_factor=1.0, end_factor=0.1, total_iters=epochs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T09:26:30.406204Z","iopub.execute_input":"2024-12-11T09:26:30.407036Z","iopub.status.idle":"2024-12-11T09:26:30.414505Z","shell.execute_reply.started":"2024-12-11T09:26:30.407001Z","shell.execute_reply":"2024-12-11T09:26:30.413587Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Funzione per l'addestramento\ndef train_model(model, dataloader, optimizer, scheduler, epochs=5):\n\n    model.train()\n    start_time = time.time()\n\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for batch in dataloader:\n            optimizer.zero_grad()\n\n            # Sposto i dati su GPU\n            input_ids = batch[\"input_ids\"].to(\"cuda\")\n            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n            labels = batch[\"label\"].to(\"cuda\")\n\n            # Calcolo della loss\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n\n            # Loss per l'epoca\n            epoch_loss += loss.item()\n\n        # Learning Rate Scheduler\n        scheduler.step()\n\n        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {epoch_loss:.4f}\")\n\n    end_time = time.time()\n    training_time = end_time - start_time\n    \n    return training_time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T09:26:36.942402Z","iopub.execute_input":"2024-12-11T09:26:36.943284Z","iopub.status.idle":"2024-12-11T09:26:36.949473Z","shell.execute_reply.started":"2024-12-11T09:26:36.943250Z","shell.execute_reply":"2024-12-11T09:26:36.948534Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Addestramento modello full fine-tuning\n\nprint(\"Inizio addestramento modello full Fine-Tuning ...\")\nfull_training_time = train_model(full_model, train_loader, full_optimizer, full_scheduler, epochs)\n\nprint(f\"Tempo totale di addestramento Full Fine-Tuning: {full_training_time:.2f} secondi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T09:26:42.708216Z","iopub.execute_input":"2024-12-11T09:26:42.709043Z","iopub.status.idle":"2024-12-11T10:11:30.367380Z","shell.execute_reply.started":"2024-12-11T09:26:42.709006Z","shell.execute_reply":"2024-12-11T10:11:30.366409Z"}},"outputs":[{"name":"stdout","text":"Inizio addestramento modello full Fine-Tuning ...\nEpoch 1/5 - Loss: 185.9664\nEpoch 2/5 - Loss: 79.4242\nEpoch 3/5 - Loss: 32.9163\nEpoch 4/5 - Loss: 19.9763\nEpoch 5/5 - Loss: 10.8248\nTempo totale di addestramento full Fine-Tuning: 2687.65 secondi\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Addestramento dei modello con LoRA\nprint(\"Inizio addestramento modello con LoRA ...\")\nlora_training_time = train_model(lora_model, train_loader, lora_optimizer, lora_scheduler, epochs)\n\nprint(f\"Tempo totale di addestramento LoRA: {lora_training_time:.2f} secondi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T10:15:57.871440Z","iopub.execute_input":"2024-12-11T10:15:57.872300Z","iopub.status.idle":"2024-12-11T10:50:19.879230Z","shell.execute_reply.started":"2024-12-11T10:15:57.872248Z","shell.execute_reply":"2024-12-11T10:50:19.878302Z"}},"outputs":[{"name":"stdout","text":"Inizio addestramento modello con LoRA ...\nEpoch 1/5 - Loss: 301.9535\nEpoch 2/5 - Loss: 191.0833\nEpoch 3/5 - Loss: 172.0531\nEpoch 4/5 - Loss: 162.6645\nEpoch 5/5 - Loss: 158.8940\nTempo totale di addestramento LoRA: 2062.00 secondi\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### 4. Valutazione dei modelli\r\n\r\nCalcolo l'accuracy e l'F1-score per valutare le prestazioni di entrambi i modelli.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\ndef evaluate_model(model, dataloader):\n    model.eval()\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(\"cuda\")\n            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n            labels = batch[\"label\"].to(\"cuda\")\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            preds = torch.argmax(outputs.logits, axis=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n    return acc, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:33:27.816443Z","iopub.execute_input":"2024-12-11T11:33:27.817151Z","iopub.status.idle":"2024-12-11T11:33:27.823492Z","shell.execute_reply.started":"2024-12-11T11:33:27.817113Z","shell.execute_reply":"2024-12-11T11:33:27.822586Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"full_acc, full_f1 = evaluate_model(full_model, val_loader)\nlora_acc, lora_f1 = evaluate_model(lora_model, val_loader)\nprint(f\"Full Fine-Tuning - Accuracy: {full_acc}, F1: {full_f1}\")\nprint(f\"LoRA Fine-Tuning - Accuracy: {lora_acc}, F1: {lora_f1}\")\n\nprint(f\"Tempo totale di addestramento Full Fine-Tuning: {full_training_time:.2f} secondi\")\nprint(f\"Tempo totale di addestramento LoRA: {lora_training_time:.2f} secondi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:33:33.316505Z","iopub.execute_input":"2024-12-11T11:33:33.316874Z","iopub.status.idle":"2024-12-11T11:33:59.417575Z","shell.execute_reply.started":"2024-12-11T11:33:33.316814Z","shell.execute_reply":"2024-12-11T11:33:59.416552Z"}},"outputs":[{"name":"stdout","text":"Full Fine-Tuning - Accuracy: 0.9094036697247706, F1: 0.9093481969598188\nLoRA Fine-Tuning - Accuracy: 0.893348623853211, F1: 0.8933170394170504\nTempo totale di addestramento Full Fine-Tuning: 2687.65 secondi\nTempo totale di addestramento LoRA: 2062.00 secondi\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"### 5. Salvataggio dei moduli LoRA","metadata":{}},{"cell_type":"code","source":"def save_lora_parameters(model, file_path):\n    lora_params = {}\n    for name, param in model.named_parameters():\n        if \"lora_\" in name and param.requires_grad:\n            lora_params[name] = param.detach().cpu()\n    torch.save(lora_params, file_path)\n    print(f\"LoRA parameters saved to {file_path}\")\n\nsave_lora_parameters(lora_model.bert, \"lora_adapter_params.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:34:59.372952Z","iopub.execute_input":"2024-12-11T11:34:59.373273Z","iopub.status.idle":"2024-12-11T11:34:59.390062Z","shell.execute_reply.started":"2024-12-11T11:34:59.373245Z","shell.execute_reply":"2024-12-11T11:34:59.388887Z"}},"outputs":[{"name":"stdout","text":"LoRA parameters saved to lora_adapter_params.pt\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"def load_lora_parameters(model, file_path):\n    lora_params = torch.load(file_path)\n    with torch.no_grad():\n        for name, param in model.named_parameters():\n            if name in lora_params:\n                param.copy_(lora_params[name])\n    print(f\"LoRA parameters loaded from {file_path}\")\n\nload_lora_parameters(lora_model.bert, \"lora_adapter_params.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:35:03.200638Z","iopub.execute_input":"2024-12-11T11:35:03.201286Z","iopub.status.idle":"2024-12-11T11:35:03.217005Z","shell.execute_reply.started":"2024-12-11T11:35:03.201249Z","shell.execute_reply":"2024-12-11T11:35:03.215993Z"}},"outputs":[{"name":"stdout","text":"LoRA parameters loaded from lora_adapter_params.pt\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_23/3971394709.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  lora_params = torch.load(file_path)\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"## Classificazione di testi","metadata":{}},{"cell_type":"markdown","source":"### 1. Ottenimento dei dati e preprocessing\r\n\r\nProseguo confrontando i due tipi di fine-tuning sul task di topic classificazione utilizzando il dataset AG News.AG News T-2 contiene esempi che consistono in articoli di notizie etichettati in 4 classi: World, Sport, Buisness, Sci/Tech.\r\n\r\nUtilizzo sempre la libreria `datasets` di Hugging Face per caricare il dataset e dividerlo in trainion set e test set.","metadata":{}},{"cell_type":"code","source":"dataset2 = load_dataset('ag_news')\n\nprint(dataset2)\n\ntrain_data = dataset2['train'].shuffle(seed=42).select(range(10000))\ntest_data = dataset2['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:35:09.106261Z","iopub.execute_input":"2024-12-11T11:35:09.106647Z","iopub.status.idle":"2024-12-11T11:35:12.198640Z","shell.execute_reply.started":"2024-12-11T11:35:09.106615Z","shell.execute_reply":"2024-12-11T11:35:12.197743Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a780a69c04c24535b9e77c78a6a6e535"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea0029bce4cf40d1998f34a23396bab6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae34b1da623f40178597e9f9b7140f4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"602f7b75a5544522a4f0caf739555ffe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8adeccb7fed4171a6fcb5a16279472d"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 120000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 7600\n    })\n})\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"labels = [example['label'] for example in test_data]\nprint(Counter(labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:36:31.765663Z","iopub.execute_input":"2024-12-11T11:36:31.766086Z","iopub.status.idle":"2024-12-11T11:36:36.348263Z","shell.execute_reply.started":"2024-12-11T11:36:31.766052Z","shell.execute_reply":"2024-12-11T11:36:36.347262Z"}},"outputs":[{"name":"stdout","text":"Counter({2: 1900, 3: 1900, 1: 1900, 0: 1900})\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"# Tokenizzazione\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n\ndef tokenize_function(batch):\n    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n\ntrain_data = train_data.map(tokenize_function, batched=True)\ntest_data = test_data.map(tokenize_function, batched=True)\n\n\n\n# Conversione dei dati in tensori\ntrain_loader = to_tensor(train_data)\ntest_loader = to_tensor(test_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:35:43.393626Z","iopub.execute_input":"2024-12-11T11:35:43.394358Z","iopub.status.idle":"2024-12-11T11:35:49.172363Z","shell.execute_reply.started":"2024-12-11T11:35:43.394324Z","shell.execute_reply":"2024-12-11T11:35:49.171344Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"157f6908a5e44003b79eefade9839238"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2636081927314a53a139378e208b7cc0"}},"metadata":{}}],"execution_count":57},{"cell_type":"markdown","source":"### 2. Configurazione dei modelli\r\n\r\n*Modello standard*","metadata":{}},{"cell_type":"code","source":"full_model = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", num_labels=4\n)\nfull_model.to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:35:52.178401Z","iopub.execute_input":"2024-12-11T11:35:52.179080Z","iopub.status.idle":"2024-12-11T11:35:52.443412Z","shell.execute_reply.started":"2024-12-11T11:35:52.179042Z","shell.execute_reply":"2024-12-11T11:35:52.442355Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)"},"metadata":{}}],"execution_count":58},{"cell_type":"markdown","source":"*Modello con LoRA*","metadata":{}},{"cell_type":"code","source":"lora_model = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", num_labels=4\n)\nlora_utils.add_lora_to_bert(lora_model.bert, r=8)\nlora_utils.mark_only_lora_as_trainable(lora_model.bert)\nlora_model.to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:35:57.610475Z","iopub.execute_input":"2024-12-11T11:35:57.611220Z","iopub.status.idle":"2024-12-11T11:35:58.141055Z","shell.execute_reply.started":"2024-12-11T11:35:57.611183Z","shell.execute_reply":"2024-12-11T11:35:58.140153Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)"},"metadata":{}}],"execution_count":59},{"cell_type":"markdown","source":"### 3. Addestramento dei modelli","metadata":{}},{"cell_type":"code","source":"epochs = 5\n\n# Aggiungi il Learning Rate Scheduler lineare\nfull_scheduler = LinearLR(full_optimizer, start_factor=1.0, end_factor=0.1, total_iters=epochs)  # Modifica total_iters in base alle epoche\nlora_scheduler = LinearLR(lora_optimizer, start_factor=1.0, end_factor=0.1, total_iters=epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:36:48.642517Z","iopub.execute_input":"2024-12-11T11:36:48.642895Z","iopub.status.idle":"2024-12-11T11:36:48.647916Z","shell.execute_reply.started":"2024-12-11T11:36:48.642852Z","shell.execute_reply":"2024-12-11T11:36:48.646893Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# Addestramento modello full fine-tuning\n\nprint(\"Inizio addestramento modello full Fine-Tuning ...\")\nfull_training_time = train_model(full_model, train_loader, full_optimizer, full_scheduler, epochs)\n\nprint(f\"Tempo totale di addestramento Full Fine-Tuning: {full_training_time:.2f} secondi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:36:52.110498Z","iopub.execute_input":"2024-12-11T11:36:52.111331Z","iopub.status.idle":"2024-12-11T12:21:05.023232Z","shell.execute_reply.started":"2024-12-11T11:36:52.111290Z","shell.execute_reply":"2024-12-11T12:21:05.022116Z"}},"outputs":[{"name":"stdout","text":"Inizio addestramento modello full Fine-Tuning ...\nEpoch 1/5 - Loss: 887.2023\nEpoch 2/5 - Loss: 888.3462\nEpoch 3/5 - Loss: 889.7485\nEpoch 4/5 - Loss: 889.6065\nEpoch 5/5 - Loss: 889.1687\nTempo totale di addestramento Full Fine-Tuning: 2652.91 secondi\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"# Addestramento dei modello con LoRA\nprint(\"Inizio addestramento modello con LoRA ...\")\nlora_training_time = train_model(lora_model, train_loader, lora_optimizer, lora_scheduler, epochs)\n\nprint(f\"Tempo totale di addestramento LoRA: {lora_training_time:.2f} secondi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:23:52.263946Z","iopub.execute_input":"2024-12-11T12:23:52.264817Z","iopub.status.idle":"2024-12-11T12:58:15.010949Z","shell.execute_reply.started":"2024-12-11T12:23:52.264784Z","shell.execute_reply":"2024-12-11T12:58:15.009979Z"}},"outputs":[{"name":"stdout","text":"Inizio addestramento modello con LoRA ...\nEpoch 1/5 - Loss: 906.1448\nEpoch 2/5 - Loss: 905.2233\nEpoch 3/5 - Loss: 904.6969\nEpoch 4/5 - Loss: 905.7273\nEpoch 5/5 - Loss: 906.4725\nTempo totale di addestramento LoRA: 2062.74 secondi\n","output_type":"stream"}],"execution_count":65},{"cell_type":"markdown","source":"### 4. Valutazione dei modelli\r\n\r\nCalcolo l'accuracy e l'F1-score per valutare le prestazioni di entrambi i modelli.","metadata":{}},{"cell_type":"code","source":"full_acc, full_f1 = evaluate_model(full_model, test_loader)\nlora_acc, lora_f1 = evaluate_model(lora_model, test_loader)\nprint(f\"Full Fine-Tuning - Accuracy: {full_acc}, F1: {full_f1}\")\nprint(f\"LoRA Fine-Tuning - Accuracy: {lora_acc}, F1: {lora_f1}\")\n\nprint(f\"Tempo totale di addestramento Full Fine-Tuning: {full_training_time:.2f} secondi\")\nprint(f\"Tempo totale di addestramento LoRA: {lora_training_time:.2f} secondi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:58:15.012420Z","iopub.execute_input":"2024-12-11T12:58:15.012716Z","iopub.status.idle":"2024-12-11T13:02:04.714771Z","shell.execute_reply.started":"2024-12-11T12:58:15.012688Z","shell.execute_reply":"2024-12-11T13:02:04.713866Z"}},"outputs":[{"name":"stdout","text":"Full Fine-Tuning - Accuracy: 0.2530263157894737, F1: 0.1106988190196375\nLoRA Fine-Tuning - Accuracy: 0.23578947368421052, F1: 0.1248002773231829\nTempo totale di addestramento Full Fine-Tuning: 2652.91 secondi\nTempo totale di addestramento LoRA: 2062.74 secondi\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"## Natural Language Inference","metadata":{}},{"cell_type":"markdown","source":"### 1. Ottenimento dei dati e preprocessing\n\nProseguo confrontando i due tipi di fine-tuning sul task di Natural Language Inference utilizzando il dataset MNLI.\n\nMNLI contiene esempi che consistono in una coppia di frasi (premessa e ipotesi). Le etichette indicano la relazione tra queste due frasi. Le possibili etichette sono:\n- *Entailment*: l'ipotesi è implicata dalla premessa.\n- *Contradiction*: l'ipotesi contraddice la premessa.\n- *Neutral*: nessuna relazione specifica.\n\nUtilizzo sempre la libreria `datasets` di Hugging Face per caricare il dataset e dividerlo in training set, validation set e test set.","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset('glue', 'mnli')\n\nprint(dataset)\n\n# Ottengo train, validation e test set\ntrain_data = dataset['train'].shuffle(seed=42).select(range(10000))  \nval_data_matched = dataset['validation_matched']  # matched: gli esempi provengono dagli stessi domini presenti nel set di addestramento.\nval_data_mismatched = dataset['validation_mismatched'].select(range(5000))  # mismatched: gli esempi provengono da domini diversi rispetto a quelli del set di addestramento.\ntest_data_matched = dataset['test_matched'] \ntest_data_mismatched = dataset['test_mismatched']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:41:23.250620Z","iopub.execute_input":"2024-12-11T13:41:23.251440Z","iopub.status.idle":"2024-12-11T13:41:24.794023Z","shell.execute_reply.started":"2024-12-11T13:41:23.251402Z","shell.execute_reply":"2024-12-11T13:41:24.793057Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 392702\n    })\n    validation_matched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9815\n    })\n    validation_mismatched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9832\n    })\n    test_matched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9796\n    })\n    test_mismatched: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'idx'],\n        num_rows: 9847\n    })\n})\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"labels = [example['label'] for example in val_data_mismatched]\nprint(Counter(labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:41:40.250504Z","iopub.execute_input":"2024-12-11T13:41:40.251204Z","iopub.status.idle":"2024-12-11T13:41:40.461171Z","shell.execute_reply.started":"2024-12-11T13:41:40.251169Z","shell.execute_reply":"2024-12-11T13:41:40.460130Z"}},"outputs":[{"name":"stdout","text":"Counter({0: 1797, 2: 1663, 1: 1540})\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"# Tokenizzazione\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_function(batch):\n    return tokenizer(\n        batch[\"premise\"], \n        batch[\"hypothesis\"], \n        padding=\"max_length\", \n        truncation=True, \n        max_length=512\n    )\n\ntrain_data = train_data.map(tokenize_function, batched=True)\nval_data_matched = val_data_matched.map(tokenize_function, batched=True)\nval_data_mismatched = val_data_mismatched.map(tokenize_function, batched=True)\ntest_data_matched = test_data_matched.map(tokenize_function, batched=True)\ntest_data_mismatched = test_data_mismatched.map(tokenize_function, batched=True)\n\n# Rimozione delle colonne non necessarie\ncolumns_to_keep = [\"input_ids\", \"attention_mask\", \"label\"]\n\ntrain_data = train_data.remove_columns([col for col in train_data.column_names if col not in columns_to_keep])\nval_data_matched = val_data_matched.remove_columns([col for col in val_data_matched.column_names if col not in columns_to_keep])\nval_data_mismatched = val_data_mismatched.remove_columns([col for col in val_data_mismatched.column_names if col not in columns_to_keep])\ntest_data_matched = test_data_matched.remove_columns([col for col in test_data_matched.column_names if col not in columns_to_keep])\ntest_data_mismatched = test_data_mismatched.remove_columns([col for col in test_data_mismatched.column_names if col not in columns_to_keep])\n\n\n\n# Conversione dei dati\n\ndef to_tensor(dataset):\n    return torch.utils.data.DataLoader(\n        dataset.with_format(\"torch\"), \n        batch_size=16, \n        collate_fn=lambda x: {\n            key: torch.stack([d[key] for d in x]) for key in x[0].keys()\n        }\n    )\n\ntrain_loader = to_tensor(train_data)\nval_matched_loader = to_tensor(val_data_matched)\nval_mismatched_loader = to_tensor(val_data_mismatched)\ntest_matched_loader = to_tensor(test_data_matched)\ntest_mismatched_loader = to_tensor(test_data_mismatched)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:41:43.476948Z","iopub.execute_input":"2024-12-11T13:41:43.477542Z","iopub.status.idle":"2024-12-11T13:41:57.914807Z","shell.execute_reply.started":"2024-12-11T13:41:43.477507Z","shell.execute_reply":"2024-12-11T13:41:57.914131Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18286f516ed9478d97a686d7b2edd519"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9815 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c695ce0b55d047bdb53be81266d7b5e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d993a0a7dc1c483b898a079285473ef2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9796 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"509fd7c4d7124c8792e648e8dbbe93db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9847 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84ea744642c949b784174eb0c93efe94"}},"metadata":{}}],"execution_count":75},{"cell_type":"markdown","source":"### 2. Configurazione dei modelli\n\n*Modello standard*","metadata":{}},{"cell_type":"code","source":"full_model = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", num_labels=3\n)\nfull_model.to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:41:58.191738Z","iopub.execute_input":"2024-12-11T13:41:58.192135Z","iopub.status.idle":"2024-12-11T13:41:58.463135Z","shell.execute_reply.started":"2024-12-11T13:41:58.192094Z","shell.execute_reply":"2024-12-11T13:41:58.462209Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}],"execution_count":77},{"cell_type":"markdown","source":"*Modello con LoRA*.","metadata":{}},{"cell_type":"code","source":"lora_model = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", num_labels=3\n)\nlora_utils.add_lora_to_bert(lora_model.bert, r=8)\nlora_utils.mark_only_lora_as_trainable(lora_model.bert)\nlora_model.to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:42:02.630382Z","iopub.execute_input":"2024-12-11T13:42:02.631226Z","iopub.status.idle":"2024-12-11T13:42:03.161976Z","shell.execute_reply.started":"2024-12-11T13:42:02.631191Z","shell.execute_reply":"2024-12-11T13:42:03.161057Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}],"execution_count":78},{"cell_type":"markdown","source":"### 3. Addestramento dei modelli","metadata":{}},{"cell_type":"code","source":"epochs = 5\n\n# Aggiungi il Learning Rate Scheduler lineare\nfull_scheduler = LinearLR(full_optimizer, start_factor=1.0, end_factor=0.1, total_iters=epochs)  # Modifica total_iters in base alle epoche\nlora_scheduler = LinearLR(lora_optimizer, start_factor=1.0, end_factor=0.1, total_iters=epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:42:12.434294Z","iopub.execute_input":"2024-12-11T13:42:12.435073Z","iopub.status.idle":"2024-12-11T13:42:12.440756Z","shell.execute_reply.started":"2024-12-11T13:42:12.435020Z","shell.execute_reply":"2024-12-11T13:42:12.439609Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"# Addestramento modello full fine-tuning\n\nprint(\"Inizio addestramento modello full Fine-Tuning ...\")\nfull_training_time = train_model(full_model, train_loader, full_optimizer, full_scheduler, epochs)\n\nprint(f\"Tempo totale di addestramento Full Fine-Tuning: {full_training_time:.2f} secondi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T13:42:15.040605Z","iopub.execute_input":"2024-12-11T13:42:15.041304Z","iopub.status.idle":"2024-12-11T14:26:27.468636Z","shell.execute_reply.started":"2024-12-11T13:42:15.041270Z","shell.execute_reply":"2024-12-11T14:26:27.467673Z"}},"outputs":[{"name":"stdout","text":"Inizio addestramento modello full Fine-Tuning ...\nEpoch 1/5 - Loss: 693.9929\nEpoch 2/5 - Loss: 695.3533\nEpoch 3/5 - Loss: 695.7772\nEpoch 4/5 - Loss: 695.7490\nEpoch 5/5 - Loss: 693.7836\nTempo totale di addestramento Full Fine-Tuning: 2652.42 secondi\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"# Addestramento dei modello con LoRA\nprint(\"Inizio addestramento modello con LoRA ...\")\nlora_training_time = train_model(lora_model, train_loader, lora_optimizer, lora_scheduler, epochs)\n\nprint(f\"Tempo totale di addestramento LoRA: {lora_training_time:.2f} secondi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T14:26:27.470691Z","iopub.execute_input":"2024-12-11T14:26:27.471335Z"}},"outputs":[{"name":"stdout","text":"Inizio addestramento modello con LoRA ...\nEpoch 1/5 - Loss: 695.5546\nEpoch 2/5 - Loss: 695.5251\nEpoch 3/5 - Loss: 695.2860\nEpoch 4/5 - Loss: 695.5838\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### 4. Valutazione dei modelli","metadata":{}},{"cell_type":"code","source":"full_acc, full_f1 = evaluate_model(full_model, test_loader)\nlora_acc, lora_f1 = evaluate_model(lora_model, test_loader)\nprint(f\"Full Fine-Tuning - Accuracy: {full_acc}, F1: {full_f1}\")\nprint(f\"LoRA Fine-Tuning - Accuracy: {lora_acc}, F1: {lora_f1}\")\n\nprint(f\"Tempo totale di addestramento Full Fine-Tuning: {full_training_time:.2f} secondi\")\nprint(f\"Tempo totale di addestramento LoRA: {lora_training_time:.2f} secondi\")","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2024-12-11T15:04:39.078088Z","shell.execute_reply.started":"2024-12-11T15:00:50.229221Z","shell.execute_reply":"2024-12-11T15:04:39.077095Z"}},"outputs":[{"name":"stdout","text":"Full Fine-Tuning - Accuracy: 0.24960526315789475, F1: 0.11765386179251446\nLoRA Fine-Tuning - Accuracy: 0.24052631578947367, F1: 0.1337429014304005\nTempo totale di addestramento Full Fine-Tuning: 2652.42 secondi\nTempo totale di addestramento LoRA: 2062.75 secondi\n","output_type":"stream"}],"execution_count":82}]}
