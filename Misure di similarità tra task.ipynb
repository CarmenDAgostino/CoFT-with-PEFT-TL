{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715},{"sourceId":10547964,"sourceType":"datasetVersion","datasetId":6514069}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Confronto delle misure di similarità tra task","metadata":{}},{"cell_type":"markdown","source":"#### Configurazioni generali","metadata":{}},{"cell_type":"markdown","source":"Importo i modulii necessari.","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:09:37.461608Z","iopub.execute_input":"2025-01-22T16:09:37.461908Z","iopub.status.idle":"2025-01-22T16:09:41.674219Z","shell.execute_reply.started":"2025-01-22T16:09:37.461885Z","shell.execute_reply":"2025-01-22T16:09:41.673446Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"Imposto il seme per la riproducibilità.","metadata":{}},{"cell_type":"code","source":"seed_value = 42\n\nos.environ['PYTHONHASHSEED'] = str(seed_value)\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\n\n# Imposto il seme casuale anche per i calcoli CUDA\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)  \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:09:41.675262Z","iopub.execute_input":"2025-01-22T16:09:41.675610Z","iopub.status.idle":"2025-01-22T16:09:41.739041Z","shell.execute_reply.started":"2025-01-22T16:09:41.675586Z","shell.execute_reply":"2025-01-22T16:09:41.738109Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Caricamento dei dataset","metadata":{}},{"cell_type":"markdown","source":"#### 1. IMDB Reviews","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Scarico l'ultima versione del dataset\nimdb_path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n\nimdb_dataset_path = imdb_path + \"/IMDB Dataset.csv\"\nimdb_dataset = pd.read_csv(imdb_dataset_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:09:41.740447Z","iopub.execute_input":"2025-01-22T16:09:41.740715Z","iopub.status.idle":"2025-01-22T16:09:43.517885Z","shell.execute_reply.started":"2025-01-22T16:09:41.740693Z","shell.execute_reply":"2025-01-22T16:09:43.517223Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"LABELS = {\"negative\": 0, \"positive\": 1}\nimdb_classes = list(LABELS.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:09:43.519093Z","iopub.execute_input":"2025-01-22T16:09:43.519394Z","iopub.status.idle":"2025-01-22T16:09:43.522822Z","shell.execute_reply.started":"2025-01-22T16:09:43.519358Z","shell.execute_reply":"2025-01-22T16:09:43.522088Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def get_data(dataset_path, n_train=5000, n_val=500, n_test=512): \n\n    dataset = pd.read_csv(dataset_path)\n    dataset['sentiment'] = dataset[\"sentiment\"].map(LABELS)\n\n    neg = dataset[ dataset['sentiment'] == LABELS['negative'] ]\n    pos = dataset[ dataset['sentiment'] == LABELS['positive'] ]\n\n    if len(neg) < n_train + n_val + n_test or len(pos) < n_train + n_val + n_test:\n        raise ValueError(\"Non ci sono abbastanza esempi per le dimensioni del train, validation e test set specificate.\")\n    \n    neg = neg.sample(frac=1, random_state=42).reset_index(drop=True)\n    pos = pos.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    neg_train, pos_train = neg[:n_train], pos[:n_train]\n    neg_val, pos_val = neg[n_train:n_train+n_val], pos[n_train:n_train+n_val]\n    neg_test, pos_test = neg[n_train+n_val:n_train+n_val+n_test], pos[n_train+n_val:n_train+n_val+n_test]\n\n    train_data = pd.concat([neg_train, pos_train])\n    val_data = pd.concat([neg_val, pos_val])\n    test_data = pd.concat([neg_test, pos_test])\n\n    train_data = train_data.sample(frac=1, random_state=42).reset_index(drop=True)\n    val_data = val_data.sample(frac=1, random_state=42).reset_index(drop=True)\n    test_data = test_data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    sentences_train, labels_train = train_data['review'] , train_data['sentiment']\n    sentences_val, labels_val = val_data['review'] , val_data['sentiment']\n    sentences_test, labels_test = test_data['review'] , test_data['sentiment']\n\n    return sentences_train, labels_train, sentences_val, labels_val, sentences_test, labels_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:09:43.523572Z","iopub.execute_input":"2025-01-22T16:09:43.523805Z","iopub.status.idle":"2025-01-22T16:09:43.538296Z","shell.execute_reply.started":"2025-01-22T16:09:43.523783Z","shell.execute_reply":"2025-01-22T16:09:43.537540Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass IMDBDataset(Dataset):\n\n    def __init__(self, sentences, labels, tokenizer, max_len):\n        self.sentences = sentences\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    def __getitem__(self,index):\n        sentence = self.sentences[index]\n        label = self.labels[index]\n        \n        encoding = self.tokenizer.encode_plus(\n            sentence,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors='pt')\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding[\"token_type_ids\"].flatten(),\n            'labels': torch.tensor(label, dtype=torch.float)      \n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:09:43.538938Z","iopub.execute_input":"2025-01-22T16:09:43.539236Z","iopub.status.idle":"2025-01-22T16:09:43.554785Z","shell.execute_reply.started":"2025-01-22T16:09:43.539214Z","shell.execute_reply":"2025-01-22T16:09:43.554106Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 128\nBATCH_SIZE = 32\n\nimdb_sentences_train, imdb_labels_train, imdb_sentences_val, imdb_labels_val, imdb_sentences_test, imdb_labels_test = get_data(imdb_dataset_path)\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nimdb_training_data = IMDBDataset(sentences = imdb_sentences_train,\n                           labels = imdb_labels_train,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nimdb_validation_data = IMDBDataset(sentences = imdb_sentences_val.values,\n                           labels = imdb_labels_val.values,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nimdb_test_data = IMDBDataset(sentences = imdb_sentences_test.values,\n                           labels = imdb_labels_test.values,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\n\n# Creo i DataLoader\ntrain_loader_imdb = DataLoader(imdb_training_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader_imdb = DataLoader(imdb_validation_data, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader_imdb = DataLoader(imdb_test_data, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:09:43.555447Z","iopub.execute_input":"2025-01-22T16:09:43.555623Z","iopub.status.idle":"2025-01-22T16:09:49.607394Z","shell.execute_reply.started":"2025-01-22T16:09:43.555607Z","shell.execute_reply":"2025-01-22T16:09:49.606486Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c55dbbd113646909b4e096765cd1643"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f9c5c826ec24e1d835bd612ecdc1d73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f665de04f8ae4ffa83f0df1eb4814faa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"130ecc8549884eb48c75913c2fafe892"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"#### 2. SST-2 ","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\n\nsst_dataset = load_dataset('sst2')\n\n\nsst_data = sst_dataset['train'].shuffle(seed=42)\n\nsst_temp_data, sst_test_data, sst_temp_labels, sst_test_labels = train_test_split(sst_data['sentence'], \n                                                  sst_data['label'], \n                                                  test_size=1024, \n                                                  random_state=42,\n                                                  stratify=sst_data['label'])\n\nsst_train_data, sst_val_data, sst_train_labels, sst_val_labels = train_test_split(sst_data['sentence'], \n                                                  sst_data['label'],\n                                                  train_size=10000,\n                                                  test_size=1000, \n                                                  random_state=42,\n                                                  stratify=sst_data['label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:09:49.609880Z","iopub.execute_input":"2025-01-22T16:09:49.610328Z","iopub.status.idle":"2025-01-22T16:09:58.325578Z","shell.execute_reply.started":"2025-01-22T16:09:49.610303Z","shell.execute_reply":"2025-01-22T16:09:58.324696Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16cee6b119124fcc95e5011173eaa358"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a04bbb9b1ee4970a7063f5d293395a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d42d3491858c42dea0cc2309615f62b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79ec1d9b731e4d9fb273ec4fcd2f198b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"166641f30a0a470d874a7125bbcd8a39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3844ce85f2a940b6b25731186eb6f7a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c0e6873c99848acba5cd4448604d7c9"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass SSTDataset(Dataset):\n\n    def __init__(self, sentences, labels, tokenizer, max_len):\n        self.sentences = sentences\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    def __getitem__(self,index):\n        sentence = self.sentences[index]\n        label = self.labels[index]\n        \n        encoding = self.tokenizer.encode_plus(\n            sentence,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors='pt')\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding[\"token_type_ids\"].flatten(),\n            'labels': torch.tensor(label, dtype=torch.float)\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:09:58.327135Z","iopub.execute_input":"2025-01-22T16:09:58.327577Z","iopub.status.idle":"2025-01-22T16:09:58.333246Z","shell.execute_reply.started":"2025-01-22T16:09:58.327553Z","shell.execute_reply":"2025-01-22T16:09:58.332498Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 128\n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\nsst_training_data = SSTDataset(sentences = sst_train_data,\n                           labels = sst_train_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nsst_validation_data = SSTDataset(sentences = sst_val_data,\n                           labels = sst_val_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\nsst_test_data = SSTDataset(sentences = sst_test_data,\n                           labels = sst_test_labels,\n                           tokenizer = tokenizer,\n                           max_len = MAX_SEQ_LEN)\n\n# Creo i DataLoader\ntrain_loader_sst = DataLoader(sst_training_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader_sst = DataLoader(sst_validation_data, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader_sst = DataLoader(sst_test_data, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:09:58.334123Z","iopub.execute_input":"2025-01-22T16:09:58.334467Z","iopub.status.idle":"2025-01-22T16:09:58.510203Z","shell.execute_reply.started":"2025-01-22T16:09:58.334435Z","shell.execute_reply":"2025-01-22T16:09:58.509390Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"#### 3. MNLI","metadata":{}},{"cell_type":"code","source":"# Carico il datast\nmnli_dataset = load_dataset('glue', 'mnli')\n\n# Divido i dati in training set, validation set e test set\nmnli_data = mnli_dataset['train'].shuffle(seed=42)\n\nmnli_temp_premises, mnli_test_premises, mnli_temp_hypotheses, mnli_test_hypotheses, mnli_temp_labels, mnli_test_labels = train_test_split(mnli_data['premise'], \n                                                  mnli_data['hypothesis'],                \n                                                  mnli_data['label'], \n                                                  test_size=1024, \n                                                  random_state=42,\n                                                  stratify=mnli_data['label'])\n\nmnli_train_premises, mnli_val_premises, mnli_train_hypotheses, mnli_val_hypotheses, mnli_train_labels, mnli_val_labels = train_test_split(mnli_data['premise'], \n                                                  mnli_data['hypothesis'],\n                                                  mnli_data['label'],\n                                                  train_size=10000,\n                                                  test_size=1000, \n                                                  random_state=42,\n                                                  stratify=mnli_data['label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:09:58.511123Z","iopub.execute_input":"2025-01-22T16:09:58.511471Z","iopub.status.idle":"2025-01-22T16:10:23.291371Z","shell.execute_reply.started":"2025-01-22T16:09:58.511436Z","shell.execute_reply":"2025-01-22T16:10:23.290651Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79b8caa939c34df8b24c06b896f3b419"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/52.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dd7d0b2ef6f4369aa34188b4265f5c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)alidation_matched-00000-of-00001.parquet:   0%|          | 0.00/1.21M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2e0a14485a142a6a61b280f6bdbb505"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)dation_mismatched-00000-of-00001.parquet:   0%|          | 0.00/1.25M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dc8cc523d32402badaaadd6cf901019"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test_matched-00000-of-00001.parquet:   0%|          | 0.00/1.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53dd3fb171f9462f8eb253c104b18400"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test_mismatched-00000-of-00001.parquet:   0%|          | 0.00/1.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2cf5bbca5cb45d8a2f7c925b8ff249b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"836b88c3b35944d99f5606bf8e991df7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7916d4ffcdf741548722bb54188ecda9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c3f924d16a54ddd9a77c4f457e1a881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_matched split:   0%|          | 0/9796 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc419a4c28044e4fbf31ba94b83b2f61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test_mismatched split:   0%|          | 0/9847 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c341ea568b548de94f5474e51f6562b"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass MNLIDataset(Dataset):\n\n    def __init__(self, premises, hypotheses , labels, tokenizer, max_len):\n        self.premises = premises\n        self.hypotheses = hypotheses\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.premises)\n    \n    def __getitem__(self,index):\n        premise = self.premises[index]\n        hyphotesis = self.hypotheses[index]\n        label = self.labels[index]\n        \n        encoding = self.tokenizer.encode_plus(\n            premise,\n            hyphotesis,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            truncation=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors='pt')\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'token_type_ids': encoding[\"token_type_ids\"].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:23.292206Z","iopub.execute_input":"2025-01-22T16:10:23.292459Z","iopub.status.idle":"2025-01-22T16:10:23.299046Z","shell.execute_reply.started":"2025-01-22T16:10:23.292437Z","shell.execute_reply":"2025-01-22T16:10:23.298143Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from transformers import BertTokenizer\nfrom torch.utils.data import DataLoader\n\nMAX_SEQ_LEN = 256 \nBATCH_SIZE = 32\n\n# Inizializza il Tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n#Ottieni i dataset\nmnli_training_data = MNLIDataset(premises = mnli_train_premises,\n                            hypotheses = mnli_train_hypotheses,\n                            labels = mnli_train_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)\n\nmnli_validation_data = MNLIDataset(premises = mnli_train_premises,\n                            hypotheses = mnli_train_hypotheses,\n                            labels = mnli_train_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)\n\nmnli_test_data = MNLIDataset(premises = mnli_train_premises,\n                            hypotheses = mnli_train_hypotheses,\n                            labels = mnli_train_labels,\n                            tokenizer = tokenizer,\n                            max_len = MAX_SEQ_LEN)\n\n# Creo i DataLoader\ntrain_loader_mnli = DataLoader(mnli_training_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader_mnli = DataLoader(mnli_validation_data, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader_mnli = DataLoader(mnli_test_data, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:23.299805Z","iopub.execute_input":"2025-01-22T16:10:23.300080Z","iopub.status.idle":"2025-01-22T16:10:23.483300Z","shell.execute_reply.started":"2025-01-22T16:10:23.300050Z","shell.execute_reply":"2025-01-22T16:10:23.482443Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Caricamento dei modelli","metadata":{}},{"cell_type":"markdown","source":"#### 1. IMDB Rewiews","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel\n\nclass BERTClassifierIMDB(nn.Module):\n    \n    def __init__(self, lora: bool = False, r: int = 16):\n        super(BERTClassifierIMDB, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.avg_pooling = nn.AdaptiveAvgPool1d(1) \n        self.linear = nn.Linear(self.bert.config.hidden_size, 1)\n\n        if lora:\n            print(\"Adding LoRA to BERT\")\n            lora_utils.add_lora_to_bert(self.bert, r=r)\n            lora_utils.mark_only_lora_as_trainable(self.bert)\n\n    \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_bert = self.bert(\n            input_ids, \n            attention_mask=attention_mask, \n            token_type_ids=token_type_ids\n        )\n        last_hidden_state = output_bert.last_hidden_state  \n        avg_pooled = self.avg_pooling(last_hidden_state.transpose(1, 2)).squeeze(-1)\n        logits = self.linear(avg_pooled)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:23.484168Z","iopub.execute_input":"2025-01-22T16:10:23.484464Z","iopub.status.idle":"2025-01-22T16:10:35.026032Z","shell.execute_reply.started":"2025-01-22T16:10:23.484424Z","shell.execute_reply":"2025-01-22T16:10:35.025384Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:35.026730Z","iopub.execute_input":"2025-01-22T16:10:35.027277Z","iopub.status.idle":"2025-01-22T16:10:35.031035Z","shell.execute_reply.started":"2025-01-22T16:10:35.027252Z","shell.execute_reply":"2025-01-22T16:10:35.030101Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"imdb_model = BERTClassifierIMDB(lora=False)\nimdb_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:35.032041Z","iopub.execute_input":"2025-01-22T16:10:35.032340Z","iopub.status.idle":"2025-01-22T16:10:38.073633Z","shell.execute_reply.started":"2025-01-22T16:10:35.032310Z","shell.execute_reply":"2025-01-22T16:10:38.072586Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2814e3f2df2b438492288417733071c4"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"BERTClassifierIMDB(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (avg_pooling): AdaptiveAvgPool1d(output_size=1)\n  (linear): Linear(in_features=768, out_features=1, bias=True)\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"imdb_model.load_state_dict(torch.load(\"/kaggle/input/saved-models/imbd_best_full_model_state.bin\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:38.074651Z","iopub.execute_input":"2025-01-22T16:10:38.074908Z","iopub.status.idle":"2025-01-22T16:10:41.355020Z","shell.execute_reply.started":"2025-01-22T16:10:38.074884Z","shell.execute_reply":"2025-01-22T16:10:41.354010Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-17-16ac1b07d264>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  imdb_model.load_state_dict(torch.load(\"/kaggle/input/saved-models/imbd_best_full_model_state.bin\"))\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"#### 2. SST-2","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel\n\nclass BERTClassifierSST(nn.Module):\n    \n    def __init__(self, lora: bool = False, r: int = 16):\n        super(BERTClassifierSST, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = torch.nn.Dropout(p=0.3)\n        self.linear = nn.Linear(self.bert.config.hidden_size, 1)\n\n        if lora:\n            print(\"Adding LoRA to BERT\")\n            lora_utils.add_lora_to_bert(self.bert, r=r)\n            lora_utils.mark_only_lora_as_trainable(self.bert)\n\n    \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_bert = self.bert(\n            input_ids, \n            attention_mask=attention_mask, \n            token_type_ids=token_type_ids\n        )\n        output_dropout = self.dropout(output_bert.pooler_output)\n        output = self.linear(output_dropout)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:41.356010Z","iopub.execute_input":"2025-01-22T16:10:41.356362Z","iopub.status.idle":"2025-01-22T16:10:41.362119Z","shell.execute_reply.started":"2025-01-22T16:10:41.356326Z","shell.execute_reply":"2025-01-22T16:10:41.361342Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"sst_model = BERTClassifierSST(lora=False)\nsst_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:41.363281Z","iopub.execute_input":"2025-01-22T16:10:41.363658Z","iopub.status.idle":"2025-01-22T16:10:42.089761Z","shell.execute_reply.started":"2025-01-22T16:10:41.363623Z","shell.execute_reply":"2025-01-22T16:10:42.088979Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"BERTClassifierSST(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=768, out_features=1, bias=True)\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"sst_model.load_state_dict(torch.load(\"/kaggle/input/saved-models/sa_full_model_best_model_state.bin\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:42.090514Z","iopub.execute_input":"2025-01-22T16:10:42.090777Z","iopub.status.idle":"2025-01-22T16:10:44.279574Z","shell.execute_reply.started":"2025-01-22T16:10:42.090755Z","shell.execute_reply":"2025-01-22T16:10:44.278698Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-20-9ea455b3aab8>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  sst_model.load_state_dict(torch.load(\"/kaggle/input/saved-models/sa_full_model_best_model_state.bin\"))\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"#### 3. MNLI","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel\n\nclass BERTClassifierNLI(nn.Module):\n    \n    def __init__(self, num_classes: int = 3, lora: bool = False, r: int = 16):\n        super(BERTClassifierNLI, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(p=0.3)\n        self.linear = nn.Linear(self.bert.config.hidden_size, num_classes)\n\n        if lora:\n            print(\"Adding LoRA to BERT\")\n            lora_utils.add_lora_to_bert(self.bert, r=r)\n            lora_utils.mark_only_lora_as_trainable(self.bert)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_bert = self.bert(\n            input_ids, \n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        output_dropout = self.dropout(output_bert.pooler_output)\n        logits = self.linear(output_dropout)\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:44.280421Z","iopub.execute_input":"2025-01-22T16:10:44.280717Z","iopub.status.idle":"2025-01-22T16:10:44.288193Z","shell.execute_reply.started":"2025-01-22T16:10:44.280687Z","shell.execute_reply":"2025-01-22T16:10:44.287242Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"mnli_model = BERTClassifierNLI(lora=False)\nmnli_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:44.289097Z","iopub.execute_input":"2025-01-22T16:10:44.289501Z","iopub.status.idle":"2025-01-22T16:10:44.721229Z","shell.execute_reply.started":"2025-01-22T16:10:44.289464Z","shell.execute_reply":"2025-01-22T16:10:44.720273Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"BERTClassifierNLI(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"mnli_model.load_state_dict(torch.load(\"/kaggle/input/saved-models/nli_full_model_best_model_state.bin\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:44.724767Z","iopub.execute_input":"2025-01-22T16:10:44.725041Z","iopub.status.idle":"2025-01-22T16:10:46.723420Z","shell.execute_reply.started":"2025-01-22T16:10:44.725015Z","shell.execute_reply":"2025-01-22T16:10:46.722543Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-23-6628b8c92126>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  mnli_model.load_state_dict(torch.load(\"/kaggle/input/saved-models/nli_full_model_best_model_state.bin\"))\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## Metriche di transfer learning","metadata":{}},{"cell_type":"markdown","source":"Installo la libreria tllib.","metadata":{}},{"cell_type":"code","source":"pip install -i https://test.pypi.org/simple/ tllib==0.4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:46.724709Z","iopub.execute_input":"2025-01-22T16:10:46.724945Z","iopub.status.idle":"2025-01-22T16:10:51.870476Z","shell.execute_reply.started":"2025-01-22T16:10:46.724925Z","shell.execute_reply":"2025-01-22T16:10:51.869506Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://test.pypi.org/simple/\nCollecting tllib==0.4\n  Downloading https://test-files.pythonhosted.org/packages/33/07/38ed6a831287654ea1d9b281fee7a3629f8065f274d8b503b5c64d3c1556/tllib-0.4-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from tllib==0.4) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tllib==0.4) (0.20.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tllib==0.4) (1.26.4)\nRequirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from tllib==0.4) (3.12.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tllib==0.4) (4.67.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from tllib==0.4) (1.2.2)\nRequirement already satisfied: webcolors in /usr/local/lib/python3.10/dist-packages (from tllib==0.4) (24.11.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from tllib==0.4) (3.7.5)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from tllib==0.4) (4.10.0.84)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from tllib==0.4) (0.60.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->tllib==0.4) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->tllib==0.4) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->tllib==0.4) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->tllib==0.4) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->tllib==0.4) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->tllib==0.4) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7.0->tllib==0.4) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->tllib==0.4) (11.0.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tllib==0.4) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tllib==0.4) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tllib==0.4) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tllib==0.4) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tllib==0.4) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tllib==0.4) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->tllib==0.4) (2.8.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->tllib==0.4) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->tllib==0.4) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->tllib==0.4) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->tllib==0.4) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->tllib==0.4) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->tllib==0.4) (2.4.1)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->tllib==0.4) (0.43.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->tllib==0.4) (0.2.13)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->tllib==0.4) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->tllib==0.4) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->tllib==0.4) (3.5.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->tllib==0.4) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->tllib==0.4) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->tllib==0.4) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->tllib==0.4) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->tllib==0.4) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->tllib==0.4) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->tllib==0.4) (2024.2.0)\nDownloading https://test-files.pythonhosted.org/packages/33/07/38ed6a831287654ea1d9b281fee7a3629f8065f274d8b503b5c64d3c1556/tllib-0.4-py3-none-any.whl (287 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.6/287.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tllib\nSuccessfully installed tllib-0.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### LEEP Score","metadata":{}},{"cell_type":"markdown","source":"Implementazione del leep score.","metadata":{}},{"cell_type":"code","source":"def leep_score(pretrained_model, target_dataset_loader, device):\n    \"\"\"\n    Calcola il LEEP score dato un modello pre-addestrato e un dataset target.\n    \n    Args:\n        pretrained_model (torch.nn.Module): Modello pre-addestrato θ.\n        target_dataset_loader (DataLoader): DataLoader per il dataset target D.\n        device (torch.device): Device per il calcolo (CPU/GPU).\n        \n    Returns:\n        float: LEEP score.\n    \"\"\"\n\n    # Imposto il modello in modalità di valutazione e lo sposto sul device\n    pretrained_model.eval()  \n    pretrained_model.to(device)\n\n\n    # 1) Calcolo delle distribuzioni dummy θ(xi)\n    dummy_distributions = []  # distribuzioni θ(xi) per ogni esempio\n    all_target_labels = []    # etichette target yi, serve solo per poter calcolare il numero di classi del dataset target\n    \n    with torch.no_grad():\n        for batch in target_dataset_loader:    # itero sugli esempi del dataset target D\n\n            # Ottengo gli esempi del dataset target e li sposto sul device\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['labels'].to(device) \n\n            # Memorizzo l'etichetta \n            all_target_labels.extend(labels.cpu().numpy())\n            \n            # Mando gli esempi in input al modello e ottengo l'output del modello (ovvero i logits)\n            outputs = pretrained_model(input_ids, attention_mask, token_type_ids)\n\n            # Trasformo i logits in probabilità e calcolo così θ(xi)\n            prob_class_1 = torch.sigmoid(outputs)  # Visto che il modello ha solo un neurone di output e restituisce quindi la probabilità della classe 1, calcolo manualmente la probabilità della classe 0\n            prob_class_0 = 1 - prob_class_1 \n            probabilities = torch.cat([prob_class_0, prob_class_1], dim=-1)\n            \n            # appendo alla lista di dummy distribution\n            dummy_distributions.append(probabilities)\n\n    # Concateno tutti i batch in un unico tensore\n    dummy_distributions = torch.cat(dummy_distributions, dim=0)  # (n, |Z|)\n    \n    \n    # Determino il numero di classi sorgente e il numero di classi target\n    num_source_labels = dummy_distributions.size(1)  # |Z|\n    num_target_labels = len(np.unique(all_target_labels))  # |Y|\n    \n    num_target_examples = dummy_distributions.size(0)  # n\n    \n    # 2) Calcolo della distribuzione condizionale empirica\n\n    # 2.1) Calcolo della distribuzione condizionale empirica P(y,z)\n    joint_distribution = np.zeros((num_target_labels, num_source_labels))\n    for y in range(num_target_labels):\n        # Trovo gli indici degli esempi nel dataset target che hanno etichetta y\n        indices = torch.where(torch.tensor(all_target_labels) == y)[0]\n    \n        # Sommo θ(xi)_z per ogni esempio xi appartenente alla classe y\n        for z in range(num_source_labels):\n            joint_distribution[y][z] = dummy_distributions[indices, z].sum().item()\n    \n    # Normalizzo dividendo per il numero totale di esempi nel dataset target\n    joint_distribution /= num_target_examples\n\n    # 2.2) Calcolo della distribuzione marginale empirica P(z)\n    marginal_distribution = joint_distribution.sum(axis=0)\n\n    # 2.3) Calcolo della distribuzione condizionale empirica P(y|z)\n    conditional_distribution = joint_distribution / marginal_distribution  # (|Y|, |Z|)\n    conditional_distribution = np.nan_to_num(conditional_distribution)     # Per gestire i casi in cui P(z) = 0, -inf, +inf\n\n    \n    # 3) Calcolo del LEEP score\n\n    log_likelihood_sum = 0.0\n    for i in range(num_target_examples):\n        # Estrai la distribuzione di probabilità θ(x_i) per l'esempio i\n        x_i_probabilities = dummy_distributions[i].cpu().numpy()  # (|Z|,)\n        y_i = int(all_target_labels[i])  # Etichetta vera y_i del target\n        \n        # Calcola p(y_i|x_i; θ, D) = ∑_{z ∈ Z} P̂(y_i|z) * θ(x_i)_z\n        eep_prediction = np.sum(\n            conditional_distribution[y_i, :] * x_i_probabilities  # Element-wise prodotto\n        )\n\n        # Calcola il logaritmo della previsione EEP e aggiungi alla somma\n        log_likelihood_sum += np.log(eep_prediction)\n\n    # Calcola la media del logaritmo\n    leep_score = log_likelihood_sum / num_target_examples\n\n    return leep_score\n\n\n# Calcola il LEEP score utilizzando il modello pre-addestrato e i dati target\nleeps = leep_score(\n    pretrained_model=sst_model,\n    target_dataset_loader=train_loader_sst,\n    device=device\n)\n\nprint(leeps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:10:51.871601Z","iopub.execute_input":"2025-01-22T16:10:51.871952Z","iopub.status.idle":"2025-01-22T16:11:25.020317Z","shell.execute_reply.started":"2025-01-22T16:10:51.871925Z","shell.execute_reply":"2025-01-22T16:11:25.019317Z"}},"outputs":[{"name":"stdout","text":"-0.023772563672319818\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"def get_model_predictions(model, data_loader):\n    model.eval()\n    model.to(device)\n    \n    all_predictions = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['labels'].to(device)\n\n            # Salvo le etichette vere\n            all_labels.extend(labels.cpu().numpy())\n            \n            # Ottiengo i logits dal modello\n            outputs = model(input_ids=input_ids, \n                            attention_mask=attention_mask, \n                            token_type_ids=token_type_ids)\n            \n            if outputs.size(-1) == 1:  # Nel caso di problema di classificazione binaria\n                prob_class_1 = torch.sigmoid(outputs)  # Probabilità della classe 1\n                prob_class_0 = 1 - prob_class_1        # Probabilità della classe 0\n                probabilities = torch.cat([prob_class_0, prob_class_1], dim=-1)\n            else:  # Nel caso di problema di classificazione multi-classe\n                probabilities = torch.softmax(outputs, dim=-1)\n\n            all_predictions.extend(probabilities.cpu().numpy())\n\n    return np.array(all_predictions), np.array(all_labels).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:11:25.021316Z","iopub.execute_input":"2025-01-22T16:11:25.021605Z","iopub.status.idle":"2025-01-22T16:11:25.028328Z","shell.execute_reply.started":"2025-01-22T16:11:25.021579Z","shell.execute_reply":"2025-01-22T16:11:25.027245Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"from tllib.ranking import log_expected_empirical_prediction as leep\n\ndef calculate_leep_scores(models, data_loaders):\n    leep_scores = []\n    for source_name, source_model in models.items():\n        for target_name, target_loader in data_loaders.items():\n            \n            predictions, labels = get_model_predictions(source_model, target_loader)\n            score = leep(predictions, labels)\n            leep_scores.append({\"Source\": source_name, \"Target\": target_name, \"Metric\": \"LEEP\", \"Score\": score})\n            print(f\"Source: {source_name}, Target: {target_name}, LEEP Score: {score:.4f}\")\n    return leep_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:11:25.029222Z","iopub.execute_input":"2025-01-22T16:11:25.029508Z","iopub.status.idle":"2025-01-22T16:11:26.600395Z","shell.execute_reply.started":"2025-01-22T16:11:25.029484Z","shell.execute_reply":"2025-01-22T16:11:26.599445Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### LogME Score","metadata":{}},{"cell_type":"code","source":"def extract_embeddings(model, data_loader, device):\n    model.eval()\n    embeddings = []\n    labels = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            label = batch['labels'].cpu().numpy()\n\n            # Ottiengo l'output del modello\n            outputs = model.bert(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids\n            )\n            # Estraggo la rappresentazione del token [CLS]\n            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n            embeddings.append(cls_embeddings)\n            labels.append(label)\n    \n    embeddings = np.vstack(embeddings)\n    labels = np.concatenate(labels)\n    labels = labels.astype(int)\n    \n    return embeddings, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:11:26.601350Z","iopub.execute_input":"2025-01-22T16:11:26.601649Z","iopub.status.idle":"2025-01-22T16:11:26.607690Z","shell.execute_reply.started":"2025-01-22T16:11:26.601613Z","shell.execute_reply":"2025-01-22T16:11:26.606742Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from tllib.ranking import log_maximum_evidence as logme\n\ndef calculate_logme_scores(models, data_loaders, device):\n    logme_scores = []\n    for source_name, source_model in models.items():\n        for target_name, target_loader in data_loaders.items():\n            \n            embeddings, labels = extract_embeddings(source_model, target_loader, device)\n            score = logme(embeddings, labels)\n            logme_scores.append({\"Source\": source_name, \"Target\": target_name, \"Metric\": \"LogME\", \"Score\": score})\n            print(f\"Source: {source_name}, Target: {target_name}, LogME Score: {score:.4f}\")\n    return logme_scores\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:11:26.608598Z","iopub.execute_input":"2025-01-22T16:11:26.609040Z","iopub.status.idle":"2025-01-22T16:11:26.632792Z","shell.execute_reply.started":"2025-01-22T16:11:26.609001Z","shell.execute_reply":"2025-01-22T16:11:26.631867Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"#### H-Score","metadata":{}},{"cell_type":"code","source":"from tllib.ranking import h_score\n\ndef calculate_h_scores(models, data_loaders, device):\n    h_scores = []\n    for source_name, source_model in models.items():\n        for target_name, target_loader in data_loaders.items():\n            \n            embeddings, labels = extract_embeddings(source_model, target_loader, device)\n            score = h_score(embeddings, labels)\n            h_scores.append({\"Source\": source_name, \"Target\": target_name, \"Metric\": \"H-Score\", \"Score\": score})\n            print(f\"Source: {source_name}, Target: {target_name}, H-Score: {score:.4f}\")\n    \n    return h_scores\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:11:26.633795Z","iopub.execute_input":"2025-01-22T16:11:26.634131Z","iopub.status.idle":"2025-01-22T16:11:26.648711Z","shell.execute_reply.started":"2025-01-22T16:11:26.634096Z","shell.execute_reply":"2025-01-22T16:11:26.647937Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"### NCE Score","metadata":{}},{"cell_type":"code","source":"def get_source_labels(model, data_loader, device):\n    model.eval()\n    all_predictions = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n\n            # Ottieni i logits dal modello\n            logits = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            \n            # Converti i logits in etichette predette (classe 0 o 1)\n            predictions = torch.sigmoid(logits).cpu().numpy().flatten() > 0.5\n            all_predictions.extend(predictions.astype(int))\n    \n    return np.array(all_predictions)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:11:26.649575Z","iopub.execute_input":"2025-01-22T16:11:26.649855Z","iopub.status.idle":"2025-01-22T16:11:26.670508Z","shell.execute_reply.started":"2025-01-22T16:11:26.649822Z","shell.execute_reply":"2025-01-22T16:11:26.669527Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"from tllib.ranking import negative_conditional_entropy as nce\n\ndef calculate_nce_scores(models, data_loaders, device):\n    nce_scores = []\n    \n    for source_name, source_model in models.items():\n        for target_name, target_loader in data_loaders.items():\n            \n            source_labels = get_source_labels(source_model, target_loader, device)\n            target_labels = np.array(target_loader.dataset.labels)\n            score = nce(source_labels, target_labels)\n            nce_scores.append({\"Source\": source_name, \"Target\": target_name, \"Metric\": \"NCE\", \"Score\": score})\n            print(f\"Source: {source_name}, Target: {target_name}, NCE Score: {score:.4f}\")\n    \n    return nce_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:11:26.671613Z","iopub.execute_input":"2025-01-22T16:11:26.671977Z","iopub.status.idle":"2025-01-22T16:11:26.686264Z","shell.execute_reply.started":"2025-01-22T16:11:26.671939Z","shell.execute_reply":"2025-01-22T16:11:26.685384Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"models = {\n    \"IMDB\": imdb_model,\n    \"SST\": sst_model,\n    \"MNLI\": mnli_model\n}\n\ndata_loaders = {\n    \"IMDB\": train_loader_imdb,\n    \"SST\": train_loader_sst,\n    \"MNLI\": train_loader_mnli\n}\n\n\nleep_scores = calculate_leep_scores(models, data_loaders)\nlogme_scores = calculate_logme_scores(models, data_loaders, device)\nh_scores = calculate_h_scores(models, data_loaders, device)\nnce_scores = calculate_nce_scores(models, data_loaders, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:11:26.687186Z","iopub.execute_input":"2025-01-22T16:11:26.687409Z","iopub.status.idle":"2025-01-22T16:52:15.739160Z","shell.execute_reply.started":"2025-01-22T16:11:26.687389Z","shell.execute_reply":"2025-01-22T16:52:15.738190Z"}},"outputs":[{"name":"stdout","text":"Source: IMDB, Target: IMDB, LEEP Score: -0.1040\nSource: IMDB, Target: SST, LEEP Score: -0.4576\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Source: IMDB, Target: MNLI, LEEP Score: -1.0972\nSource: SST, Target: IMDB, LEEP Score: -0.4417\nSource: SST, Target: SST, LEEP Score: -0.0238\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Source: SST, Target: MNLI, LEEP Score: -1.0959\nSource: MNLI, Target: IMDB, LEEP Score: -0.6912\nSource: MNLI, Target: SST, LEEP Score: -0.6721\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Source: MNLI, Target: MNLI, LEEP Score: -0.1147\nSource: IMDB, Target: IMDB, LogME Score: 0.5761\nSource: IMDB, Target: SST, LogME Score: -0.2786\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Source: IMDB, Target: MNLI, LogME Score: -0.5949\nSource: SST, Target: IMDB, LogME Score: -0.2925\nSource: SST, Target: SST, LogME Score: 1.3886\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Source: SST, Target: MNLI, LogME Score: -0.5869\nSource: MNLI, Target: IMDB, LogME Score: -0.5106\nSource: MNLI, Target: SST, LogME Score: -0.3623\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Source: MNLI, Target: MNLI, LogME Score: 0.6378\nSource: IMDB, Target: IMDB, H-Score: 38.7167\nSource: IMDB, Target: SST, H-Score: -3.0726\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Source: IMDB, Target: MNLI, H-Score: -0.9263\nSource: SST, Target: IMDB, H-Score: -8.0096\nSource: SST, Target: SST, H-Score: 2.1262\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Source: SST, Target: MNLI, H-Score: 0.6774\nSource: MNLI, Target: IMDB, H-Score: 1.2456\nSource: MNLI, Target: SST, H-Score: -3.0537\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Source: MNLI, Target: MNLI, H-Score: -62.4062\nSource: IMDB, Target: IMDB, NCE Score: -0.6931\nSource: IMDB, Target: SST, NCE Score: -0.6865\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Source: IMDB, Target: MNLI, NCE Score: -1.0985\nSource: SST, Target: IMDB, NCE Score: -0.6931\nSource: SST, Target: SST, NCE Score: -0.6864\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Source: SST, Target: MNLI, NCE Score: -1.0984\nSource: MNLI, Target: IMDB, NCE Score: -0.2310\nSource: MNLI, Target: SST, NCE Score: -0.2288\n","output_type":"stream"},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"name":"stdout","text":"Source: MNLI, Target: MNLI, NCE Score: -0.3662\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"all_scores = leep_scores + logme_scores + h_scores + nce_scores\nresults_df = pd.DataFrame(all_scores)\n\npivot_df = results_df.pivot_table(\n    index=[\"Source\", \"Target\"],\n    columns=\"Metric\",\n    values=\"Score\"\n).reset_index()\n\n# Ordiniamo le righe in base a Source e Target\npivot_df = pivot_df.sort_values(by=[\"Source\", \"Target\"])\n\n# Visualizziamo il risultato\nprint(pivot_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T16:52:15.779052Z","iopub.execute_input":"2025-01-22T16:52:15.779294Z","iopub.status.idle":"2025-01-22T16:52:15.795041Z","shell.execute_reply.started":"2025-01-22T16:52:15.779273Z","shell.execute_reply":"2025-01-22T16:52:15.793871Z"}},"outputs":[{"name":"stdout","text":"Metric Source Target    H-Score      LEEP     LogME       NCE\n0        IMDB   IMDB  38.716675 -0.104039  0.576065 -0.693146\n1        IMDB   MNLI  -0.926340 -1.097195 -0.594876 -1.098514\n2        IMDB    SST  -3.072601 -0.457571 -0.278590 -0.686450\n3        MNLI   IMDB   1.245621 -0.691206 -0.510617 -0.231047\n4        MNLI   MNLI -62.406204 -0.114683  0.637751 -0.366173\n5        MNLI    SST  -3.053725 -0.672133 -0.362303 -0.228816\n6         SST   IMDB  -8.009583 -0.441727 -0.292533 -0.693110\n7         SST   MNLI   0.677395 -1.095884 -0.586855 -1.098433\n8         SST    SST   2.126160 -0.023773  1.388554 -0.686380\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"- Il LEEP assume valori nell'intervallo (− inf,0). Più il suo valore si avvicina a zero, maggiore è la trasferibilità del modello. Questo significa che un modello addestrato sul task di partenza è meglio adattato a risolvere il task di destinazione.\n- Il LogME può assumere valori nell'intervallo (−inf,+inf). In generale, valori più elevati di LogME indicano una migliore performance di trasferimento.\n- Un H-score più alto segnala una maggiore qualità delle caratteristiche del modello per il trasferimento.\n+ Il NCE varia anch'esso nell'intervallo (−inf,0) e a valori più bassi di NCE sono generalmente associati a una minore trasferibilità.","metadata":{}}]}